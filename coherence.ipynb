{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Cell 1: 모델 실행, 평가 지표 실행, 기타 결과 분석\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from gensim import models, corpora\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.preprocessing import MinMaxScaler\n","from gensim.models.coherencemodel import CoherenceModel\n","import time\n","import json\n","from nltk.corpus import stopwords\n","from math import log\n","from itertools import combinations\n","from tqdm import tqdm\n","import logging\n","from collections import Counter, defaultdict\n","import gensim\n","from gensim import corpora\n","from scipy.sparse import csr_matrix\n","from gensim.utils import simple_preprocess\n","from gensim.corpora import Dictionary\n","from transformers import BertTokenizer, BertModel\n","from bertopic import BERTopic\n","import seaborn as sns\n","from scipy import stats\n","import os\n","import re\n","import matplotlib\n","from tabulate import tabulate\n","matplotlib.use('Agg')\n","import matplotlib.pyplot as plt\n","from sklearn.manifold import MDS\n","from sklearn.preprocessing import MinMaxScaler\n","from torch.utils.data import DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.cluster import KMeans\n","\n","# NLTK 데이터 다운로드\n","import nltk\n","nltk.download('punkt', quiet=True)\n","nltk.download('stopwords', quiet=True)\n","\n","# 로깅 설정\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","\n","# stop_words 정의\n","stop_words = set(stopwords.words('english'))\n","\n","# 전역 변수로 BERT 모델과 토크나이저 선언\n","global tokenizer, bert_model\n","\n","# BERT 모델 로딩 함수\n","def load_bert_model():\n","    global tokenizer, bert_model\n","    if 'tokenizer' not in globals() or 'bert_model' not in globals():\n","        logging.info(\"BERT 모델 및 토크나이저 로딩 중...\")\n","        try:\n","            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","            bert_model = BertModel.from_pretrained('bert-base-uncased')\n","            \n","            # GPU 사용 가능 여부 확인 및 설정\n","            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","            bert_model = bert_model.to(device)\n","            \n","            logging.info(f\"BERT 모델 및 토크나이저 로딩 완료. 사용 중인 디바이스: {device}\")\n","        except Exception as e:\n","            logging.error(f\"BERT 모델 로딩 중 오류 발생: {e}\")\n","            raise\n","\n","def load_data(file_path, sample_size=5000):\n","    try:\n","        df = pd.read_csv(file_path, header=None, names=['text'])\n","    except FileNotFoundError:\n","        logging.error(f\"File not found: {file_path}\")\n","        return []\n","    except Exception as e:\n","        logging.error(f\"Error loading file {file_path}: {e}\")\n","        return []\n","    texts = df['text'].astype(str)\n","    if len(texts) > sample_size:\n","        texts = texts.sample(n=sample_size, random_state=42)\n","    print(f\"Loaded {len(texts)} texts from {file_path}\")\n","    return texts.tolist()\n","\n","def load_all_datasets():\n","    datasets = {\n","        'academy': {\n","            'covid': load_data('data/academy/covid.csv')\n","        },\n","        'media': {\n","            'clothing_review': load_data('data/media/clothing_review.csv')\n","        },\n","        'news': {\n","            'agnews': load_data('data/news/agnews.csv')\n","        }\n","    }\n","    return datasets\n","\n","class VAE(nn.Module):\n","    def __init__(self, input_dim, hidden_dim=50, latent_dim=None):\n","        if latent_dim is None:\n","            raise ValueError(\"latent_dim must be specified\")\n","        super(VAE, self).__init__()\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        self.fc21 = nn.Linear(hidden_dim, latent_dim)  \n","        self.fc22 = nn.Linear(hidden_dim, latent_dim)  \n","        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n","        self.fc4 = nn.Linear(hidden_dim, input_dim)\n","\n","    def encode(self, x):\n","        h1 = F.relu(self.fc1(x))\n","        return self.fc21(h1), self.fc22(h1)\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5 * logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps * std\n","\n","    def decode(self, z):\n","        h3 = F.relu(self.fc3(z))\n","        return torch.sigmoid(self.fc4(h3))\n","\n","    def forward(self, x):\n","        mu, logvar = self.encode(x)\n","        z = self.reparameterize(mu, logvar)\n","        return self.decode(z), mu, logvar\n","\n","def vae_loss(recon_x, x, mu, logvar):\n","    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n","    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    return BCE + KLD\n","\n","def extract_vae_topics(vae_model, vectorizer, num_topics, top_n=10):\n","    with torch.no_grad():\n","        latent_vectors = torch.eye(num_topics).to(vae_model.fc3.weight.device)\n","        decoder_output = vae_model.decode(latent_vectors)\n","        decoder_output = decoder_output.cpu().numpy()\n","\n","    feature_names = vectorizer.get_feature_names_out()\n","    topics = []\n","    for topic_distribution in decoder_output:\n","        top_indices = topic_distribution.argsort()[-top_n:][::-1]\n","        topic_words = [feature_names[i] for i in top_indices]\n","        topics.append(topic_words)\n","    return topics\n","\n","# def perform_vae_topic_modeling(data, num_topics, num_epochs=5, hidden_dim=50):\n","#     data = [str(doc) for doc in data if isinstance(doc, str) and len(doc) > 0]\n","\n","#     # TfidfVectorizer 사용\n","#     vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n","#     doc_term_matrix = vectorizer.fit_transform(data)\n","\n","#     # MinMaxScaler를 사용하여 0-1 사이로 정규화\n","#     scaler = MinMaxScaler()\n","#     normalized_matrix = scaler.fit_transform(doc_term_matrix.toarray())\n","\n","#     vocab_size = len(vectorizer.get_feature_names_out())\n","#     input_dim = vocab_size\n","\n","#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#     vae_model = VAE(input_dim=input_dim, hidden_dim=hidden_dim, latent_dim=num_topics).to(device)\n","#     optimizer = torch.optim.Adam(vae_model.parameters(), lr=1e-3)\n","\n","#     batch_size = 64\n","#     data_loader = DataLoader(normalized_matrix.astype(np.float32), batch_size=batch_size, shuffle=True)\n","\n","#     vae_model.train()\n","#     for epoch in range(num_epochs):\n","#         train_loss = 0\n","#         for batch in data_loader:\n","#             batch = batch.to(device)\n","#             optimizer.zero_grad()\n","#             recon_batch, mu, logvar = vae_model(batch)\n","#             loss = vae_loss(recon_batch, batch, mu, logvar)\n","#             loss.backward()\n","#             optimizer.step()\n","#             train_loss += loss.item()\n","#         logging.info(f\"에폭 {epoch+1}/{num_epochs}, 손실: {train_loss / len(data_loader.dataset):.4f}\")\n","\n","#     topics = extract_vae_topics(vae_model, vectorizer, num_topics)\n","#     return vae_model, topics\n","\n","def perform_bertopic_modeling(data):\n","    if isinstance(data, dict):\n","        data = list(data.values())[0]\n","    elif isinstance(data, pd.DataFrame):\n","        data = data['text'].tolist() if 'text' in data.columns else data.values.flatten().tolist()\n","    elif isinstance(data, pd.Series):\n","        data = data.tolist()\n","    elif isinstance(data, np.ndarray):\n","        data = data.flatten().tolist()\n","    elif isinstance(data, list):\n","        pass\n","    else:\n","        raise ValueError(f\"Unsupported data format for BERTopic modeling: {type(data)}\")\n","\n","    # 데이터가 문자열 리스트인지 확인\n","    if not all(isinstance(item, str) for item in data):\n","        raise ValueError(\"All items in the data must be strings\")\n","\n","    try:\n","        bertopic_model = BERTopic(language=\"english\", calculate_probabilities=True)\n","        topics, _ = bertopic_model.fit_transform(data)\n","        \n","        num_topics = len(bertopic_model.get_topics())\n","        topic_words = []\n","        for i in range(num_topics):\n","            topic = bertopic_model.get_topic(i)\n","            if topic:\n","                words = [word for word, _ in topic[:10]]  # 상위 10개 단어만 추출\n","                topic_words.append(words)\n","        \n","        return bertopic_model, topic_words, num_topics\n","    except AttributeError as e:\n","        logging.error(f\"BERTopic 모델링 중 오류 발생: {e}\")\n","        return None, None, None\n","    \n","\n","def perform_lowbertopic_modeling(data, low_num_topics):\n","    # 토픽 수를 num_topics로 설정 (기본값 5)\n","    low_num_topics = 5\n","    \n","    # 데이터 전처리\n","    if isinstance(data, dict):\n","        data = list(data.values())[0]\n","    elif isinstance(data, pd.DataFrame):\n","        data = data['text'].tolist() if 'text' in data.columns else data.values.flatten().tolist()\n","    elif isinstance(data, pd.Series):\n","        data = data.tolist()\n","    elif isinstance(data, np.ndarray):\n","        data = data.flatten().tolist()\n","    elif isinstance(data, list):\n","        pass\n","    else:\n","        raise ValueError(f\"Unsupported data format for BERTopic modeling: {type(data)}\")\n","\n","    # 데이터가 문자열 리스트인지 확인\n","    if not all(isinstance(item, str) for item in data):\n","        raise ValueError(\"All items in the data must be strings\")\n","\n","    try:\n","        \n","        # BERTopic 모델 생성\n","        lowbertopic_model = BERTopic(language=\"english\", calculate_probabilities=True, nr_topics=low_num_topics)\n","        \n","        # 모델 학습\n","        topics, _ = lowbertopic_model.fit_transform(data)\n","        \n","        # 토픽 단어 추출\n","        topic_words = []\n","        for i in range(low_num_topics):\n","            topic = lowbertopic_model.get_topic(i)\n","            if topic:\n","                words = [word for word, _ in topic[:10]]  # 상위 10개 단어 추출\n","                topic_words.append(words)\n","        \n","        return lowbertopic_model, topic_words, low_num_topics\n","    except Exception as e:\n","        logging.error(f\"LowBERTopic 모델링 중 오류 발생: {e}\")\n","        return None, None, None\n","\n","\n","def perform_vae_topic_modeling(data, num_topics, num_epochs=5, hidden_dim=50):\n","    try:\n","        # 데이터 전처리\n","        data = [str(doc) for doc in data if isinstance(doc, str) and len(doc) > 0]\n","        vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n","        doc_term_matrix = vectorizer.fit_transform(data)\n","\n","        # MinMaxScaler를 사용하여 0-1 사이로 정규화\n","        scaler = MinMaxScaler()\n","        normalized_matrix = scaler.fit_transform(doc_term_matrix.toarray())\n","\n","        # VAE 모델 초기화 및 학습\n","        input_dim = doc_term_matrix.shape[1]\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        vae_model = VAE(input_dim=input_dim, hidden_dim=hidden_dim, latent_dim=num_topics).to(device)\n","        optimizer = torch.optim.Adam(vae_model.parameters(), lr=1e-3)\n","\n","        batch_size = 64\n","        data_loader = DataLoader(normalized_matrix.astype(np.float32), batch_size=batch_size, shuffle=True)\n","\n","        vae_model.train()\n","        for epoch in range(num_epochs):\n","            train_loss = 0\n","            for batch in data_loader:\n","                batch = batch.to(device)\n","                optimizer.zero_grad()\n","                recon_batch, mu, logvar = vae_model(batch)\n","                loss = vae_loss(recon_batch, batch, mu, logvar)\n","                loss.backward()\n","                optimizer.step()\n","                train_loss += loss.item()\n","            logging.info(f\"에폭 {epoch+1}/{num_epochs}, 손실: {train_loss / len(data_loader.dataset):.4f}\")\n","\n","        topics = extract_vae_topics(vae_model, vectorizer, num_topics)\n","        return vae_model, topics\n","    except Exception as e:\n","        logging.error(f\"VAE 모델링 중 오류 발생: {e}\")\n","        return None, None\n","\n","# def calculate_coherence(topics, tokenizer, bert_model):\n","#     \"\"\"\n","#     Calculate the coherence score for given topics using BERT embeddings.\n","\n","#     Args:\n","#     topics (list): List of topic word lists.\n","#     tokenizer (BertTokenizer): BERT tokenizer.\n","#     bert_model (BertModel): Pre-trained BERT model.\n","\n","#     Returns:\n","#     float: Average coherence score across all topics.\n","#     \"\"\"\n","#     coherence_scores = []\n","#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","#     bert_model.to(device)\n","#     bert_model.eval()\n","\n","#     for topic_words in topics:\n","#         inputs = tokenizer(topic_words, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","#         with torch.no_grad():\n","#             outputs = bert_model(**inputs)\n","#         embeddings = outputs.last_hidden_state[:, 0, :]  # [CLS] 토큰의 임베딩 사용\n","\n","#         num_words = len(topic_words)\n","#         if num_words < 2:\n","#             coherence_scores.append(0)\n","#             continue\n","\n","#         pairwise_similarities = []\n","#         for i in range(num_words):\n","#             for j in range(i + 1, num_words):\n","#                 cosine_sim = torch.nn.functional.cosine_similarity(embeddings[i], embeddings[j], dim=0)\n","#                 pairwise_similarities.append(cosine_sim.item())\n","\n","#         coherence = np.mean(pairwise_similarities)\n","#         coherence_scores.append(coherence)\n","\n","#     final_coherence = np.mean(coherence_scores) if coherence_scores else 0\n","#     return final_coherence\n","\n","def calculate_coherence(topics, tokenizer, bert_model, top_n=10, batch_size=16):\n","    coherence_scores = []\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    bert_model.to(device)\n","    bert_model.eval()\n","\n","    for topic_words in topics:\n","        # 토픽 단어 수 제한\n","        topic_words = topic_words[:top_n]\n","        # 유효한 단어만 선택\n","        topic_words = [word for word in topic_words if word and isinstance(word, str)]\n","        num_words = len(topic_words)\n","\n","        if num_words < 2:\n","            coherence_scores.append(0)\n","            continue\n","\n","        # 단어 임베딩을 배치로 계산\n","        embeddings = []\n","        for i in range(0, num_words, batch_size):\n","            batch_words = topic_words[i:i + batch_size]\n","            inputs = tokenizer(batch_words, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","            with torch.no_grad():\n","                outputs = bert_model(**inputs)\n","            batch_embeddings = outputs.last_hidden_state[:, 0, :]  # [CLS] 토큰의 임베딩 사용\n","            embeddings.append(batch_embeddings)\n","            # 메모리 관리\n","            del outputs\n","            torch.cuda.empty_cache()\n","\n","        embeddings = torch.cat(embeddings, dim=0)\n","\n","        # 코사인 유사도 계산\n","        pairwise_similarities = []\n","        for i in range(num_words):\n","            for j in range(i + 1, num_words):\n","                cosine_sim = torch.nn.functional.cosine_similarity(embeddings[i], embeddings[j], dim=0)\n","                pairwise_similarities.append(cosine_sim.item())\n","\n","        coherence = np.mean(pairwise_similarities)\n","        coherence_scores.append(coherence)\n","\n","    final_coherence = np.mean(coherence_scores) if coherence_scores else 0\n","    return final_coherence\n","\n","\n","def process_metrics(domain, model_type, topics, data, metrics_list, tokenizer, bert_model):\n","    tokenized_data = [simple_preprocess(doc) for doc in data]\n","    dictionary = Dictionary(tokenized_data)\n","    corpus = [dictionary.doc2bow(text) for text in tokenized_data]\n","\n","    coherence = calculate_coherence(topics, tokenizer, bert_model)\n","    npmi = calculate_npmi(topics, corpus, dictionary)\n","    umass = calculate_umass(topics, corpus, dictionary)\n","\n","    metrics_list.append({\n","        'Domain': domain,\n","        'Model': model_type,\n","        'Coherence': coherence,\n","        'NPMI': npmi,\n","        'U_Mass': umass\n","    })\n","\n","    logging.info(f\"Coherence: {coherence:.4f}, NPMI: {npmi:.4f}, U_Mass: {umass:.4f}\")\n","    \n","    return [metrics_list[-1]]  # 마지막에 추가된 메트릭을 리스트로 반환\n","\n","def calculate_npmi(topics, corpus, dictionary, top_n=10):\n","    # 토픽에서 사용된 모든 단어의 집합 생성\n","    topic_words_set = set()\n","    for topic in topics:\n","        topic_words_set.update(topic[:top_n])\n","\n","    # 단어를 ID로 매핑\n","    word2id = {word: dictionary.token2id[word] for word in topic_words_set if word in dictionary.token2id}\n","    id2word = {id: word for word, id in word2id.items()}\n","\n","    # 단어와 단어 쌍의 문서 빈도 계산\n","    total_docs = len(corpus)\n","    word_doc_freq = defaultdict(int)\n","    pair_doc_freq = defaultdict(int)\n","\n","    for doc in corpus:\n","        doc_word_ids = set([id for id, _ in doc])\n","        topic_word_ids_in_doc = doc_word_ids.intersection(set(word2id.values()))\n","\n","        for word_id in topic_word_ids_in_doc:\n","            word_doc_freq[word_id] += 1\n","\n","        for word_id1, word_id2 in combinations(topic_word_ids_in_doc, 2):\n","            pair = tuple(sorted((word_id1, word_id2)))\n","            pair_doc_freq[pair] += 1\n","\n","    # NPMI 계산\n","    npmi_scores = []\n","    for topic in topics:\n","        topic_word_ids = [word2id[word] for word in topic[:top_n] if word in word2id]\n","        if len(topic_word_ids) < 2:\n","            continue\n","        pair_npmi_scores = []\n","        for word_id1, word_id2 in combinations(topic_word_ids, 2):\n","            pair = tuple(sorted((word_id1, word_id2)))\n","            co_doc_count = pair_doc_freq.get(pair, 0)\n","            if co_doc_count == 0:\n","                continue\n","            p_w1_w2 = co_doc_count / total_docs\n","            p_w1 = word_doc_freq[word_id1] / total_docs\n","            p_w2 = word_doc_freq[word_id2] / total_docs\n","\n","            pmi = np.log(p_w1_w2 / (p_w1 * p_w2) + 1e-12)\n","            npmi = pmi / (-np.log(p_w1_w2 + 1e-12))\n","            pair_npmi_scores.append(npmi)\n","        if pair_npmi_scores:\n","            npmi_scores.append(np.mean(pair_npmi_scores))\n","\n","    return np.mean(npmi_scores) if npmi_scores else float('nan')\n","\n","def calculate_umass(topics, corpus, dictionary, top_n=10):\n","    # 토픽에서 사용된 모든 단어의 집합 생성\n","    topic_words_set = set()\n","    for topic in topics:\n","        topic_words_set.update(topic[:top_n])\n","\n","    # 단어를 ID로 매핑\n","    word2id = {word: dictionary.token2id[word] for word in topic_words_set if word in dictionary.token2id}\n","\n","    # 단어와 단어 쌍의 빈도 계산\n","    word_counts = defaultdict(int)\n","    pair_counts = defaultdict(int)\n","\n","    for doc in corpus:\n","        doc_word_ids = set([id for id, _ in doc])\n","        topic_word_ids_in_doc = doc_word_ids.intersection(set(word2id.values()))\n","\n","        for word_id in topic_word_ids_in_doc:\n","            word_counts[word_id] += 1\n","\n","        for word_id1, word_id2 in combinations(topic_word_ids_in_doc, 2):\n","            pair = tuple(sorted((word_id1, word_id2)))\n","            pair_counts[pair] += 1\n","\n","    # U_Mass 계산\n","    umass_scores = []\n","    for topic in topics:\n","        topic_word_ids = [word2id[word] for word in topic[:top_n] if word in word2id]\n","        if len(topic_word_ids) < 2:\n","            continue\n","        pair_umass_scores = []\n","        for i, word_id1 in enumerate(topic_word_ids):\n","            for word_id2 in topic_word_ids[:i]:\n","                pair = tuple(sorted((word_id1, word_id2)))\n","                co_occurrence = pair_counts.get(pair, 0) + 1  # 스무딩을 위해 +1\n","                word2_count = word_counts[word_id2] + 1  # 스무딩을 위해 +1\n","                umass = np.log(co_occurrence / word2_count)\n","                pair_umass_scores.append(umass)\n","        if pair_umass_scores:\n","            umass_scores.append(np.mean(pair_umass_scores))\n","\n","    return np.mean(umass_scores) if umass_scores else float('nan')\n","\n","# 일치도 분석 함수\n","def analyze_agreement(metrics_df):\n","    agreement_results = {}\n","    \n","    # Add debugging information\n","    print(\"DataFrame columns:\", metrics_df.columns)\n","    print(\"DataFrame head:\")\n","    print(metrics_df.head())\n","    \n","    for domain in metrics_df['Domain'].unique():\n","        domain_df = metrics_df[metrics_df['Domain'] == domain]\n","        for model in ['BERTopic', 'VAE', 'LowBERTopic']:\n","            model_df = domain_df[domain_df['Model'] == model]\n","            if len(model_df) == 0:\n","                continue\n","            \n","            # Check if the required columns exist\n","            required_columns = ['Coherence', 'NPMI', 'U_Mass']\n","            if not all(col in model_df.columns for col in required_columns):\n","                print(f\"Warning: Missing required columns for {domain} - {model}\")\n","                continue\n","            \n","            coherence = model_df['Coherence'].values[0]\n","            npmi = model_df['NPMI'].values[0]\n","            umass = model_df['U_Mass'].values[0]\n","            \n","            # 메트릭 간 일치도 계산\n","            metrics = [coherence, npmi, -umass]  # U_Mass는 낮을수록 좋으므로 부호를 바꿈\n","            agreement = np.std(metrics) / np.mean(metrics)  # 변동계수 (Coefficient of Variation)\n","            \n","            if domain not in agreement_results:\n","                agreement_results[domain] = {}\n","            agreement_results[domain][model] = {\n","                'Coherence': coherence,\n","                'NPMI': npmi,\n","                'U_Mass': umass,\n","                'Agreement': agreement\n","            }\n","    \n","    return agreement_results\n","\n","# 안정성 분석 함수\n","def analyze_stability(datasets, model_types, n_runs=10, sample_ratio=0.8):\n","    stability_results = []\n","    \n","    for domain, domain_datasets in datasets.items():\n","        # 각 도메인에서 첫 번째 데이터셋만 사용\n","        data = next(iter(domain_datasets.values()))\n","        \n","        logging.info(f\"Analyzing stability for domain: {domain}\")\n","        logging.info(f\"Original data type: {type(data)}\")\n","        \n","        # 데이터 형식 확인 및 변환\n","        if isinstance(data, pd.DataFrame):\n","            data = data['text'].tolist() if 'text' in data.columns else data.values.flatten().tolist()\n","        elif isinstance(data, pd.Series):\n","            data = data.tolist()\n","        elif isinstance(data, np.ndarray):\n","            data = data.flatten().tolist()\n","        elif isinstance(data, list):\n","            pass\n","        else:\n","            raise ValueError(f\"Unsupported data format for domain {domain}: {type(data)}\")\n","        \n","        logging.info(f\"Processed data type: {type(data)}\")\n","        logging.info(f\"Sample of processed data: {data[:5]}\")  # 처음 5개 항목 출력\n","        \n","        # BERTopic으로 초기 토픽 수 결정\n","        _, _, num_topics = perform_bertopic_modeling(data)\n","        \n","        for model_type in model_types:\n","            metric_values = {\n","                'Coherence': [],\n","                'NPMI': [],\n","                'U_Mass': []\n","            }\n","            \n","            for _ in range(n_runs):\n","                sampled_data = np.random.choice(data, size=int(len(data) * sample_ratio), replace=False)\n","                sampled_data = sampled_data.tolist()  # numpy array를 리스트로 변환\n","                \n","                logging.info(f\"Sampled data type: {type(sampled_data)}\")\n","                logging.info(f\"Sample of sampled data: {sampled_data[:5]}\")  # 처음 5개 항목 출력\n","                \n","                if model_type == 'BERTopic':\n","                    model, topics, _ = perform_bertopic_modeling(sampled_data)\n","                elif model_type == 'VAE':\n","                    model, topics = perform_vae_topic_modeling(sampled_data, num_topics)\n","                elif model_type == 'LowBERTopic':\n","                    model, topics, _ = perform_lowbertopic_modeling(sampled_data, num_topics)  # num_topics 인자 제거\n","                else:\n","                    raise ValueError(f\"Unsupported model type: {model_type}\")\n","                \n","                if model is None or topics is None:\n","                    logging.warning(f\"Model or topics is None for {model_type} in domain {domain}\")\n","                    continue\n","                \n","                tokenized_data = [simple_preprocess(doc) for doc in sampled_data]\n","                dictionary = Dictionary(tokenized_data)\n","                corpus = [dictionary.doc2bow(text) for text in tokenized_data]\n","                \n","                coherence = calculate_coherence(topics, tokenizer, bert_model)\n","                npmi = calculate_npmi(topics, corpus, dictionary)\n","                umass = calculate_umass(topics, corpus, dictionary)\n","                \n","                metric_values['Coherence'].append(coherence)\n","                metric_values['NPMI'].append(npmi)\n","                metric_values['U_Mass'].append(umass)\n","            \n","            for metric, values in metric_values.items():\n","                cv = np.std(values) / np.mean(values) if np.mean(values) != 0 else float('nan')\n","                stability_results.append({\n","                    'Domain': domain,\n","                    'Model': model_type,\n","                    'Metric': metric,\n","                    'CV': cv\n","                })\n","    \n","    return pd.DataFrame(stability_results)\n","\n","# 개선된 토픽 품질 시각화 함수\n","\n","def visualize_topic_quality(metrics_df):\n","    plt.figure(figsize=(12, 8))\n","    \n","    # Add debugging information\n","    print(\"DataFrame columns:\", metrics_df.columns)\n","    print(\"DataFrame head:\")\n","    print(metrics_df.head())\n","    \n","    # Check if required columns exist\n","    required_columns = ['Model', 'Coherence', 'NPMI']\n","    if not all(col in metrics_df.columns for col in required_columns):\n","        print(f\"Error: Missing required columns. Available columns: {metrics_df.columns}\")\n","        return\n","    \n","    # 모델별로 다른 마커 사용\n","    markers = {'BERTopic': 'o', 'VAE': 's', 'LowBERTopic': '^'}\n","    \n","    for model in ['BERTopic', 'VAE', 'LowBERTopic']:\n","        model_data = metrics_df[metrics_df['Model'] == model]\n","        coherence = model_data['Coherence']\n","        npmi = model_data['NPMI']\n","        plt.scatter(coherence, npmi, label=model, marker=markers[model])\n","    \n","    plt.xlabel('Coherence')\n","    plt.ylabel('NPMI')\n","    plt.title('Topic Quality: Coherence vs NPMI')\n","    plt.legend()\n","    \n","    # 도메인 레이블 추가\n","    for _, row in metrics_df.iterrows():\n","        plt.annotate(row['Domain'], (row['Coherence'], row['NPMI']), xytext=(5, 5), \n","                     textcoords='offset points', fontsize=8)\n","    \n","    plt.tight_layout()\n","    plt.savefig('topic_quality.png')\n","    plt.close()\n","    \n","    logging.info(\"토픽 품질 시각화 완료: topic_quality.png\")\n","\n","\n","def analyze_llm_results(llm_df):\n","    llm_df['LLM_Avg_Score'] = llm_df['LLM_Scores'].apply(lambda scores: np.mean([s for s in scores if s is not None]))\n","    llm_df['LLM_Std_Score'] = llm_df['LLM_Scores'].apply(lambda scores: np.std([s for s in scores if s is not None]))\n","    llm_df['LLM_Median_Score'] = llm_df['LLM_Scores'].apply(lambda scores: np.median([s for s in scores if s is not None]))\n","\n","    print(\"\\nLLM 평가 결과:\")\n","    print(llm_df[['Domain', 'Model', 'LLM_Avg_Score', 'LLM_Std_Score', 'LLM_Median_Score']])\n","\n","def llm_auto_metric_correlation(metrics_df, llm_df):\n","    merged_df = pd.merge(metrics_df, llm_df, on=['Domain', 'Model'])\n","\n","    metric_names = ['Coherence', 'NPMI', 'U_Mass']\n","    for metric in metric_names:\n","        valid_idx = merged_df['LLM_Avg_Score'].notnull()\n","        pearson_corr, p_value_pearson = stats.pearsonr(merged_df.loc[valid_idx, metric], merged_df.loc[valid_idx, 'LLM_Avg_Score'])\n","        spearman_corr, p_value_spearman = stats.spearmanr(merged_df.loc[valid_idx, metric], merged_df.loc[valid_idx, 'LLM_Avg_Score'])\n","        print(f\"\\nLLM 평가 점수와 {metric}의 상관관계:\")\n","        print(f\"Pearson: 상관계수 = {pearson_corr:.4f}, p-value = {p_value_pearson:.4f}\")\n","        print(f\"Spearman: 상관계수 = {spearman_corr:.4f}, p-value = {p_value_spearman:.4f}\")\n","\n","def verify_llm_consistency(topics, documents, n_repeats=5):\n","    all_scores = []\n","    for _ in range(n_repeats):\n","        scores, _ = llm_evaluation(topics, documents)\n","        all_scores.append(scores)\n","    all_scores = np.array(all_scores)\n","    std_scores = np.std(all_scores, axis=0)\n","    avg_std = np.mean(std_scores)\n","    cv_scores = std_scores / np.mean(all_scores, axis=0)\n","    avg_cv = np.mean(cv_scores)\n","    print(f\"\\nLLM 평가의 평균 표준편차: {avg_std:.4f}\")\n","    print(f\"LLM 평가의 평균 변동계수(CV): {avg_cv:.4f}\")\n","\n","def analyze_llm_feedback(llm_df):\n","    all_words = []\n","    for feedbacks in llm_df['LLM_Feedbacks']:\n","        for feedback in feedbacks:\n","            words = feedback.lower().split()\n","            all_words.extend([word for word in words if word not in stop_words])\n","\n","    word_freq = Counter(all_words)\n","    print(\"\\n피드백에서 가장 자주 등장하는 키워드:\")\n","    for word, count in word_freq.most_common(10):\n","        print(f\"{word}: {count}\")\n","\n","    coherence_keywords = ['coherent', 'consistent', 'related', 'connected', 'meaningful']\n","    print(\"\\n일관성 관련 키워드 빈도:\")\n","    for keyword in coherence_keywords:\n","        print(f\"{keyword}: {word_freq[keyword]}\")\n","\n","    positive_keywords = ['good', 'great', 'excellent', 'well', 'clear']\n","    negative_keywords = ['poor', 'bad', 'unclear', 'confusing', 'unrelated']\n","    \n","    positive_count = sum(word_freq[word] for word in positive_keywords)\n","    negative_count = sum(word_freq[word] for word in negative_keywords)\n","    \n","    print(f\"\\n긍정적 피드백 키워드 수: {positive_count}\")\n","    print(f\"부정적 피드백 키워드 수: {negative_count}\")\n","\n","    relationship_keywords = ['related', 'similar', 'overlapping', 'connected', 'distinct']\n","    print(\"\\n토픽 간 관계 관련 키워드 빈도:\")\n","    for keyword in relationship_keywords:\n","        print(f\"{keyword}: {word_freq[keyword]}\")\n","\n","    quality_keywords = ['coherent', 'meaningful', 'interpretable', 'clear', 'specific']\n","    print(\"\\n토픽 품질 관련 키워드 빈도:\")\n","    for keyword in quality_keywords:\n","        print(f\"{keyword}: {word_freq[keyword]}\")\n","\n","    scores = [score for scores in llm_df['LLM_Scores'] for score in scores if score is not None]\n","    print(\"\\n일관성 점수 분포:\")\n","    print(f\"평균: {np.mean(scores):.2f}\")\n","    print(f\"중앙값: {np.median(scores):.2f}\")\n","    print(f\"표준편차: {np.std(scores):.2f}\")\n","    print(f\"최소값: {np.min(scores):.2f}\")\n","    print(f\"최대값: {np.max(scores):.2f}\")\n","\n","    print(\"\\n모델별 평균 일관성 점수:\")\n","    for model in llm_df['Model'].unique():\n","        model_scores = [score for scores, m in zip(llm_df['LLM_Scores'], llm_df['Model']) \n","                        for score in scores if score is not None and m == model]\n","        print(f\"{model}: {np.mean(model_scores):.2f}\")\n","\n","def visualize_llm_results(llm_df):\n","    plt.figure(figsize=(12, 6))\n","    sns.boxplot(x='Model', y='LLM_Avg_Score', data=llm_df)\n","    plt.title('모델별 LLM 평가 점수 분포')\n","    plt.savefig('llm_model_score_distribution.png')\n","    plt.close()\n","\n","    plt.figure(figsize=(12, 6))\n","    sns.scatterplot(x='Model', y='LLM_Avg_Score', data=llm_df)\n","    plt.title('모델별 LLM 평가 점수')\n","    plt.legend()\n","    plt.savefig('llm_model_score.png')\n","    plt.close()\n","\n","\n","def evaluate_coherence_stability(models, domains, datasets, n_runs=5):\n","    stability_results = []\n","    \n","    for model in models:\n","        for domain, data in zip(domains, datasets):\n","            # data가 딕셔너리인 경우 적절히 처리\n","            if isinstance(data, dict):\n","                data = list(data.values())[0]\n","            elif isinstance(data, pd.DataFrame):\n","                data = data['text'].tolist()\n","            elif not isinstance(data, list):\n","                raise ValueError(f\"Unsupported data format for domain {domain}\")\n","\n","            coherence_scores = []\n","            npmi_scores = []\n","            umass_scores = []\n","\n","            for _ in range(n_runs):\n","                # 데이터 샘플링 (예: 80%의 데이터 사용)\n","                sampled_data = np.random.choice(data, size=int(len(data) * 0.8), replace=False)\n","\n","                if model == 'BERTopic':\n","                    _, topics, _ = perform_bertopic_modeling(sampled_data)\n","                elif model == 'VAE':\n","                    _, topics = perform_vae_topic_modeling(sampled_data, num_topics=10)  # num_topics는 적절히 조정\n","                elif model == 'LowBERTopic':\n","                    _, topics, _ = perform_lowbertopic_modeling(sampled_data, num_topics=10)  # num_topics 인자 제거\n","                else:                    \n","                    raise ValueError(f\"Unsupported model type: {model}\")\n","\n","                # 토큰화된 데이터 준비\n","                tokenized_data = [simple_preprocess(doc) for doc in sampled_data]\n","                dictionary = Dictionary(tokenized_data)\n","                corpus = [dictionary.doc2bow(text) for text in tokenized_data]\n","\n","                # 일관성 메트릭 계산\n","                coherence = calculate_coherence(topics, tokenizer, bert_model)\n","                npmi = calculate_npmi(topics, corpus, dictionary)\n","                umass = calculate_umass(topics, corpus, dictionary)\n","\n","                coherence_scores.append(coherence)\n","                npmi_scores.append(npmi)\n","                umass_scores.append(umass)\n","\n","            # 안정성 계산 (변동 계수 사용)\n","            coherence_stability = np.std(coherence_scores) / np.mean(coherence_scores)\n","            npmi_stability = np.std(npmi_scores) / np.mean(npmi_scores)\n","            umass_stability = np.std(umass_scores) / np.mean(umass_scores)\n","\n","            stability_results.append({\n","                'Model': model,\n","                'Domain': domain,\n","                'Coherence_Stability': coherence_stability,\n","                'NPMI_Stability': npmi_stability,\n","                'UMass_Stability': umass_stability,\n","                'Mean_Coherence': np.mean(coherence_scores),\n","                'Mean_NPMI': np.mean(npmi_scores),\n","                'Mean_UMass': np.mean(umass_scores)\n","            })\n","\n","    return pd.DataFrame(stability_results)\n","\n","def print_results(metrics_df, agreement_results, stability_df, stability_results):\n","    logging.info(\"\\n=== 결과 분석 ===\")\n","    \n","    # 모델별 지표 평균 성능 출력\n","    logging.info(\"\\n모델별 지표 평균 성능:\")\n","    models = metrics_df['Model'].unique()\n","    for model in models:\n","        model_metrics = metrics_df[metrics_df['Model'] == model]\n","        mean_metrics = model_metrics[['Coherence', 'NPMI', 'U_Mass']].mean()\n","        logging.info(f\"\\n모델: {model}\")\n","        logging.info(f\"  - Coherence 평균: {mean_metrics['Coherence']:.4f}\")\n","        logging.info(f\"  - NPMI 평균: {mean_metrics['NPMI']:.4f}\")\n","        logging.info(f\"  - U_Mass 평균: {mean_metrics['U_Mass']:.4f}\")\n","    \n","    # 도메인별 지표 평균 성능 출력\n","    logging.info(\"\\n도메인별 지표 평균 성능:\")\n","    domains = metrics_df['Domain'].unique()\n","    for domain in domains:\n","        domain_metrics = metrics_df[metrics_df['Domain'] == domain]\n","        mean_metrics = domain_metrics[['Coherence', 'NPMI', 'U_Mass']].mean()\n","        logging.info(f\"\\n도메인: {domain}\")\n","        logging.info(f\"  - Coherence 평균: {mean_metrics['Coherence']:.4f}\")\n","        logging.info(f\"  - NPMI 평균: {mean_metrics['NPMI']:.4f}\")\n","        logging.info(f\"  - U_Mass 평균: {mean_metrics['U_Mass']:.4f}\")\n","    \n","    # 지표 간 상관관계 분석 결과 출력\n","    logging.info(\"\\n지표 간 일치도 분석 결과:\")\n","    for domain, model_results in agreement_results.items():\n","        logging.info(f\"\\n도메인: {domain}\")\n","        for model, metrics in model_results.items():\n","            logging.info(f\"  모델: {model}\")\n","            for metric, value in metrics.items():\n","                logging.info(f\"    {metric}: {value:.4f}\")\n","    \n","    # 일관성 지표의 안정성 결과 출력\n","    logging.info(\"\\n일관성지표 안정성 개별 결과:\")\n","    logging.info(stability_results)\n","    logging.info(\"\\n일관성지표 안정성 전체 결과:\")\n","    stability_summary = stability_df.groupby(['Model', 'Metric'])['CV'].mean().reset_index()\n","    for _, row in stability_summary.iterrows():\n","        logging.info(f\"모델: {row['Model']}, 지표: {row['Metric']}, CV 평균: {row['CV']:.4f}\")\n","    \n","    logging.info(\"\\n분석 완료. 결과를 확인하고 해석하세요.\")\n","\n","def process_datasets(datasets):\n","    all_metrics = []\n","    bertopic_results = {}\n","    vae_results = {}\n","    lowbertopic_results = {}  # LowBERTopic 결과 저장을 위한 딕셔너리 추가\n","    \n","    for domain, domain_datasets in datasets.items():\n","        for dataset_name, data in domain_datasets.items():\n","            # BERTopic 모델링\n","            bertopic_model, bertopic_topics, num_topics = perform_bertopic_modeling(data)\n","            bertopic_results[domain] = {\n","                'num_topics': num_topics,\n","                'topics': bertopic_topics\n","            }\n","            \n","            # BERTopic 메트릭 계산\n","            bertopic_metrics = process_metrics(domain, 'BERTopic', bertopic_topics, data, [], tokenizer, bert_model)\n","            all_metrics.extend(bertopic_metrics)\n","            \n","            # VAE 모델링 (BERTopic의 토픽 수 사용)\n","            vae_model, vae_topics = perform_vae_topic_modeling(data, num_topics)\n","            vae_results[domain] = {\n","                'num_topics': num_topics,\n","                'topics': vae_topics\n","            }\n","            \n","            # VAE 메트릭 계산\n","            vae_metrics = process_metrics(domain, 'VAE', vae_topics, data, [], tokenizer, bert_model)\n","            all_metrics.extend(vae_metrics)\n","            \n","            # LowBERTopic 모델링 추가\n","            lowbertopic_model, lowbertopic_topics, low_num_topics = perform_lowbertopic_modeling(data, low_num_topics=num_topics)\n","            lowbertopic_results[domain] = {\n","                'num_topics': low_num_topics,\n","                'topics': lowbertopic_topics\n","            }\n","            \n","            # LowBERTopic 메트릭 계산\n","            lowbertopic_metrics = process_metrics(domain, 'LowBERTopic', lowbertopic_topics, data, [], tokenizer, bert_model)\n","            all_metrics.extend(lowbertopic_metrics)\n","    \n","    return all_metrics, bertopic_results, vae_results, lowbertopic_results\n","\n","\n","def main():\n","    load_bert_model()\n","    try:\n","        logging.info(\"데이터셋 로딩 시작\")\n","        datasets = load_all_datasets()\n","        \n","        logging.info(\"토픽 모델링 및 메트릭 계산 시작\")\n","        all_metrics, bertopic_results, vae_results, lowbertopic_results = process_datasets(datasets)\n","        \n","        # 토픽 정보를 저장할 DataFrame 생성\n","        topics_df = pd.DataFrame(columns=['Domain', 'Model', 'Topics'])\n","        \n","        # BERTopic 결과 처리 및 저장\n","        logging.info(\"BERTopic 결과 출력 및 저장\")\n","        for domain, result in bertopic_results.items():\n","            print(f\"\\n도메인: {domain}\")\n","            print(f\"BERTopic 토픽 수: {result['num_topics']}\")\n","            print(\"BERTopic 토픽:\")\n","            for i, topic in enumerate(result['topics']):\n","                print(f\"  토픽 {i+1}: {', '.join(topic[:10])}\")\n","            \n","            # topics_df에 추가\n","            topics_df = pd.concat([topics_df, pd.DataFrame({\n","                'Domain': [domain],\n","                'Model': ['BERTopic'],\n","                'Topics': [result['topics']]\n","            })], ignore_index=True)\n","        \n","        # VAE 결과 처리 및 저장\n","        logging.info(\"\\nVAE 결과 출력 및 저장\")\n","        for domain, result in vae_results.items():\n","            print(f\"\\n도메인: {domain}\")\n","            print(f\"VAE 토픽 수: {result['num_topics']}\")\n","            print(\"VAE 토픽:\")\n","            for i, topic in enumerate(result['topics']):\n","                print(f\"  토픽 {i+1}: {', '.join(topic[:10])}\")\n","            \n","            # topics_df에 추가\n","            topics_df = pd.concat([topics_df, pd.DataFrame({\n","                'Domain': [domain],\n","                'Model': ['VAE'],\n","                'Topics': [result['topics']]\n","            })], ignore_index=True)\n","        \n","        # LowBERTopic 결과 처리 및 저장\n","        logging.info(\"\\nLowBERTopic 결과 출력 및 저장\")\n","        for domain, result in lowbertopic_results.items():\n","            print(f\"\\n도메인: {domain}\")\n","            print(f\"LowBERTopic 토픽 수: {result['num_topics']}\")\n","            print(\"LowBERTopic 토픽:\")\n","            for i, topic in enumerate(result['topics']):\n","                print(f\"  토픽 {i+1}: {', '.join(topic[:10])}\")\n","            \n","            # topics_df에 추가\n","            topics_df = pd.concat([topics_df, pd.DataFrame({\n","                'Domain': [domain],\n","                'Model': ['LowBERTopic'],\n","                'Topics': [result['topics']]\n","            })], ignore_index=True)\n","        \n","        # topics_df를 CSV 파일로 저장\n","        topics_df.to_csv('topics_df.csv', index=False)\n","        logging.info(\"토픽 정보가 topics_df.csv 파일로 저장되었습니다.\")\n","        \n","        logging.info(\"메트릭 분석 시작\")\n","        metrics_df = pd.DataFrame(all_metrics)\n","        \n","        # Add debugging information\n","        print(\"Metrics DataFrame shape:\", metrics_df.shape)\n","        print(\"Metrics DataFrame columns:\", metrics_df.columns)\n","        print(\"Metrics DataFrame head:\")\n","        print(metrics_df.head())\n","        \n","        metrics_df.to_csv('topic_modeling_metrics.csv', index=False)\n","        \n","        logging.info(\"일치도 분석 시작\")\n","        agreement_results = analyze_agreement(metrics_df)\n","        \n","        logging.info(\"안정성 분석 시작\")\n","        stability_df = analyze_stability(datasets, ['BERTopic', 'VAE', 'LowBERTopic'])\n","        \n","        logging.info(\"토픽 품질 시각화 시작\")\n","        visualize_topic_quality(metrics_df)\n","        \n","        logging.info(\"일관성 안정성 평가 시작\")\n","        stability_results = evaluate_coherence_stability(['BERTopic', 'VAE', 'LowBERTopic'], list(datasets.keys()), list(datasets.values()))\n","        \n","        logging.info(\"결과 출력 시작\")\n","        print_results(metrics_df, agreement_results, stability_df, stability_results)\n","        \n","        logging.info(\"모든 분석 완료\")\n","    except Exception as e:\n","        logging.error(f\"메인 함수 실행 중 예상치 못한 오류 발생: {e}\")\n","        raise\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Cell 2: LLM 평가 관련 함수와 실행 코드\n","import anthropic\n","from tenacity import retry, stop_after_attempt, wait_random_exponential\n","\n","@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(3))\n","def call_anthropic_api(prompt: str, max_tokens_to_sample: int = 3000) -> str:\n","    anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n","    if not anthropic_api_key:\n","        raise ValueError(\"Anthropic API key not found in environment variable ANTHROPIC_API_KEY\")\n","    \n","    client = anthropic.Anthropic(api_key=anthropic_api_key)\n","    \n","    try:\n","        logging.info(\"Calling Anthropic API...\")\n","        response = client.completions.create(\n","            model=\"claude-2\",\n","            prompt=f\"{anthropic.HUMAN_PROMPT} {prompt} {anthropic.AI_PROMPT}\",\n","            max_tokens_to_sample=max_tokens_to_sample,\n","            temperature=0,\n","        )\n","        logging.info(\"Anthropic API call successful\")\n","        return response.completion\n","    except Exception as e:\n","        logging.error(f\"Error in Anthropic API call: {e}\")\n","        logging.error(f\"Client object: {client}\")\n","        logging.error(f\"Prompt: {prompt[:100]}...\")  # 처음 100자만 로깅\n","        raise\n","\n","\n","def llm_evaluation(topics):\n","    scores = []\n","    feedbacks = []\n","\n","    prompt = f\"\"\"\n","    Evaluate the following topics based on their coherence. Coherence is an important metric for assessing the quality of topic modeling:\n","\n","    1. Coherence measures how semantically related the words within each topic are.\n","    2. It is typically calculated by considering the co-occurrence probabilities of word pairs within the topic.\n","    3. Higher coherence scores indicate that the words in a topic are closely related and form a meaningful theme.\n","    4. Lower coherence scores suggest that the topic may be less meaningful or coherent.\n","\n","    Please evaluate the following topics. For each topic, provide a coherence score on a scale of 1-10 and explain your reasoning:\n","\n","    {topics}\n","\n","    When evaluating, consider:\n","    1. How semantically related are the words within each topic?\n","    2. How clear and interpretable is the topic?\n","    3. Do the words in the topic represent a consistent theme or concept?\n","\n","    Please respond for each topic in the following format:\n","    Topic X: [score]\n","    Reason: [explanation]\n","    \"\"\"\n","\n","    try:\n","        evaluation = call_anthropic_api(prompt)\n","\n","        # Updated parsing logic to extract structured responses\n","        topic_evaluations = re.findall(r\"Topic \\d+:.*?(?=Topic \\d+:|$)\", evaluation, re.DOTALL)\n","        for eval in topic_evaluations:\n","            score_match = re.search(r'Topic (\\d+):\\s*(\\d+)', eval)\n","            reason_match = re.search(r'Reason:\\s*(.*)', eval, re.DOTALL)\n","            if score_match and reason_match:\n","                topic_score = int(score_match.group(2))\n","                if 1 <= topic_score <= 10:\n","                    scores.append(topic_score)\n","                    feedbacks.append(reason_match.group(1).strip())\n","                else:\n","                    print(f\"Invalid score (not between 1 and 10): {eval}\")\n","            else:\n","                print(f\"Could not extract score or reason: {eval}\")\n","\n","    except Exception as e:\n","        print(f\"Unexpected error: {e}\")\n","        raise\n","\n","    return scores, feedbacks\n","\n","def run_llm_evaluation(sample_size=100, chunk_size=10):\n","    topics_df = pd.read_csv('topics_df.csv')\n","    llm_results = []\n","    actual_sample_size = min(sample_size, len(topics_df))\n","    \n","    for index, row in tqdm(topics_df.sample(n=actual_sample_size, random_state=42).iterrows(), total=actual_sample_size):\n","        domain = row['Domain']\n","        model_type = row['Model']\n","        topics = eval(row['Topics'])  # 문자열을 리스트로 변환\n","        \n","        logging.info(f\"LLM 평가 진행 중 - 도메인: {domain}, 모델: {model_type}\")\n","\n","        try:\n","            scores, feedbacks = llm_evaluation(topics)  # documents 인자 제거\n","\n","            result = {\n","                'Domain': domain,\n","                'Model': model_type,\n","                'LLM_Scores': scores,\n","                'LLM_Feedbacks': feedbacks\n","            }\n","            llm_results.append(result)\n","\n","            if len(llm_results) % chunk_size == 0:\n","                save_results_chunk(llm_results[-chunk_size:])\n","                \n","        except Exception as e:\n","            logging.error(f\"Error processing {domain} - {model_type}: {str(e)}\")\n","            continue\n","\n","    if len(llm_results) % chunk_size != 0:\n","        save_results_chunk(llm_results[-(len(llm_results) % chunk_size):])\n","\n","    llm_df = pd.DataFrame(llm_results)\n","    return llm_df\n","\n","def save_results_chunk(results_chunk):\n","    with open('llm_evaluation_results.json', 'a') as f:\n","        for result in results_chunk:\n","            json.dump(result, f)\n","            f.write('\\n')\n","\n","# LLM 평가 실행\n","logging.info(\"LLM 평가 시작\")\n","metrics_df = pd.read_csv('topic_modeling_metrics.csv')\n","\n","# topics_df 인자 제거\n","llm_df = run_llm_evaluation()\n","analyze_llm_results(llm_df)\n","visualize_llm_results(llm_df)\n","\n","logging.info(\"LLM 평가와 자동 메트릭 상관관계 분석 시작\")\n","llm_auto_metric_correlation(metrics_df, llm_df)\n","\n","logging.info(\"LLM 피드백 분석 시작\")\n","analyze_llm_feedback(llm_df)\n","\n","logging.info(\"LLM 평가 완료\")"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyN1HMc8DJjEA0c4VoVpICAu","gpuType":"T4","mount_file_id":"1A2KBLTvWLDpZRfvqTW6X-K99HG5QCvQ-","provenance":[]},"kernelspec":{"display_name":"topic","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}
