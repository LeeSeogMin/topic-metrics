{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Cell 1: Model execution, evaluation metric execution, and other result analysis\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from gensim import models, corpora\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.preprocessing import MinMaxScaler\n","from gensim.models.coherencemodel import CoherenceModel\n","import time\n","import json\n","from nltk.corpus import stopwords\n","from math import log\n","from itertools import combinations\n","from tqdm import tqdm\n","import logging\n","from collections import Counter, defaultdict\n","import gensim\n","from gensim import corpora\n","from scipy.sparse import csr_matrix\n","from gensim.utils import simple_preprocess\n","from gensim.corpora import Dictionary\n","from transformers import BertTokenizer, BertModel\n","from bertopic import BERTopic\n","import seaborn as sns\n","from scipy import stats\n","import os\n","import re\n","import matplotlib\n","from tabulate import tabulate\n","matplotlib.use('Agg')\n","import matplotlib.pyplot as plt\n","from sklearn.manifold import MDS\n","from sklearn.preprocessing import MinMaxScaler\n","from torch.utils.data import DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.cluster import KMeans\n","\n","# NLTK 데이터 다운로드\n","import nltk\n","nltk.download('punkt', quiet=True)\n","nltk.download('stopwords', quiet=True)\n","\n","# 로깅 설정\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","\n","# stop_words 정의\n","stop_words = set(stopwords.words('english'))\n","\n","# 전역 변수로 BERT 모델과 토크나이저 선언\n","global tokenizer, bert_model\n","\n","# BERT 모델 로딩 함수\n","def load_bert_model():\n","    global tokenizer, bert_model\n","    if 'tokenizer' not in globals() or 'bert_model' not in globals():\n","        logging.info(\"Loading BERT model and tokenizer...\")\n","        try:\n","            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","            bert_model = BertModel.from_pretrained('bert-base-uncased')\n","            \n","            # Check GPU availability and set device\n","            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","            bert_model = bert_model.to(device)\n","            \n","            logging.info(f\"BERT model and tokenizer loaded. Using device: {device}\")\n","        except Exception as e:\n","            logging.error(f\"Error loading BERT model: {e}\")\n","            raise\n","\n","def load_data(file_path, sample_size=5000):\n","    try:\n","        df = pd.read_csv(file_path, header=None, names=['text'])\n","    except FileNotFoundError:\n","        logging.error(f\"File not found: {file_path}\")\n","        return []\n","    except Exception as e:\n","        logging.error(f\"Error loading file {file_path}: {e}\")\n","        return []\n","    texts = df['text'].astype(str)\n","    if len(texts) > sample_size:\n","        texts = texts.sample(n=sample_size, random_state=42)\n","    print(f\"Loaded {len(texts)} texts from {file_path}\")\n","    return texts.tolist()\n","\n","def load_all_datasets():\n","    datasets = {\n","        'academy': {\n","            'covid': load_data('data/academy/covid.csv')\n","        },\n","        'media': {\n","            'clothing_review': load_data('data/media/clothing_review.csv')\n","        },\n","        'news': {\n","            'agnews': load_data('data/news/agnews.csv')\n","        }\n","    }\n","    return datasets\n","\n","class VAE(nn.Module):\n","    def __init__(self, input_dim, hidden_dim=50, latent_dim=None):\n","        if latent_dim is None:\n","            raise ValueError(\"latent_dim must be specified\")\n","        super(VAE, self).__init__()\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        self.fc21 = nn.Linear(hidden_dim, latent_dim)  \n","        self.fc22 = nn.Linear(hidden_dim, latent_dim)  \n","        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n","        self.fc4 = nn.Linear(hidden_dim, input_dim)\n","\n","    def encode(self, x):\n","        h1 = F.relu(self.fc1(x))\n","        return self.fc21(h1), self.fc22(h1)\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5 * logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps * std\n","\n","    def decode(self, z):\n","        h3 = F.relu(self.fc3(z))\n","        return torch.sigmoid(self.fc4(h3))\n","\n","    def forward(self, x):\n","        mu, logvar = self.encode(x)\n","        z = self.reparameterize(mu, logvar)\n","        return self.decode(z), mu, logvar\n","\n","def vae_loss(recon_x, x, mu, logvar):\n","    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n","    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    return BCE + KLD\n","\n","def extract_vae_topics(vae_model, vectorizer, num_topics, top_n=10):\n","    with torch.no_grad():\n","        latent_vectors = torch.eye(num_topics).to(vae_model.fc3.weight.device)\n","        decoder_output = vae_model.decode(latent_vectors)\n","        decoder_output = decoder_output.cpu().numpy()\n","\n","    feature_names = vectorizer.get_feature_names_out()\n","    topics = []\n","    for topic_distribution in decoder_output:\n","        top_indices = topic_distribution.argsort()[-top_n:][::-1]\n","        topic_words = [feature_names[i] for i in top_indices]\n","        topics.append(topic_words)\n","    return topics\n","\n","def perform_bertopic_modeling(data):\n","    if isinstance(data, dict):\n","        data = list(data.values())[0]\n","    elif isinstance(data, pd.DataFrame):\n","        data = data['text'].tolist() if 'text' in data.columns else data.values.flatten().tolist()\n","    elif isinstance(data, pd.Series):\n","        data = data.tolist()\n","    elif isinstance(data, np.ndarray):\n","        data = data.flatten().tolist()\n","    elif isinstance(data, list):\n","        pass\n","    else:\n","        raise ValueError(f\"Unsupported data format for BERTopic modeling: {type(data)}\")\n","\n","    # 데이터가 문자열 리스트인지 확인\n","    if not all(isinstance(item, str) for item in data):\n","        raise ValueError(\"All items in the data must be strings\")\n","\n","    try:\n","        bertopic_model = BERTopic(language=\"english\", calculate_probabilities=True)\n","        topics, _ = bertopic_model.fit_transform(data)\n","        \n","        num_topics = len(bertopic_model.get_topics())\n","        topic_words = []\n","        for i in range(num_topics):\n","            topic = bertopic_model.get_topic(i)\n","            if topic:\n","                words = [word for word, _ in topic[:10]]  # 상위 10개 단어만 추출\n","                topic_words.append(words)\n","        \n","        return bertopic_model, topic_words, num_topics\n","    except AttributeError as e:\n","        logging.error(f\"Error in BERTopic modeling: {e}\")\n","        return None, None, None\n","    \n","\n","def perform_lowbertopic_modeling(data, low_num_topics):\n","    # 토픽 수를 num_topics로 설정 (기본값 5)\n","    low_num_topics = 5\n","    \n","    # 데이터 전처리\n","    if isinstance(data, dict):\n","        data = list(data.values())[0]\n","    elif isinstance(data, pd.DataFrame):\n","        data = data['text'].tolist() if 'text' in data.columns else data.values.flatten().tolist()\n","    elif isinstance(data, pd.Series):\n","        data = data.tolist()\n","    elif isinstance(data, np.ndarray):\n","        data = data.flatten().tolist()\n","    elif isinstance(data, list):\n","        pass\n","    else:\n","        raise ValueError(f\"Unsupported data format for BERTopic modeling: {type(data)}\")\n","\n","    # 데이터가 문자열 리스트인지 확인\n","    if not all(isinstance(item, str) for item in data):\n","        raise ValueError(\"All items in the data must be strings\")\n","\n","    try:\n","        \n","        # BERTopic 모델 생성\n","        lowbertopic_model = BERTopic(language=\"english\", calculate_probabilities=True, nr_topics=low_num_topics)\n","        \n","        # 모델 학습\n","        topics, _ = lowbertopic_model.fit_transform(data)\n","        \n","        # 토픽 단어 추출\n","        topic_words = []\n","        for i in range(low_num_topics):\n","            topic = lowbertopic_model.get_topic(i)\n","            if topic:\n","                words = [word for word, _ in topic[:10]]  # 상위 10개 단어 추출\n","                topic_words.append(words)\n","        \n","        return lowbertopic_model, topic_words, low_num_topics\n","    except Exception as e:\n","        logging.error(f\"Error in LowBERTopic modeling: {e}\")\n","        return None, None, None\n","\n","\n","def perform_vae_topic_modeling(data, num_topics, num_epochs=5, hidden_dim=50):\n","    try:\n","        # 데이터 전처리\n","        data = [str(doc) for doc in data if isinstance(doc, str) and len(doc) > 0]\n","        vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n","        doc_term_matrix = vectorizer.fit_transform(data)\n","\n","        # MinMaxScaler를 사용하여 0-1 사이로 정규화\n","        scaler = MinMaxScaler()\n","        normalized_matrix = scaler.fit_transform(doc_term_matrix.toarray())\n","\n","        # VAE 모델 초기화 및 학습\n","        input_dim = doc_term_matrix.shape[1]\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        vae_model = VAE(input_dim=input_dim, hidden_dim=hidden_dim, latent_dim=num_topics).to(device)\n","        optimizer = torch.optim.Adam(vae_model.parameters(), lr=1e-3)\n","\n","        batch_size = 64\n","        data_loader = DataLoader(normalized_matrix.astype(np.float32), batch_size=batch_size, shuffle=True)\n","\n","        vae_model.train()\n","        for epoch in range(num_epochs):\n","            train_loss = 0\n","            for batch in data_loader:\n","                batch = batch.to(device)\n","                optimizer.zero_grad()\n","                recon_batch, mu, logvar = vae_model(batch)\n","                loss = vae_loss(recon_batch, batch, mu, logvar)\n","                loss.backward()\n","                optimizer.step()\n","                train_loss += loss.item()\n","            logging.info(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss / len(data_loader.dataset):.4f}\")\n","\n","        topics = extract_vae_topics(vae_model, vectorizer, num_topics)\n","        return vae_model, topics\n","    except Exception as e:\n","        logging.error(f\"Error in VAE modeling: {e}\")\n","        return None, None\n","\n","def calculate_coherence(topics, tokenizer, bert_model, top_n=10, batch_size=16):\n","    coherence_scores = []\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    bert_model.to(device)\n","    bert_model.eval()\n","\n","    for topic_words in topics:\n","        # 토픽 단어 수 제한\n","        topic_words = topic_words[:top_n]\n","        # 유효한 단어만 선택\n","        topic_words = [word for word in topic_words if word and isinstance(word, str)]\n","        num_words = len(topic_words)\n","\n","        if num_words < 2:\n","            coherence_scores.append(0)\n","            continue\n","\n","        # 단어 임베딩을 배치로 계산\n","        embeddings = []\n","        for i in range(0, num_words, batch_size):\n","            batch_words = topic_words[i:i + batch_size]\n","            inputs = tokenizer(batch_words, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","            with torch.no_grad():\n","                outputs = bert_model(**inputs)\n","            batch_embeddings = outputs.last_hidden_state[:, 0, :]  # [CLS] 토큰의 임베딩 사용\n","            embeddings.append(batch_embeddings)\n","            # 메모리 관리\n","            del outputs\n","            torch.cuda.empty_cache()\n","\n","        embeddings = torch.cat(embeddings, dim=0)\n","\n","        # 코사인 유사도 계산\n","        pairwise_similarities = []\n","        for i in range(num_words):\n","            for j in range(i + 1, num_words):\n","                cosine_sim = torch.nn.functional.cosine_similarity(embeddings[i], embeddings[j], dim=0)\n","                pairwise_similarities.append(cosine_sim.item())\n","\n","        coherence = np.mean(pairwise_similarities)\n","        coherence_scores.append(coherence)\n","\n","    final_coherence = np.mean(coherence_scores) if coherence_scores else 0\n","    return final_coherence\n","\n","\n","def process_metrics(domain, model_type, topics, data, metrics_list, tokenizer, bert_model):\n","    tokenized_data = [simple_preprocess(doc) for doc in data]\n","    dictionary = Dictionary(tokenized_data)\n","    corpus = [dictionary.doc2bow(text) for text in tokenized_data]\n","\n","    coherence = calculate_coherence(topics, tokenizer, bert_model)\n","    npmi = calculate_npmi(topics, corpus, dictionary)\n","    umass = calculate_umass(topics, corpus, dictionary)\n","\n","    metrics_list.append({\n","        'Domain': domain,\n","        'Model': model_type,\n","        'Coherence': coherence,\n","        'NPMI': npmi,\n","        'U_Mass': umass\n","    })\n","\n","    logging.info(f\"Coherence: {coherence:.4f}, NPMI: {npmi:.4f}, U_Mass: {umass:.4f}\")\n","    \n","    return [metrics_list[-1]]  # Return the last added metric as a list\n","\n","def calculate_npmi(topics, corpus, dictionary, top_n=10):\n","    # Create a set of all words used in topics\n","    topic_words_set = set()\n","    for topic in topics:\n","        topic_words_set.update(topic[:top_n])\n","\n","    # Map words to IDs\n","    word2id = {word: dictionary.token2id[word] for word in topic_words_set if word in dictionary.token2id}\n","    id2word = {id: word for word, id in word2id.items()}\n","\n","    # Calculate word and word pair document frequencies\n","    total_docs = len(corpus)\n","    word_doc_freq = defaultdict(int)\n","    pair_doc_freq = defaultdict(int)\n","\n","    for doc in corpus:\n","        doc_word_ids = set([id for id, _ in doc])     \n","        topic_word_ids_in_doc = doc_word_ids.intersection(set(word2id.values()))\n","\n","        for word_id in topic_word_ids_in_doc:\n","            word_doc_freq[word_id] += 1\n","\n","        for word_id1, word_id2 in combinations(topic_word_ids_in_doc, 2):\n","            pair = tuple(sorted((word_id1, word_id2)))\n","            pair_doc_freq[pair] += 1\n","\n","    # Calculate NPMI\n","    npmi_scores = []\n","    for topic in topics:\n","        topic_word_ids = [word2id[word] for word in topic[:top_n] if word in word2id]\n","        if len(topic_word_ids) < 2:\n","            continue\n","        pair_npmi_scores = []\n","        for word_id1, word_id2 in combinations(topic_word_ids, 2):\n","            pair = tuple(sorted((word_id1, word_id2)))\n","            co_doc_count = pair_doc_freq.get(pair, 0)\n","            if co_doc_count == 0:\n","                continue\n","            p_w1_w2 = co_doc_count / total_docs\n","            p_w1 = word_doc_freq[word_id1] / total_docs\n","            p_w2 = word_doc_freq[word_id2] / total_docs\n","\n","            pmi = np.log(p_w1_w2 / (p_w1 * p_w2) + 1e-12)\n","            npmi = pmi / (-np.log(p_w1_w2 + 1e-12))\n","            pair_npmi_scores.append(npmi)\n","        if pair_npmi_scores:\n","            npmi_scores.append(np.mean(pair_npmi_scores))\n","\n","    return np.mean(npmi_scores) if npmi_scores else float('nan')\n","\n","def calculate_umass(topics, corpus, dictionary, top_n=10):\n","    # Create a set of all words used in topics\n","    topic_words_set = set()\n","    for topic in topics:\n","        topic_words_set.update(topic[:top_n])\n","\n","    # Map words to IDs\n","    word2id = {word: dictionary.token2id[word] for word in topic_words_set if word in dictionary.token2id}\n","\n","    # Calculate word and word pair frequencies\n","    word_counts = defaultdict(int)\n","    pair_counts = defaultdict(int)\n","\n","    for doc in corpus:\n","        doc_word_ids = set([id for id, _ in doc])\n","        topic_word_ids_in_doc = doc_word_ids.intersection(set(word2id.values()))\n","\n","        for word_id in topic_word_ids_in_doc:\n","            word_counts[word_id] += 1\n","\n","        for word_id1, word_id2 in combinations(topic_word_ids_in_doc, 2):\n","            pair = tuple(sorted((word_id1, word_id2)))\n","            pair_counts[pair] += 1\n","\n","    # Calculate U_Mass\n","    umass_scores = []\n","    for topic in topics:\n","        topic_word_ids = [word2id[word] for word in topic[:top_n] if word in word2id]\n","        if len(topic_word_ids) < 2:\n","            continue\n","        pair_umass_scores = []\n","        for i, word_id1 in enumerate(topic_word_ids):\n","            for word_id2 in topic_word_ids[:i]:\n","                pair = tuple(sorted((word_id1, word_id2)))\n","                co_occurrence = pair_counts.get(pair, 0) + 1  # Add 1 for smoothing\n","                word2_count = word_counts[word_id2] + 1  # Add 1 for smoothing\n","                umass = np.log(co_occurrence / word2_count)\n","                pair_umass_scores.append(umass)\n","        if pair_umass_scores:\n","            umass_scores.append(np.mean(pair_umass_scores))\n","\n","    return np.mean(umass_scores) if umass_scores else float('nan')\n","\n","# 일치도 분석 함수\n","def analyze_agreement(metrics_df):\n","    agreement_results = {}\n","    \n","    # Add debugging information\n","    print(\"DataFrame columns:\", metrics_df.columns)\n","    print(\"DataFrame head:\")\n","    print(metrics_df.head())\n","    \n","    for domain in metrics_df['Domain'].unique():\n","        domain_df = metrics_df[metrics_df['Domain'] == domain]\n","        for model in ['BERTopic', 'VAE', 'LowBERTopic']:\n","            model_df = domain_df[domain_df['Model'] == model]\n","            if len(model_df) == 0:\n","                continue\n","            \n","            # Check if the required columns exist\n","            required_columns = ['Coherence', 'NPMI', 'U_Mass']\n","            if not all(col in model_df.columns for col in required_columns):\n","                print(f\"Warning: Missing required columns for {domain} - {model}\")\n","                continue\n","            \n","            coherence = model_df['Coherence'].values[0]\n","            npmi = model_df['NPMI'].values[0]\n","            umass = model_df['U_Mass'].values[0]\n","            \n","            # Calculate agreement between metrics\n","            metrics = [coherence, npmi, -umass]  # Invert U_Mass as lower is better\n","            agreement = np.std(metrics) / np.mean(metrics)  # Coefficient of Variation\n","            \n","            if domain not in agreement_results:\n","                agreement_results[domain] = {}\n","            agreement_results[domain][model] = {\n","                'Coherence': coherence,\n","                'NPMI': npmi,\n","                'U_Mass': umass,\n","                'Agreement': agreement\n","            }\n","    \n","    return agreement_results\n","\n","# 안정성 분석 함수\n","def analyze_stability(datasets, model_types, n_runs=10, sample_ratio=0.8):\n","    stability_results = []\n","    \n","    for domain, domain_datasets in datasets.items():\n","        # 각 도메인에서 첫 번째 데이터셋만 사용\n","        data = next(iter(domain_datasets.values()))\n","        \n","        logging.info(f\"Analyzing stability for domain: {domain}\")\n","        logging.info(f\"Original data type: {type(data)}\")\n","        \n","        # 데이터 형식 확인 및 변환\n","        if isinstance(data, pd.DataFrame):\n","            data = data['text'].tolist() if 'text' in data.columns else data.values.flatten().tolist()\n","        elif isinstance(data, pd.Series):\n","            data = data.tolist()\n","        elif isinstance(data, np.ndarray):\n","            data = data.flatten().tolist()\n","        elif isinstance(data, list):\n","            pass\n","        else:\n","            raise ValueError(f\"Unsupported data format for domain {domain}: {type(data)}\")\n","        \n","        logging.info(f\"Processed data type: {type(data)}\")\n","        logging.info(f\"Sample of processed data: {data[:5]}\")  # 처음 5개 항목 출력\n","        \n","        # BERTopic으로 초기 토픽 수 결정\n","        _, _, num_topics = perform_bertopic_modeling(data)\n","        \n","        for model_type in model_types:\n","            metric_values = {\n","                'Coherence': [],\n","                'NPMI': [],\n","                'U_Mass': []\n","            }\n","            \n","            for _ in range(n_runs):\n","                sampled_data = np.random.choice(data, size=int(len(data) * sample_ratio), replace=False)\n","                sampled_data = sampled_data.tolist()  # numpy array를 리스트로 변환\n","                \n","                logging.info(f\"Sampled data type: {type(sampled_data)}\")\n","                logging.info(f\"Sample of sampled data: {sampled_data[:5]}\")  # 처음 5개 항목 출력\n","                \n","                if model_type == 'BERTopic':\n","                    model, topics, _ = perform_bertopic_modeling(sampled_data)\n","                elif model_type == 'VAE':\n","                    model, topics = perform_vae_topic_modeling(sampled_data, num_topics)\n","                elif model_type == 'LowBERTopic':\n","                    model, topics, _ = perform_lowbertopic_modeling(sampled_data, num_topics)  # num_topics 인자 제거\n","                else:\n","                    raise ValueError(f\"Unsupported model type: {model_type}\")\n","                \n","                if model is None or topics is None:\n","                    logging.warning(f\"Model or topics is None for {model_type} in domain {domain}\")\n","                    continue\n","                \n","                tokenized_data = [simple_preprocess(doc) for doc in sampled_data]\n","                dictionary = Dictionary(tokenized_data)\n","                corpus = [dictionary.doc2bow(text) for text in tokenized_data]\n","                \n","                coherence = calculate_coherence(topics, tokenizer, bert_model)\n","                npmi = calculate_npmi(topics, corpus, dictionary)\n","                umass = calculate_umass(topics, corpus, dictionary)\n","                \n","                metric_values['Coherence'].append(coherence)\n","                metric_values['NPMI'].append(npmi)\n","                metric_values['U_Mass'].append(umass)\n","            \n","            for metric, values in metric_values.items():\n","                cv = np.std(values) / np.mean(values) if np.mean(values) != 0 else float('nan')\n","                stability_results.append({\n","                    'Domain': domain,\n","                    'Model': model_type,\n","                    'Metric': metric,\n","                    'CV': cv\n","                })\n","    \n","    return pd.DataFrame(stability_results)\n","\n","# 개선된 토픽 품질 시각화 함수\n","\n","def visualize_topic_quality(metrics_df):\n","    plt.figure(figsize=(12, 8))\n","    \n","    # Add debugging information\n","    print(\"DataFrame columns:\", metrics_df.columns)\n","    print(\"DataFrame head:\")\n","    print(metrics_df.head())\n","    \n","    # Check if required columns exist\n","    required_columns = ['Model', 'Coherence', 'NPMI']\n","    if not all(col in metrics_df.columns for col in required_columns):\n","        print(f\"Error: Missing required columns. Available columns: {metrics_df.columns}\")\n","        return\n","    \n","    # Use different markers for each model\n","    markers = {'BERTopic': 'o', 'VAE': 's', 'LowBERTopic': '^'}\n","    \n","    for model in ['BERTopic', 'VAE', 'LowBERTopic']:\n","        model_data = metrics_df[metrics_df['Model'] == model]\n","        coherence = model_data['Coherence']\n","        npmi = model_data['NPMI']\n","        plt.scatter(coherence, npmi, label=model, marker=markers[model])\n","    \n","    plt.xlabel('Coherence')\n","    plt.ylabel('NPMI')\n","    plt.title('Topic Quality: Coherence vs NPMI')\n","    plt.legend()\n","    \n","    # Add domain labels\n","    for _, row in metrics_df.iterrows():\n","        plt.annotate(row['Domain'], (row['Coherence'], row['NPMI']), xytext=(5, 5), \n","                     textcoords='offset points', fontsize=8)\n","    \n","    plt.tight_layout()\n","    plt.savefig('topic_quality.png')\n","    plt.close()\n","    \n","    logging.info(\"Topic quality visualization completed: topic_quality.png\")\n","\n","\n","def analyze_llm_results(llm_df):\n","    llm_df['LLM_Avg_Score'] = llm_df['LLM_Scores'].apply(lambda scores: np.mean([s for s in scores if s is not None]))\n","    llm_df['LLM_Std_Score'] = llm_df['LLM_Scores'].apply(lambda scores: np.std([s for s in scores if s is not None]))\n","    llm_df['LLM_Median_Score'] = llm_df['LLM_Scores'].apply(lambda scores: np.median([s for s in scores if s is not None]))\n","\n","    print(\"\\nLLM Evaluation Results:\")\n","    print(llm_df[['Domain', 'Model', 'LLM_Avg_Score', 'LLM_Std_Score', 'LLM_Median_Score']])\n","\n","def llm_auto_metric_correlation(metrics_df, llm_df):\n","    merged_df = pd.merge(metrics_df, llm_df, on=['Domain', 'Model'])\n","\n","    metric_names = ['Coherence', 'NPMI', 'U_Mass']\n","    for metric in metric_names:\n","        valid_idx = merged_df['LLM_Avg_Score'].notnull()\n","        pearson_corr, p_value_pearson = stats.pearsonr(merged_df.loc[valid_idx, metric], merged_df.loc[valid_idx, 'LLM_Avg_Score'])\n","        spearman_corr, p_value_spearman = stats.spearmanr(merged_df.loc[valid_idx, metric], merged_df.loc[valid_idx, 'LLM_Avg_Score'])\n","        print(f\"\\nCorrelation between LLM evaluation scores and {metric}:\")\n","        print(f\"Pearson: correlation coefficient = {pearson_corr:.4f}, p-value = {p_value_pearson:.4f}\")\n","        print(f\"Spearman: correlation coefficient = {spearman_corr:.4f}, p-value = {p_value_spearman:.4f}\")\n","\n","def verify_llm_consistency(topics, documents, n_repeats=5):\n","    all_scores = []\n","    for _ in range(n_repeats):\n","        scores, _ = llm_evaluation(topics, documents)\n","        all_scores.append(scores)\n","    all_scores = np.array(all_scores)\n","    std_scores = np.std(all_scores, axis=0)\n","    avg_std = np.mean(std_scores)\n","    cv_scores = std_scores / np.mean(all_scores, axis=0)\n","    avg_cv = np.mean(cv_scores)\n","    print(f\"\\nAverage standard deviation of LLM evaluation: {avg_std:.4f}\")\n","    print(f\"Average coefficient of variation (CV) of LLM evaluation: {avg_cv:.4f}\")\n","\n","def analyze_llm_feedback(llm_df):\n","    all_words = []\n","    for feedbacks in llm_df['LLM_Feedbacks']:\n","        for feedback in feedbacks:\n","            words = feedback.lower().split()\n","            all_words.extend([word for word in words if word not in stop_words])\n","\n","    word_freq = Counter(all_words)\n","    print(\"\\nMost frequent keywords in feedback:\")\n","    for word, count in word_freq.most_common(10):\n","        print(f\"{word}: {count}\")\n","\n","    coherence_keywords = ['coherent', 'consistent', 'related', 'connected', 'meaningful']\n","    print(\"\\nFrequency of coherence-related keywords:\")\n","    for keyword in coherence_keywords:\n","        print(f\"{keyword}: {word_freq[keyword]}\")\n","\n","    positive_keywords = ['good', 'great', 'excellent', 'well', 'clear']\n","    negative_keywords = ['poor', 'bad', 'unclear', 'confusing', 'unrelated']\n","    \n","    positive_count = sum(word_freq[word] for word in positive_keywords)\n","    negative_count = sum(word_freq[word] for word in negative_keywords)\n","    \n","    print(f\"\\nNumber of positive feedback keywords: {positive_count}\")\n","    print(f\"Number of negative feedback keywords: {negative_count}\")\n","\n","    relationship_keywords = ['related', 'similar', 'overlapping', 'connected', 'distinct']          \n","              \n","    print(\"\\nFrequency of topic relationship keywords:\")\n","    for keyword in relationship_keywords:\n","        print(f\"{keyword}: {word_freq[keyword]}\")\n","\n","    quality_keywords = ['coherent', 'meaningful', 'interpretable', 'clear', 'specific']\n","    print(\"\\nFrequency of topic quality keywords:\")\n","    for keyword in quality_keywords:\n","        print(f\"{keyword}: {word_freq[keyword]}\")\n","\n","    scores = [score for scores in llm_df['LLM_Scores'] for score in scores if score is not None]\n","    print(\"\\nDistribution of coherence scores:\")\n","    print(f\"Mean: {np.mean(scores):.2f}\")\n","    print(f\"Median: {np.median(scores):.2f}\")\n","    print(f\"Standard deviation: {np.std(scores):.2f}\")\n","    print(f\"Minimum: {np.min(scores):.2f}\")\n","    print(f\"Maximum: {np.max(scores):.2f}\")\n","\n","    print(\"\\nAverage coherence score by model:\")\n","    for model in llm_df['Model'].unique():\n","        model_scores = [score for scores, m in zip(llm_df['LLM_Scores'], llm_df['Model']) \n","                        for score in scores if score is not None and m == model]\n","        print(f\"{model}: {np.mean(model_scores):.2f}\")\n","\n","def visualize_llm_results(llm_df):\n","    plt.figure(figsize=(12, 6))\n","    sns.boxplot(x='Model', y='LLM_Avg_Score', data=llm_df)\n","    plt.title('Distribution of LLM Evaluation Scores by Model')\n","    plt.savefig('llm_model_score_distribution.png')\n","    plt.close()\n","\n","    plt.figure(figsize=(12, 6))\n","    sns.scatterplot(x='Model', y='LLM_Avg_Score', data=llm_df)\n","    plt.title('LLM Evaluation Scores by Model')\n","    plt.legend()\n","    plt.savefig('llm_model_score.png')\n","    plt.close()\n","\n","\n","def evaluate_coherence_stability(models, domains, datasets, n_runs=5):\n","    stability_results = []\n","    \n","    for model in models:\n","        for domain, data in zip(domains, datasets):\n","            # Handle dictionary data appropriately\n","            if isinstance(data, dict):\n","                data = list(data.values())[0]\n","            elif isinstance(data, pd.DataFrame):\n","                data = data['text'].tolist()\n","            elif not isinstance(data, list):\n","                raise ValueError(f\"Unsupported data format for domain {domain}\")\n","\n","            coherence_scores = []\n","            npmi_scores = []\n","            umass_scores = []\n","\n","            for _ in range(n_runs):\n","                # Sample 80% of the data\n","                sampled_data = np.random.choice(data, size=int(len(data) * 0.8), replace=False)\n","\n","                if model == 'BERTopic':\n","                    _, topics, _ = perform_bertopic_modeling(sampled_data)\n","                elif model == 'VAE':\n","                    _, topics = perform_vae_topic_modeling(sampled_data, num_topics=10)  # Adjust num_topics as needed\n","                elif model == 'LowBERTopic':\n","                    _, topics, _ = perform_lowbertopic_modeling(sampled_data, num_topics=10)  # Remove num_topics argument\n","                else:                    \n","                    raise ValueError(f\"Unsupported model type: {model}\")\n","\n","                # Prepare tokenized data\n","                tokenized_data = [simple_preprocess(doc) for doc in sampled_data]\n","                dictionary = Dictionary(tokenized_data)\n","                corpus = [dictionary.doc2bow(text) for text in tokenized_data]\n","\n","                # Calculate coherence metrics\n","                coherence = calculate_coherence(topics, tokenizer, bert_model)\n","                npmi = calculate_npmi(topics, corpus, dictionary)\n","                umass = calculate_umass(topics, corpus, dictionary)\n","\n","                coherence_scores.append(coherence)\n","                npmi_scores.append(npmi)\n","                umass_scores.append(umass)\n","\n","            # Calculate stability (coefficient of variation)\n","            coherence_stability = np.std(coherence_scores) / np.mean(coherence_scores)\n","            npmi_stability = np.std(npmi_scores) / np.mean(npmi_scores)\n","            umass_stability = np.std(umass_scores) / np.mean(umass_scores)\n","\n","            stability_results.append({\n","                'Model': model,\n","                'Domain': domain,\n","                'Coherence_Stability': coherence_stability,\n","                'NPMI_Stability': npmi_stability,\n","                'UMass_Stability': umass_stability,\n","                'Mean_Coherence': np.mean(coherence_scores),\n","                'Mean_NPMI': np.mean(npmi_scores),\n","                'Mean_UMass': np.mean(umass_scores)\n","            })\n","\n","    return pd.DataFrame(stability_results)\n","\n","def print_results(metrics_df, agreement_results, stability_df, stability_results):\n","    logging.info(\"\\n=== Results Analysis ===\")\n","    \n","    # Print average performance of metrics by model\n","    logging.info(\"\\nAverage performance of metrics by model:\")\n","    models = metrics_df['Model'].unique()\n","    for model in models:\n","        model_metrics = metrics_df[metrics_df['Model'] == model]\n","        mean_metrics = model_metrics[['Coherence', 'NPMI', 'U_Mass']].mean()\n","        logging.info(f\"\\nModel: {model}\")\n","        logging.info(f\"  - Average Coherence: {mean_metrics['Coherence']:.4f}\")\n","        logging.info(f\"  - Average NPMI: {mean_metrics['NPMI']:.4f}\")\n","        logging.info(f\"  - Average U_Mass: {mean_metrics['U_Mass']:.4f}\")\n","    \n","    # Print average performance of metrics by domain\n","    logging.info(\"\\nAverage performance of metrics by domain:\")\n","    domains = metrics_df['Domain'].unique()\n","    for domain in domains:\n","        domain_metrics = metrics_df[metrics_df['Domain'] == domain]\n","        mean_metrics = domain_metrics[['Coherence', 'NPMI', 'U_Mass']].mean()\n","        logging.info(f\"\\nDomain: {domain}\")\n","        logging.info(f\"  - Average Coherence: {mean_metrics['Coherence']:.4f}\")\n","        logging.info(f\"  - Average NPMI: {mean_metrics['NPMI']:.4f}\")\n","        logging.info(f\"  - Average U_Mass: {mean_metrics['U_Mass']:.4f}\")\n","    \n","    # Print agreement analysis results\n","    logging.info(\"\\nAgreement analysis results between metrics:\")\n","    for domain, model_results in agreement_results.items():\n","        logging.info(f\"\\nDomain: {domain}\")\n","        for model, metrics in model_results.items():\n","            logging.info(f\"  Model: {model}\")\n","            for metric, value in metrics.items():\n","                logging.info(f\"    {metric}: {value:.4f}\")\n","    \n","    # Print coherence metric stability results\n","    logging.info(\"\\nIndividual results of coherence metric stability:\")\n","    logging.info(stability_results)\n","    logging.info(\"\\nOverall results of coherence metric stability:\")\n","    stability_summary = stability_df.groupby(['Model', 'Metric'])['CV'].mean().reset_index()\n","    for _, row in stability_summary.iterrows():\n","        logging.info(f\"Model: {row['Model']}, Metric: {row['Metric']}, Average CV: {row['CV']:.4f}\")\n","    \n","    logging.info(\"\\nAnalysis complete. Please review and interpret the results.\")\n","\n","def process_datasets(datasets):\n","    all_metrics = []\n","    bertopic_results = {}\n","    vae_results = {}\n","    lowbertopic_results = {}  # Add dictionary for LowBERTopic results\n","    \n","    for domain, domain_datasets in datasets.items():\n","        for dataset_name, data in domain_datasets.items():\n","            # BERTopic modeling\n","            bertopic_model, bertopic_topics, num_topics = perform_bertopic_modeling(data)\n","            bertopic_results[domain] = {\n","                'num_topics': num_topics,\n","                'topics': bertopic_topics\n","            }\n","            \n","            # Calculate BERTopic metrics\n","            bertopic_metrics = process_metrics(domain, 'BERTopic', bertopic_topics, data, [], tokenizer, bert_model)\n","            all_metrics.extend(bertopic_metrics)\n","            \n","            # VAE modeling (using the number of topics from BERTopic)\n","            vae_model, vae_topics = perform_vae_topic_modeling(data, num_topics)\n","            vae_results[domain] = {\n","                'num_topics': num_topics,\n","                'topics': vae_topics\n","            }\n","            \n","            # Calculate VAE metrics\n","            vae_metrics = process_metrics(domain, 'VAE', vae_topics, data, [], tokenizer, bert_model)\n","            all_metrics.extend(vae_metrics)\n","            \n","            # Add LowBERTopic modeling\n","            lowbertopic_model, lowbertopic_topics, low_num_topics = perform_lowbertopic_modeling(data, low_num_topics=num_topics)\n","            lowbertopic_results[domain] = {\n","                'num_topics': low_num_topics,\n","                'topics': lowbertopic_topics\n","            }\n","            \n","            # Calculate LowBERTopic metrics\n","            lowbertopic_metrics = process_metrics(domain, 'LowBERTopic', lowbertopic_topics, data, [], tokenizer, bert_model)\n","            all_metrics.extend(lowbertopic_metrics)\n","    \n","    return all_metrics, bertopic_results, vae_results, lowbertopic_results\n","\n","\n","def main():\n","    load_bert_model()\n","    try:\n","        logging.info(\"Starting dataset loading\")\n","        datasets = load_all_datasets()\n","        \n","        logging.info(\"Starting topic modeling and metric calculation\")\n","        all_metrics, bertopic_results, vae_results, lowbertopic_results = process_datasets(datasets)\n","        \n","        # Create DataFrame to store topic information\n","        topics_df = pd.DataFrame(columns=['Domain', 'Model', 'Topics'])\n","        \n","        # Process and save BERTopic results\n","        logging.info(\"Outputting and saving BERTopic results\")\n","        for domain, result in bertopic_results.items():\n","            print(f\"\\nDomain: {domain}\")\n","            print(f\"Number of BERTopic topics: {result['num_topics']}\")\n","            print(\"BERTopic topics:\")\n","            for i, topic in enumerate(result['topics']):\n","                print(f\"  Topic {i+1}: {', '.join(topic[:10])}\")\n","            \n","            # Add to topics_df\n","            topics_df = pd.concat([topics_df, pd.DataFrame({\n","                'Domain': [domain],\n","                'Model': ['BERTopic'],\n","                'Topics': [result['topics']]\n","            })], ignore_index=True)\n","        \n","        # Process and save VAE results\n","        logging.info(\"\\nOutputting and saving VAE results\")\n","        for domain, result in vae_results.items():\n","            print(f\"\\nDomain: {domain}\")\n","            print(f\"Number of VAE topics: {result['num_topics']}\")\n","            print(\"VAE topics:\")\n","            for i, topic in enumerate(result['topics']):\n","                print(f\"  Topic {i+1}: {', '.join(topic[:10])}\")\n","            \n","            # Add to topics_df\n","            topics_df = pd.concat([topics_df, pd.DataFrame({\n","                'Domain': [domain],\n","                'Model': ['VAE'],\n","                'Topics': [result['topics']]\n","            })], ignore_index=True)\n","        \n","        # Process and save LowBERTopic results\n","        logging.info(\"\\nOutputting and saving LowBERTopic results\")\n","        for domain, result in lowbertopic_results.items():\n","            print(f\"\\nDomain: {domain}\")\n","            print(f\"Number of LowBERTopic topics: {result['num_topics']}\")\n","            print(\"LowBERTopic topics:\")\n","            for i, topic in enumerate(result['topics']):\n","                print(f\"  Topic {i+1}: {', '.join(topic[:10])}\")\n","            \n","            # Add to topics_df\n","            topics_df = pd.concat([topics_df, pd.DataFrame({\n","                'Domain': [domain],\n","                'Model': ['LowBERTopic'],\n","                'Topics': [result['topics']]\n","            })], ignore_index=True)\n","        \n","        # Save topics_df to CSV file\n","        topics_df.to_csv('topics_df.csv', index=False)\n","        logging.info(\"Topic information has been saved to topics_df.csv file.\")\n","        \n","        logging.info(\"Starting metric analysis\")\n","        metrics_df = pd.DataFrame(all_metrics)\n","        \n","        # Add debugging information\n","        print(\"Metrics DataFrame shape:\", metrics_df.shape)\n","        print(\"Metrics DataFrame columns:\", metrics_df.columns)\n","        print(\"Metrics DataFrame head:\")\n","        print(metrics_df.head())\n","        \n","        metrics_df.to_csv('topic_modeling_metrics.csv', index=False)\n","        \n","        logging.info(\"Starting agreement analysis\")\n","        agreement_results = analyze_agreement(metrics_df)\n","        \n","        logging.info(\"Starting stability analysis\")\n","        stability_df = analyze_stability(datasets, ['BERTopic', 'VAE', 'LowBERTopic'])\n","        \n","        logging.info(\"Starting topic quality visualization\")\n","        visualize_topic_quality(metrics_df)\n","        \n","        logging.info(\"Starting coherence stability evaluation\")\n","        stability_results = evaluate_coherence_stability(['BERTopic', 'VAE', 'LowBERTopic'], list(datasets.keys()), list(datasets.values()))\n","        \n","        logging.info(\"Starting result output\")\n","        print_results(metrics_df, agreement_results, stability_df, stability_results)\n","        \n","        logging.info(\"All analyses completed\")\n","    except Exception as e:\n","        logging.error(f\"Unexpected error occurred in main function execution: {e}\")\n","        raise\n","\n","\n","if __name__ == \"__main__\":\n","    main()                                                   "]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\user\\anaconda3\\envs\\topic\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","2024-10-11 16:38:46,171 - INFO - BERT 모델 및 토크나이저 로딩 중...\n","2024-10-11 16:38:46,940 - INFO - BERT 모델 및 토크나이저 로딩 완료. 사용 중인 디바이스: cpu\n","2024-10-11 16:38:46,940 - INFO - 데이터셋 로딩 시작\n","2024-10-11 16:38:47,265 - INFO - 토픽 모델링 및 메트릭 계산 시작\n","2024-10-11 16:38:47,273 - INFO - Use pytorch device_name: cpu\n","2024-10-11 16:38:47,273 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"]},{"name":"stdout","output_type":"stream","text":["Loaded 5000 texts from data/academy/covid.csv\n","Loaded 5000 texts from data/media/clothing_review.csv\n","Loaded 5000 texts from data/news/agnews.csv\n"]},{"name":"stderr","output_type":"stream","text":["2024-10-11 16:40:25,786 - INFO - adding document #0 to Dictionary<0 unique tokens: []>\n","2024-10-11 16:40:26,375 - INFO - built Dictionary<31276 unique tokens: ['acquired', 'acquisition', 'additional', 'although', 'andor']...> from 5000 documents (total 639576 corpus positions)\n","2024-10-11 16:40:26,375 - INFO - Dictionary lifecycle event {'msg': \"built Dictionary<31276 unique tokens: ['acquired', 'acquisition', 'additional', 'although', 'andor']...> from 5000 documents (total 639576 corpus positions)\", 'datetime': '2024-10-11T16:40:26.375549', 'gensim': '4.3.3', 'python': '3.10.14 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:44:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}\n","2024-10-11 16:40:33,759 - INFO - Coherence: 0.8738, NPMI: 0.3464, U_Mass: -2.0467\n","2024-10-11 16:40:36,000 - INFO - 에폭 1/5, 손실: 4223.6727\n","2024-10-11 16:40:37,025 - INFO - 에폭 2/5, 손실: 267.0606\n","2024-10-11 16:40:38,069 - INFO - 에폭 3/5, 손실: 233.9028\n","2024-10-11 16:40:39,220 - INFO - 에폭 4/5, 손실: 218.4277\n","2024-10-11 16:40:40,282 - INFO - 에폭 5/5, 손실: 213.7351\n","2024-10-11 16:40:41,269 - INFO - adding document #0 to Dictionary<0 unique tokens: []>\n","2024-10-11 16:40:41,854 - INFO - built Dictionary<31276 unique tokens: ['acquired', 'acquisition', 'additional', 'although', 'andor']...> from 5000 documents (total 639576 corpus positions)\n","2024-10-11 16:40:41,855 - INFO - Dictionary lifecycle event {'msg': \"built Dictionary<31276 unique tokens: ['acquired', 'acquisition', 'additional', 'although', 'andor']...> from 5000 documents (total 639576 corpus positions)\", 'datetime': '2024-10-11T16:40:41.855685', 'gensim': '4.3.3', 'python': '3.10.14 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:44:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}\n","2024-10-11 16:40:48,157 - INFO - Coherence: 0.8985, NPMI: 0.0294, U_Mass: -3.1463\n","2024-10-11 16:40:48,186 - INFO - Use pytorch device_name: cpu\n","2024-10-11 16:40:48,187 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n","2024-10-11 16:42:05,052 - INFO - adding document #0 to Dictionary<0 unique tokens: []>\n","2024-10-11 16:42:05,622 - INFO - built Dictionary<31276 unique tokens: ['acquired', 'acquisition', 'additional', 'although', 'andor']...> from 5000 documents (total 639576 corpus positions)\n","2024-10-11 16:42:05,622 - INFO - Dictionary lifecycle event {'msg': \"built Dictionary<31276 unique tokens: ['acquired', 'acquisition', 'additional', 'although', 'andor']...> from 5000 documents (total 639576 corpus positions)\", 'datetime': '2024-10-11T16:42:05.622963', 'gensim': '4.3.3', 'python': '3.10.14 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:44:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}\n","2024-10-11 16:42:06,402 - INFO - Coherence: 0.9030, NPMI: 0.2199, U_Mass: -1.7033\n","2024-10-11 16:42:06,431 - INFO - Use pytorch device_name: cpu\n","2024-10-11 16:42:06,432 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n","2024-10-11 16:42:28,563 - INFO - adding document #0 to Dictionary<0 unique tokens: []>\n","2024-10-11 16:42:28,740 - INFO - built Dictionary<7822 unique tokens: ['ageappropriate', 'cal', 'color', 'end', 'fabric']...> from 5000 documents (total 151271 corpus positions)\n","2024-10-11 16:42:28,741 - INFO - Dictionary lifecycle event {'msg': \"built Dictionary<7822 unique tokens: ['ageappropriate', 'cal', 'color', 'end', 'fabric']...> from 5000 documents (total 151271 corpus positions)\", 'datetime': '2024-10-11T16:42:28.741508', 'gensim': '4.3.3', 'python': '3.10.14 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:44:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}\n","2024-10-11 16:42:31,881 - INFO - Coherence: 0.9252, NPMI: 0.0917, U_Mass: -2.3685\n","2024-10-11 16:42:32,581 - INFO - 에폭 1/5, 손실: 1205.7129\n","2024-10-11 16:42:32,971 - INFO - 에폭 2/5, 손실: 99.3375\n","2024-10-11 16:42:33,372 - INFO - 에폭 3/5, 손실: 86.7463\n","2024-10-11 16:42:33,743 - INFO - 에폭 4/5, 손실: 81.1680\n","2024-10-11 16:42:34,131 - INFO - 에폭 5/5, 손실: 79.0663\n","2024-10-11 16:42:34,345 - INFO - adding document #0 to Dictionary<0 unique tokens: []>\n","2024-10-11 16:42:34,525 - INFO - built Dictionary<7822 unique tokens: ['ageappropriate', 'cal', 'color', 'end', 'fabric']...> from 5000 documents (total 151271 corpus positions)\n","2024-10-11 16:42:34,526 - INFO - Dictionary lifecycle event {'msg': \"built Dictionary<7822 unique tokens: ['ageappropriate', 'cal', 'color', 'end', 'fabric']...> from 5000 documents (total 151271 corpus positions)\", 'datetime': '2024-10-11T16:42:34.526800', 'gensim': '4.3.3', 'python': '3.10.14 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:44:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}\n","2024-10-11 16:42:37,374 - INFO - Coherence: 0.9127, NPMI: 0.0160, U_Mass: -2.9816\n","2024-10-11 16:42:37,391 - INFO - Use pytorch device_name: cpu\n","2024-10-11 16:42:37,392 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n","2024-10-11 16:43:00,589 - INFO - adding document #0 to Dictionary<0 unique tokens: []>\n","2024-10-11 16:43:00,766 - INFO - built Dictionary<7822 unique tokens: ['ageappropriate', 'cal', 'color', 'end', 'fabric']...> from 5000 documents (total 151271 corpus positions)\n","2024-10-11 16:43:00,766 - INFO - Dictionary lifecycle event {'msg': \"built Dictionary<7822 unique tokens: ['ageappropriate', 'cal', 'color', 'end', 'fabric']...> from 5000 documents (total 151271 corpus positions)\", 'datetime': '2024-10-11T16:43:00.766212', 'gensim': '4.3.3', 'python': '3.10.14 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:44:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}\n","2024-10-11 16:43:01,194 - INFO - Coherence: 0.9358, NPMI: 0.0953, U_Mass: -2.0726\n","2024-10-11 16:43:01,209 - INFO - Use pytorch device_name: cpu\n","2024-10-11 16:43:01,210 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n","2024-10-11 16:43:23,634 - INFO - adding document #0 to Dictionary<0 unique tokens: []>\n","2024-10-11 16:43:23,759 - INFO - built Dictionary<15510 unique tokens: ['become', 'certification', 'could', 'dell', 'distribution']...> from 5000 documents (total 95422 corpus positions)\n","2024-10-11 16:43:23,759 - INFO - Dictionary lifecycle event {'msg': \"built Dictionary<15510 unique tokens: ['become', 'certification', 'could', 'dell', 'distribution']...> from 5000 documents (total 95422 corpus positions)\", 'datetime': '2024-10-11T16:43:23.759452', 'gensim': '4.3.3', 'python': '3.10.14 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:44:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}\n","2024-10-11 16:43:29,205 - INFO - Coherence: 0.9057, NPMI: 0.4607, U_Mass: -2.0089\n","2024-10-11 16:43:30,146 - INFO - 에폭 1/5, 손실: 2301.1969\n","2024-10-11 16:43:30,675 - INFO - 에폭 2/5, 손실: 121.3780\n","2024-10-11 16:43:31,192 - INFO - 에폭 3/5, 손실: 105.5927\n","2024-10-11 16:43:31,718 - INFO - 에폭 4/5, 손실: 99.4102\n","2024-10-11 16:43:32,223 - INFO - 에폭 5/5, 손실: 97.4163\n","2024-10-11 16:43:32,412 - INFO - adding document #0 to Dictionary<0 unique tokens: []>\n","2024-10-11 16:43:32,543 - INFO - built Dictionary<15510 unique tokens: ['become', 'certification', 'could', 'dell', 'distribution']...> from 5000 documents (total 95422 corpus positions)\n","2024-10-11 16:43:32,543 - INFO - Dictionary lifecycle event {'msg': \"built Dictionary<15510 unique tokens: ['become', 'certification', 'could', 'dell', 'distribution']...> from 5000 documents (total 95422 corpus positions)\", 'datetime': '2024-10-11T16:43:32.543003', 'gensim': '4.3.3', 'python': '3.10.14 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:44:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}\n","2024-10-11 16:43:37,580 - INFO - Coherence: 0.9286, NPMI: 0.0382, U_Mass: -3.6140\n","2024-10-11 16:43:37,594 - INFO - Use pytorch device_name: cpu\n","2024-10-11 16:43:37,595 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n","2024-10-11 16:43:59,208 - INFO - adding document #0 to Dictionary<0 unique tokens: []>\n","2024-10-11 16:43:59,335 - INFO - built Dictionary<15510 unique tokens: ['become', 'certification', 'could', 'dell', 'distribution']...> from 5000 documents (total 95422 corpus positions)\n","2024-10-11 16:43:59,336 - INFO - Dictionary lifecycle event {'msg': \"built Dictionary<15510 unique tokens: ['become', 'certification', 'could', 'dell', 'distribution']...> from 5000 documents (total 95422 corpus positions)\", 'datetime': '2024-10-11T16:43:59.336396', 'gensim': '4.3.3', 'python': '3.10.14 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:44:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}\n","2024-10-11 16:43:59,692 - INFO - Coherence: 0.9175, NPMI: 0.1670, U_Mass: -2.5831\n","2024-10-11 16:43:59,712 - INFO - BERTopic 결과 출력 및 저장\n","2024-10-11 16:43:59,715 - INFO - \n","VAE 결과 출력 및 저장\n","2024-10-11 16:43:59,719 - INFO - \n","LowBERTopic 결과 출력 및 저장\n","2024-10-11 16:43:59,726 - INFO - 토픽 정보가 topics_df.csv 파일로 저장되었습니다.\n","2024-10-11 16:43:59,726 - INFO - 메트릭 분석 시작\n","2024-10-11 16:43:59,732 - INFO - 일치도 분석 시작\n","2024-10-11 16:43:59,739 - INFO - 안정성 분석 시작\n","2024-10-11 16:43:59,740 - INFO - Analyzing stability for domain: academy\n","2024-10-11 16:43:59,741 - INFO - Original data type: <class 'list'>\n","2024-10-11 16:43:59,742 - INFO - Processed data type: <class 'list'>\n","2024-10-11 16:43:59,743 - INFO - Sample of processed data: ['middle east respiratory syndrome mers highly lethal respiratory disease caused novel coronavirus merscov emerging disease high potential epidemic spread listed coalition epidemic preparedness innovation cepi important target vaccine development initially majority mers case hospital acquired continued emergence mers attributed community acquisition camel likely direct indirect source however majority patient describe camel exposure making route transmission unclear using sensitive immunological assay cohort camel worker cws welldocumented camel exposure show approximately camel worker cws kingdom saudi arabia ksa control previously infected obtained blood sample camel herder truck driver handler welldocumented camel exposure healthy donor measured merscovspecific enzymelinked immunosorbent assay elisa immunofluorescence assay ifa neutralizing antibody titer well cell response total cws healthy control donor seropositive merscovspecific elisa andor neutralizing antibody titer additional four cws seronegative contained virusspecific cell blood although virus transmission cws formally demonstrated possible explanation repeated mers outbreak cws develop mild disease transmit virus uninfected individual infection individual comorbidities result severe disease episodic appearance patient mers', 'cystic fibrosis cf caused mutation cystic fibrosis transmembrane conductance regulator cftr gene resultant characteristic ion transport defect result decreased mucociliary clearance bacterial colonisation chronic neutrophildominated inflammation much knowledge surrounding pathophysiology disease gained generation animal model despite inherent limitation failure certain mouse model recapitulate phenotypic manifestation human disease initiated generation larger animal study cf including pig ferret review summarise basic phenotype three animal model describe contribution animal study current understanding cf', 'spike glycoprotein coronavirus major target virusneutralizing antibody assumed mediate attachment virion host cell kilodalton fragment proteolytically cleaved transmissible gastroenteritis virus tgev protein previously shown bear two adjacent antigenic site b defined hightiter neutralizing antibody recombinant baculoviruses expressing cterminal truncation kilodalton region used localize functionally important determinant protein primary structure two overlapping aminoacidlong product serine common n terminus expressed site b epitope induced virusbinding antibody coexpression one truncated protein derivative aminopeptidase n apn cell surface molecule acting receptor tgev led formation complex could immunoprecipitated anti antibody data provide evidence major neutralizationmediating receptorbinding determinant reside together within domain protein behaves like independent module spite ability prevent sapn interaction neutralizing antibody appeared recognize preformed complex thus indicating antibody receptorbinding determinant essentially distinct together finding bring new insight molecular mechanism tgev neutralization', 'developed highresolution genomic mapping technique combine transposonmediated insertional mutagenesis either capillary electrophoresis massively parallel sequencing identify functionally important region venezuelan equine encephalitis virus veev genome initially used capillary electrophoresis method gain insight role veev nonstructural protein nsp viral replication identified several region nsp intolerant small bp insertion thus presumably functionally important also identified nine separate region nsp tolerate small insertion low temperature c higher temperature c c found method extremely effective identifying temperature sensitive t mutation limited capillary electrophoresis capacity replaced capillary electrophoresis massively parallel sequencing used improved method generate functional map entire veev genome identified several hundred potential t mutation throughout genome validated several mutation nsp nsp e e e capsid using singlecycle growth curve experiment virus generated reverse genetics demonstrated two nsp t mutant attenuated virulence mouse could elicit protective immunity challenge wildtype veev recombinant t mutant valuable tool study veev replication virulence moreover method developed applicable generating tool virus robust reverse genetics system', 'porcine enterovirus pevs belong family picornaviridae report complete genome sequence novel pev strain widely prevalent pig least central eastern china complete genome consists nucleotide excluding polya tail open reading frame map nucleotide position encodes aminoacid polyprotein phylogenetic analysis based cd vp region reveals pev strain belongs specie pev may represent novel serogenotype cpe group iii also report major finding bootscan analysis based whole genome pevs present study available genbank']\n","2024-10-11 16:43:59,749 - INFO - Use pytorch device_name: cpu\n","2024-10-11 16:43:59,750 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"]},{"name":"stdout","output_type":"stream","text":["\n","도메인: academy\n","BERTopic 토픽 수: 99\n","BERTopic 토픽:\n","  토픽 1: influenza, hn, lung, virus, infection, cell, response, ha, human, airway\n","  토픽 2: child, respiratory, hmpv, rsv, virus, infection, patient, viral, age, syncytial\n","  토픽 3: contact, epidemic, model, network, data, outbreak, parameter, transmission, number, dynamic\n","  토픽 4: membrane, fusion, vrna, protein, cholesterol, entry, transport, golgi, cell, vesicle\n","  토픽 5: nsp, protein, coronavirus, protease, sarscov, rna, coronaviruses, cov, replication, plpro\n","  토픽 6: patient, pneumonia, covid, chest, case, ct, clinical, cap, pneumoniae, symptom\n","  토픽 7: cns, microglia, brain, mouse, astrocyte, cell, cxcl, nervous, il, central\n","  토픽 8: animal, zoonotic, pathogen, wildlife, disease, farm, specie, health, reservoir, parasite\n","  토픽 9: bat, specie, reservoir, virus, coronaviruses, host, covs, human, cov, diversity\n","  토픽 10: sarscov, merscov, rbd, dpp, spike, receptor, ace, coronavirus, sars, syndrome\n","  토픽 11: ibv, chicken, strain, chifitm, apmv, bronchitis, genotype, gallus, isolates, locus\n","  토픽 12: site, set, comparison, protein, method, tool, scoring, biological, prediction, choice\n","  토픽 13: cat, feline, fip, felv, fcov, ffv, peritonitis, fipv, gondii, breed\n","  토픽 14: vaccine, adjuvant, mucosal, vaccination, response, ch, antibody, vaccinated, immune, antigen\n","  토픽 15: gp, ebov, filovirus, ebola, entry, ebolavirus, glycoprotein, tim, cell, zaire\n","  토픽 16: sequencing, genome, viral, read, sequence, sample, detection, sewage, virus, method\n","  토픽 17: ace, angiotensin, heart, ang, ppar, diabetic, cardiovascular, hypertension, ii, murf\n","  토픽 18: rna, di, synthesis, leader, mhv, mrna, genome, minusstrand, sequence, nucleotide\n","  토픽 19: influenza, pandemic, vaccination, respondent, perception, intervention, survey, public, risk, ci\n","  토픽 20: cancer, tumor, cell, expression, melanoma, colon, metastasis, icamp, hnscc, pancreatic\n","  토픽 21: camel, merscov, dromedary, east, middle, syndrome, respiratory, coronavirus, human, mers\n","  토픽 22: merscov, mers, saudi, case, east, middle, arabia, outbreak, korea, syndrome\n","  토픽 23: dog, canine, cpv, parvovirus, cdv, strain, sample, carnivore, ccov, fecal\n","  토픽 24: genome, cne, particle, pfv, pol, rna, protein, virus, polyamines, cdna\n","  토픽 25: genome, sequence, phyloinformatics, signature, cc, clustering, analysis, dna, annotation, psgs\n","  토픽 26: bird, specie, pdd, astroviruses, abv, ratpyv, hantavirus, avastrovirus, animal, virus\n","  토픽 27: asthma, exacerbation, child, respiratory, eosinophil, airway, asthmatic, illness, rhinovirus, severity\n","  토픽 28: frameshifting, ribosomal, prf, ribosome, frameshift, mrna, trna, programmed, pseudoknot, slippery\n","  토픽 29: dengue, denv, mosquito, aegypti, deep, fever, serotypes, sample, wolbachia, patient\n","  토픽 30: hcv, liver, nsb, hepatitis, ddxx, genotype, replication, rna, protein, core\n","  토픽 31: ncov, coronavirus, china, sarscov, pneumonia, wuhan, novel, outbreak, covid, epidemic\n","  토픽 32: hsv, pkr, kinase, replication, phosphorylation, gd, csk, activation, inhibiting, prv\n","  토픽 33: pedv, pig, porcine, piglet, swine, diarrhea, ped, antibody, farm, pdcov\n","  토픽 34: health, public, guideline, competency, rg, development, program, service, training, team\n","  토픽 35: bovine, brsv, calf, brd, bvdv, cattle, bpiv, animal, nonvaccinated, nasal\n","  토픽 36: hev, hbv, hepatitis, liver, hepatocytes, orf, virion, hbc, protein, chronic\n","  토픽 37: golgi, er, membrane, protein, transmembrane, tmd, compartment, secretory, sorting, connexins\n","  토픽 38: zikv, zika, cc, microcephaly, neurological, flavivirus, rf, disorder, fetal, congenital\n","  토픽 39: structure, rna, prediction, base, ire, lna, omethyl, pseudoknots, trna, secondary\n","  토픽 40: demyelination, cns, sclerosis, remyelination, demyelinating, cord, spinal, myelin, eae, npc\n","  토픽 41: compound, anticancer, acid, bismuth, ic, cancer, plant, dihydropyrimidinase, glycyrrhizinic, target\n","  토픽 42: lncrnas, mirnas, mir, mirna, mirb, expression, differentially, apoptosis, micrornas, infection\n","  토픽 43: hiv, env, dcsign, mabs, msm, neutralizing, antibody, cluster, dc, gp\n","  토픽 44: health, international, global, ihr, security, public, emergency, legal, law, world\n","  토픽 45: rigi, ifit, signaling, innate, nfkappab, ifitm, immune, sting, antiviral, recognition\n","  토픽 46: prrsv, nsp, pig, porcine, reproductive, pcbp, syndrome, nendou, production, ilra\n","  토픽 47: antibody, scfv, monoclonal, rf, affinity, scfvs, library, rhumig, vc, binding\n","  토픽 48: extract, resveratrol, activity, antiviral, compound, lozenge, effect, chloroquine, leaf, ic\n","  토픽 49: pedv, ptov, strain, diarrhea, porcine, sequence, china, epidemic, cv, pedvs\n","  토픽 50: pandemic, resource, model, influenza, impact, estimate, png, policy, scenario, intervention\n","  토픽 51: autophagy, autophagosomes, atg, autophagic, autophagosome, formation, nsp, lc, replication, starvation\n","  토픽 52: hcovnl, hcovoc, hcov, hcovhku, hcovs, hcove, nl, oc, respiratory, genotype\n","  토픽 53: aerosol, respirator, fit, air, mask, airflow, ventilation, toilet, meningococcal, indoor\n","  토픽 54: ifn, isgs, interferon, orov, pathway, antiviral, irf, temperature, ifitm, response\n","  토픽 55: cristina, anca, daniela, alina, streinucercel, raluca, mihaela, alexandra, popescu, patient\n","  토픽 56: substrate, oas, protein, enzyme, ata, dimer, yoph, thnt, aminoacids, hydrophobic\n","  토픽 57: spike, fusion, hr, hcove, protein, cleavage, hapn, trimer, coronavirus, binding\n","  토픽 58: ev, hfmd, parp, enterovirus, fmdv, cva, fmd, foot, mouth, kombucha\n","  토픽 59: ventilation, patient, mortality, ards, niv, ffas, ci, prone, aki, pulmonary\n","  토픽 60: malaria, falciparum, lair, plasmodium, antibody, erythrocyte, insertion, donor, vivax, parasite\n","  토픽 61: china, covid, case, iran, epidemic, country, travel, wuhan, nepal, chinese\n","  토픽 62: rat, lp, mgkg, ketoprofen, pain, corticosterone, euthanasia, intestine, dietary, sc\n","  토픽 63: surveillance, syndromic, system, health, data, reporting, public, collection, trend, paperbased\n","  토픽 64: wuhan, china, covid, february, epidemic, hubei, city, january, march, number\n","  토픽 65: glycan, glycans, glcnac, sialic, cancer, terminal, pvl, dcsign, carbohydrate, binding\n","  토픽 66: sarscov, sars, detection, assay, rtpcr, gapdh, sensitive, sample, sensitivity, serum\n","  토픽 67: rna, tbsv, helicases, replicase, yeast, replication, bmv, atrh, plusstrand, atp\n","  토픽 68: ebola, west, evd, africa, outbreak, epidemic, trial, bdi, ongoing, transmission\n","  토픽 69: tuberculosis, tb, mtb, mycobacterium, bcg, vaccine, verapamil, gquadruplex, il, masp\n","  토픽 70: ubiquitin, ubiquitination, usp, mavs, otulin, ctar, signaling, prrsv, trim, lmp\n","  토픽 71: psychological, anxiety, depressive, suicide, depression, symptom, psychogenic, psychiatric, mental, covid\n","  토픽 72: hbov, bocavirus, child, bocaviruses, parvovirus, capsid, tract, npa, respiratory, fold\n","  토픽 73: network, regulatory, host, regulator, module, microbial, toxin, response, htr, nec\n","  토픽 74: mhvr, mhv, mouse, mmcgm, receptor, bgp, mhva, ceacama, sjl, isoforms\n","  토픽 75: nanoparticles, toxicity, agnps, silver, antimicrobial, biosensors, plasmonic, optical, property, antisense\n","  토픽 76: tfr, arenavirus, nw, lassa, macv, ow, mxra, gp, residue, alphavirus\n","  토픽 77: microbiota, infant, hrv, nasal, colonization, symptomatic, ari, diversity, cluster, life\n","  토픽 78: ifit, ifn, mouse, vsv, mpv, wildtype, type, interferon, oasb, sev\n","  토픽 79: influenza, transmission, pig, virus, air, humidity, hn, bioaerosols, microwave, aerosol\n","  토픽 80: apoptosis, caspase, ap, cleavage, necroptosis, bid, mlkl, ibvinduced, cell, kinase\n","  토픽 81: calf, cryptosporidium, parvum, oocysts, cryptosporidiosis, bieneusi, diarrhea, sa, peafowl, spp\n","  토픽 82: ub, kinetochore, ubiquitin, madmad, rzz, dub, sac, teb, hlse, miu\n","  토픽 83: poultry, hpai, midwestern, transition, industry, premise, hn, epidemiological, geographic, commercial\n","  토픽 84: disaster, emergency, preparedness, theory, health, phep, indicator, theme, resilience, hics\n","  토픽 85: reference, gene, expression, chromatin, rps, tissue, normalization, pbmc, stability, genorm\n","  토픽 86: hla, peptide, dr, tcell, epitope, panda, hlaa, allele, pmhci, class\n","  토픽 87: orfa, sarscov, genome, coronavirus, sequence, ncov, itbatcovs, ratg, hku, evolution\n","  토픽 88: rhabdovirus, uafi, overt, ekpoma, ekv, nucleic, individual, unrecognized, discovery, ng\n","  토픽 89: adenovirus, hadv, recombination, tmadv, water, adenoviral, reca, av, marmoset, human\n","  토픽 90: mosquito, aahig, flavivirus, nervous, flaviviral, neural, aegypti, denv, hig, lifespan\n","  토픽 91: amphiregulin, gvhd, cytokine, ceacam, basophil, cell, pten, car, cd, inflammatory\n","  토픽 92: chikv, chik, chikungunya, rtrpa, vaccine, chadox, assay, antichikv, chikf, rtlamp\n","  토픽 93: rotavirus, calf, bovine, wdtcc, diarrhea, latex, serotype, eias, fecal, nebraska\n","  토픽 94: hht, antiviral, pyrimidine, inhibitor, nucleoside, cellbased, target, biosynthesis, activity, therapeutic\n","  토픽 95: rotavirus, gii, child, hbov, diarrhea, gastroenteritis, stool, nov, age, whipplei\n","  토픽 96: malaria, febrile, child, plasmodium, parasitemia, student, fever, density, school, illness\n","  토픽 97: sirna, delivery, dendriplexes, chimera, cpgsirna, silencing, pmdis, formulation, fracture, therapeutic\n","  토픽 98: tick, tickborne, powv, oshima, cchfv, elisaarray, lyme, haemorrhagic, powassan, encephalitis\n","\n","도메인: media\n","BERTopic 토픽 수: 44\n","BERTopic 토픽:\n","  토픽 1: sweater, jacket, sleeve, coat, like, color, soft, look, love, small\n","  토픽 2: shirt, tee, look, like, im, material, nice, fit, size, great\n","  토픽 3: skirt, waist, size, wear, work, great, love, really, fit, color\n","  토픽 4: dress, size, fit, bust, petite, small, im, would, chest, length\n","  토픽 5: top, size, like, love, boxy, fit, ordered, look, cute, really\n","  토픽 6: pant, pair, leg, fit, great, love, size, stretch, waist, pilcro\n","  토픽 7: jean, pair, stretch, pilcro, fit, size, perfect, ankle, denim, leg\n","  토픽 8: blouse, size, white, fit, top, wear, sheer, beautiful, xl, well\n","  토픽 9: dress, wedding, beautiful, flower, love, beading, color, person, picture, justice\n","  토픽 10: dress, price, online, love, fabric, review, retailer, felt, like, much\n","  토픽 11: cardigan, color, versatile, look, winter, fit, light, nice, sleeve, well\n","  토픽 12: tank, strap, white, top, love, color, summer, fit, black, cute\n","  토픽 13: bra, nude, cup, fit, size, wear, top, loose, sheer, fabric\n","  토픽 14: dress, comfortable, slip, flattering, fit, casual, color, wear, easy, feminine\n","  토픽 15: jumpsuit, jumper, well, lb, leg, torso, perfectly, petite, im, love\n","  토픽 16: run, small, medium, large, usually, size, dress, im, little, wear\n","  토픽 17: tunic, legging, slit, long, comfortable, color, love, look, tegan, design\n","  토픽 18: suit, swimsuit, bathing, bottom, cup, coverage, size, small, medium, support\n","  토픽 19: vest, tied, look, outfit, add, around, color, love, piece, interest\n","  토픽 20: romper, short, long, torso, one, even, im, cute, really, bottom\n","  토픽 21: jean, top, skinny, love, denim, polished, edgy, casual, pant, white\n","  토픽 22: chest, bust, large, top, area, size, tight, small, fit, medium\n","  토픽 23: short, leg, length, pair, ankle, pilcro, long, year, bought, dont\n","  토픽 24: large, store, cute, medium, small, try, probably, size, big, im\n","  토픽 25: saw, price, sale, store, skort, piece, full, got, looked, be\n","  토픽 26: washed, shrunk, dry, wash, shrink, cold, washing, clean, water, label\n","  토픽 27: dress, fabric, tie, look, wearing, like, unzipping, bc, pretty, without\n","  토픽 28: dress, petite, flattering, love, bust, body, fit, size, flat, cut\n","  토픽 29: staple, closet, soft, great, comfortable, perfect, relaxed, many, dolman, medium\n","  토픽 30: heel, boot, dress, comfortable, im, flat, purchase, worked, ballet, perfectly\n","  토픽 31: soft, spring, fabric, summer, plaid, comfortable, top, highly, quality, material\n","  토픽 32: top, shape, chested, bust, bra, look, like, material, would, pretty\n","  토픽 33: legging, yoga, ml, dancer, slide, cozy, wear, px, dont, comfortable\n","  토픽 34: blue, summer, spring, color, grab, heat, hair, already, got, bring\n","  토픽 35: arm, could, dress, would, size, probably, ala, sp, cute, xl\n","  토픽 36: print, photo, weird, construction, little, boxy, silk, fabric, closet, pressed\n","  토픽 37: poncho, even, fall, warm, unfortunately, sweater, right, back, cozy, maine\n","  토픽 38: love, top, fabric, detail, pretty, beautiful, romantic, little, generally, necklace\n","  토픽 39: button, wife, top, everything, back, two, torn, showed, white, nice\n","  토픽 40: dress, side, returning, strange, shape, neckline, one, bunching, creates, totally\n","  토픽 41: dress, sleeve, taking, fit, appropriate, incredibly, loosernot, extent, overallthe, conference\n","  토픽 42: pink, green, lilac, color, like, yellow, blue, one, brunette, drab\n","  토픽 43: button, dress, buttoning, half, one, split, owned, wrap, stay, abou\n","\n","도메인: news\n","BERTopic 토픽 수: 93\n","BERTopic 토픽:\n","  토픽 1: sox, red, yankee, inning, series, league, boston, baseball, hit, game\n","  토픽 2: wireless, phone, mobile, broadband, access, network, service, communication, internet, subscriber\n","  토픽 3: league, madrid, goal, manchester, champion, real, arsenal, cup, striker, match\n","  토픽 4: profit, sale, thirdquarter, earnings, percent, revenue, quarter, reported, store, lta\n","  토픽 5: touchdown, yard, pass, threw, victory, quarterback, ran, georgia, tennessee, defense\n","  토픽 6: athens, olympic, medal, gold, olympics, greece, basketball, men, hamm, game\n","  토픽 7: oil, price, crude, supply, barrel, nigeria, fell, future, output, opec\n","  토픽 8: iraqi, baghdad, insurgent, iraq, fallujah, soldier, force, military, killed, marine\n","  토픽 9: drug, pharmaceutical, heart, vioxx, arthritis, patient, merck, johnson, risk, food\n","  토픽 10: security, virus, window, worm, patch, update, vulnerability, computer, microsoft, xp\n","  토픽 11: sudan, darfur, region, congo, rebel, rwandan, government, un, ivory, crisis\n","  토픽 12: buy, mining, investment, billion, takeover, gold, group, cash, company, bank\n","  토픽 13: tennis, roddick, open, andy, seed, master, quarterfinal, hewitt, final, title\n","  토픽 14: sun, server, microsystems, system, management, version, enterprise, java, bea, oracle\n","  토픽 15: music, apple, itunes, song, digital, ipod, online, store, player, musicmatch\n","  토픽 16: point, scored, night, raptor, carter, toronto, rebound, victory, vince, pointer\n","  토픽 17: wood, tiger, golf, ryder, vijay, singh, hole, tournament, cup, tour\n","  토픽 18: airline, bankruptcy, union, pilot, employee, delta, airway, attendant, plan, ata\n","  토픽 19: court, judge, microsoft, european, antitrust, appeal, lawsuit, settlement, software, case\n","  토픽 20: vaccine, flu, researcher, stem, human, cloning, research, cell, british, experiment\n","  토픽 21: hostage, iraq, baghdad, kidnapper, iraqi, kidnapped, italian, militant, release, held\n","  토픽 22: stock, investor, oil, price, higher, york, future, lower, dow, profit\n","  토픽 23: hurricane, wind, earthquake, storm, ivan, rain, island, typhoon, cuba, mount\n","  토픽 24: kerry, john, bush, president, presidential, democratic, sen, democrat, kerrys, republican\n","  토픽 25: people, killed, bomb, separatist, embassy, kashmir, jakarta, police, killing, bombing\n","  토픽 26: knee, miss, linebacker, ligament, left, injury, season, injured, back, running\n","  토픽 27: chip, processor, amd, intel, server, ibm, pentium, supercomputer, athlon, transistor\n","  토픽 28: specie, human, bird, scientist, hunter, wildlife, discovered, fossil, extinct, evolution\n","  토픽 29: korea, north, nuclear, south, seoul, korean, pyongyang, talk, japanese, weapon\n","  토픽 30: coach, football, ohio, head, jim, tressel, willingham, college, tyrone, gator\n","  토픽 31: prix, formula, grand, schumacher, sebastien, loeb, race, driver, rally, one\n","  토픽 32: space, flight, prize, astronaut, mojave, manned, spaceshipone, nasa, shuttle, rocket\n","  토픽 33: search, google, engine, internet, yahoo, page, web, local, online, result\n","  토픽 34: iran, nuclear, uranium, enrichment, tehran, program, watchdog, un, atomic, iranian\n","  토픽 35: game, video, portable, nintendo, gaming, playstation, sony, halo, sequel, persia\n","  토픽 36: blair, tony, chirac, jacques, iraq, french, minister, prime, british, president\n","  토픽 37: afghanistan, afghan, kabul, karzai, hamid, pakistan, election, taliban, presidential, laden\n","  토픽 38: movie, kazaa, filesharing, recording, piracy, peertopeer, music, illegally, industry, copyright\n","  토픽 39: peoplesoft, oracle, takeover, bid, hostile, conway, rival, craig, software, corp\n","  토픽 40: theft, cybercrime, spyware, fraud, cyber, identity, arrested, credit, email, thief\n","  토픽 41: russian, chechen, russia, vladimir, putin, chechnya, beslan, moscow, school, president\n","  토픽 42: cut, job, plant, restructuring, percent, worker, reduce, cutting, layoff, workforce\n","  토픽 43: accounting, fraud, qwest, enron, charge, guilty, former, pleaded, associate, pay\n","  토픽 44: cricket, test, india, australia, nagpur, wicket, tendulkar, sachin, first, ganguly\n","  토픽 45: economy, economic, grow, growth, percent, reserve, recovery, grew, export, gross\n","  토픽 46: moon, saturn, cassini, planet, titan, orbit, spacecraft, scientist, earth, solar\n","  토픽 47: myanmar, burma, democracy, aung, suu, junta, kyi, minister, prime, prisoner\n","  토픽 48: chief, executive, resigned, officer, hollinger, lta, conrad, chairman, black, heyer\n","  토픽 49: dvd, tv, toshiba, sony, studio, hollywood, format, hdtv, bluray, recorder\n","  토픽 50: serb, crime, netherlands, detainee, war, bosnian, milosevic, islamic, hague, muslim\n","  토픽 51: arafat, yasser, palestinian, leader, death, compound, funeral, ailing, poisoned, arab\n","  토픽 52: salvation, christmas, bell, kettle, army, volunteer, square, holiday, shopping, tree\n","  토픽 53: aristide, jeanbertrand, ousted, pinochet, augusto, dictator, chilean, haitian, colombia, police\n","  토픽 54: dollar, euro, yen, low, currency, reuters, rich, record, edged, treasury\n","  토픽 55: israeli, gaza, palestinian, strip, hamas, militant, killed, witness, army, wounded\n","  토픽 56: plane, passenger, crashed, crash, airport, crew, train, motorsports, hendrick, aircraft\n","  토픽 57: browser, mozilla, firefox, explorer, microsoft, opensource, internet, foundation, web, release\n","  토픽 58: airbus, boeing, european, dhl, aircraft, plane, deal, brussels, rival, eads\n","  토픽 59: arctic, climate, warming, sea, study, global, change, polar, scientist, smog\n","  토픽 60: security, fund, mutual, exchange, commission, investigation, mortgage, gabelli, civil, wrist\n","  토픽 61: tokyo, nikkei, average, stock, gain, yen, dollar, percent, midday, japanese\n","  토픽 62: pc, computer, china, personal, hewlettpackard, sell, analyst, market, lenovo, maker\n","  토픽 63: trophy, england, champion, icc, vaughan, australia, captain, lanka, africa, sri\n","  토픽 64: speedway, nextel, race, busch, nascar, indy, cup, kurt, homestead, pm\n","  토픽 65: rate, reserve, interest, inflation, federal, keep, bank, gradually, economy, shortterm\n","  토픽 66: nhl, player, hockey, lockout, league, toronto, bargaining, minor, presented, cbc\n","  토픽 67: brawl, pacer, suspension, indiana, fan, nba, piston, player, sprewell, latrell\n","  토픽 68: taiwan, chen, china, jintao, hu, shuibian, singapore, independence, beijing, policy\n","  토픽 69: mar, rover, water, crater, spirit, planet, nasa, rock, opportunity, mission\n","  토픽 70: machine, voting, vote, zetter, voter, kim, electronic, election, office, touchscreen\n","  토픽 71: job, payroll, employer, worker, economist, economy, survey, jobseekers, uneven, braved\n","  토픽 72: missile, defence, india, islamabad, orissa, airsea, testfired, pakistan, range, ballistic\n","  토픽 73: najaf, cleric, shrine, moqtada, shiite, alsadr, ali, sadr, alsistani, imam\n","  토픽 74: ukraine, viktor, kiev, yushchenko, opposition, ukrainian, election, presidential, dioxin, lukashenko\n","  토픽 75: gate, ballmer, steve, microsoft, bill, university, science, student, executive, ceo\n","  토픽 76: police, accused, vigilante, canadian, federal, regina, cp, charge, calgary, toronto\n","  토픽 77: astronomer, nasa, birth, genesis, utah, universe, space, anticipation, meteor, powered\n","  토픽 78: israel, gaza, strip, palestinian, rocket, israeli, jerusalem, militant, leader, syrian\n","  토픽 79: european, commissioner, commission, rocco, barroso, buttiglione, hispano, santander, abbey, competition\n","  토픽 80: yukos, russian, oil, moscow, tax, gazprom, production, asset, oao, beleaguered\n","  토픽 81: musharraf, pervez, pakistan, president, aziz, shaukat, kashmir, army, prime, parliament\n","  토픽 82: fuel, airline, surcharge, virgin, cost, airport, usd, airway, domestic, carrier\n","  토픽 83: ariel, sharon, israeli, jerusalem, prime, minister, party, settler, coalition, gaza\n","  토픽 84: mortgage, rate, house, lowest, housing, price, home, level, lending, application\n","  토픽 85: election, country, iraq, troop, prime, january, minister, pullout, poland, hussein\n","  토픽 86: heavyweight, fight, danny, vitali, klitschko, wbc, williams, champion, bout, mystery\n","  토픽 87: google, ipo, share, range, public, per, initial, slashed, offering, eagerly\n","  토픽 88: apple, expo, imac, presentation, geek, macworld, paris, store, consumer, computer\n","  토픽 89: domain, sender, id, icann, internet, name, transfer, aol, email, assigned\n","  토픽 90: senate, recommendation, democrat, sept, house, confirmation, terrorfighting, partisan, senelect, salazar\n","  토픽 91: trigenix, qualcomm, freescale, million, semiconductor, siemens, euro, component, longterm, kingdombased\n","  토픽 92: henan, province, chinese, killed, china, trapped, coal, ethnic, miner, mine\n","\n","도메인: academy\n","VAE 토픽 수: 99\n","VAE 토픽:\n","  토픽 1: response, confirmed, interaction, using, component, conducted, detected, separate, weak, primary\n","  토픽 2: identified, elucidate, confirmed, called, diverse, used, change, response, pharmacological, procedure\n","  토픽 3: separate, used, diarrhea, structural, data, confirmed, detection, advanced, poor, affinity\n","  토픽 4: epicenter, negativestrand, switch, callicebus, nsp, representing, discus, component, nucleic, costeffectiveness\n","  토픽 5: reduce, morphologically, detected, histidine, poor, switch, tailored, tea, ribosomal, tracking\n","  토픽 6: specimen, confirmed, recently, component, occurred, central, separate, analyzed, data, replication\n","  토픽 7: reduce, separate, isothermal, restriction, detected, bottleneck, crucial, emergency, ubiquitination, change\n","  토픽 8: quantified, purified, upregulated, separate, igg, detected, intense, fitting, confirmed, acquisition\n","  토픽 9: component, reserved, priori, activate, data, detected, response, panel, change, pipeline\n","  토픽 10: reduce, detected, confirmed, purified, fold, valid, norm, identified, change, respiratory\n","  토픽 11: pharmacological, negativestrand, reaction, weak, tracking, blocking, priori, isolate, time, intense\n","  토픽 12: nucleic, separate, delegation, sociological, tracking, weibo, confirmed, twice, ubiquitination, detected\n","  토픽 13: detected, reduce, diarrhea, conducted, data, response, poor, time, component, weak\n","  토픽 14: antibody, component, reduce, used, central, domain, igg, isolate, poor, identifying\n","  토픽 15: separate, negativestrand, reduce, retrotransposons, detected, propensity, uvcrosslinking, meeting, progression, economic\n","  토픽 16: separate, used, limitation, detected, using, nucleic, suggested, switch, respiratory, showed\n","  토픽 17: diarrhea, insight, detected, component, fold, change, article, deletion, evidence, structural\n","  토픽 18: separate, twice, reduce, stem, panel, possesses, called, morphologically, weak, histidine\n","  토픽 19: data, detected, diverse, reduce, reaction, delivery, time, article, elucidate, larger\n","  토픽 20: data, reduce, component, detected, using, switch, time, conducted, common, separate\n","  토픽 21: detected, confirmed, separate, significant, interaction, response, used, respiratory, infectious, set\n","  토픽 22: economic, separate, occurred, detected, confirmed, using, requires, vulnerable, purified, data\n","  토픽 23: confirmed, advanced, data, separate, conducted, infectious, used, central, component, agricultural\n","  토픽 24: blocking, improving, separate, discus, reduce, representing, emergency, detected, pharmacological, oncogenic\n","  토픽 25: tracking, central, haemophilus, intense, nucleic, representing, negativestrand, exclusively, change, priori\n","  토픽 26: using, isolates, detected, time, structural, detection, poor, nucleic, confirmed, ubiquitination\n","  토픽 27: reduce, switch, confirmed, insight, separate, larger, lower, data, response, identified\n","  토픽 28: data, confirmed, used, separate, detected, reduce, cancer, response, isothermal, using\n","  토픽 29: data, reduce, analyzed, occurred, switch, time, transcriptase, central, activate, lower\n","  토픽 30: ultimately, central, reduce, stem, nucleic, identified, panel, twice, fold, pipeline\n","  토픽 31: diarrhea, data, variation, intense, weak, change, nsp, fold, exist, fitting\n","  토픽 32: conducted, detected, change, analyzed, isothermal, inside, central, used, nsp, related\n","  토픽 33: used, isothermal, conducted, weak, taken, fold, wholesale, immunological, panel, variation\n","  토픽 34: component, identified, change, reduce, negativestrand, data, patient, limitation, actual, time\n","  토픽 35: procedure, confirmed, purified, serine, root, posed, epicenter, japonica, tracking, affinity\n","  토픽 36: inside, confirmed, fitting, central, progression, intense, purified, ultimately, affinity, panel\n","  토픽 37: detected, reduce, isothermal, separate, histidine, loss, morphologically, identified, reduced, data\n","  토픽 38: conducted, weak, elucidate, blocking, polyabinding, agricultural, picture, issue, mouth, posed\n","  토픽 39: confirmed, separate, detected, activate, course, fold, central, valid, stability, purified\n","  토픽 40: elucidate, confirmed, interaction, central, nucleic, requires, identified, transcriptase, data, separate\n","  토픽 41: time, used, component, conducted, reduce, detected, considerable, characterization, respiratory, needed\n","  토픽 42: conducted, separate, component, data, confirmed, nucleic, agricultural, interaction, ribosomal, central\n","  토픽 43: reduce, data, detected, change, identified, structural, response, exist, fold, statistical\n","  토픽 44: data, identified, loss, twice, reduce, finding, separate, activate, panel, fitting\n","  토픽 45: haemophilus, structural, data, fitting, exist, insight, combination, pharmacological, respond, progression\n","  토픽 46: used, structural, fitting, deliberate, separate, seven, time, change, identified, blocking\n","  토픽 47: data, reduce, central, poor, loss, separate, isothermal, nm, used, acidic\n","  토픽 48: detected, using, separate, elucidate, isolates, conducted, time, common, blocking, component\n","  토픽 49: separate, detected, variation, purified, improving, exist, intense, abnormality, quantified, poor\n","  토픽 50: confirmed, range, used, component, central, identified, stability, isolate, needed, elucidate\n","  토픽 51: poor, ultrastructure, regional, using, nucleic, detected, interaction, data, microscopy, representing\n","  토픽 52: detected, statement, separate, stem, sent, data, curative, docking, posed, weak\n","  토픽 53: detected, data, elucidate, reduce, response, blocking, crucial, identified, delivery, needed\n","  토픽 54: limitation, used, confirmed, significant, specimen, blocking, respiratory, separate, occurred, economic\n","  토픽 55: reduce, identified, needed, confirmed, data, poor, isolates, elucidate, central, ards\n","  토픽 56: response, detected, confirmed, identified, separate, range, structural, diarrhea, interaction, potential\n","  토픽 57: elucidate, using, detected, occurred, isolates, used, reporting, finding, confirmed, central\n","  토픽 58: discus, condition, separate, twice, isolate, nucleic, patient, confirmed, exist, occurred\n","  토픽 59: component, isolate, epicenter, significant, offer, respiratory, signaling, separate, primary, localization\n","  토픽 60: reduce, data, response, separate, change, scenario, respiratory, diverse, microscopy, indicate\n","  토픽 61: falciparum, putative, data, change, isolates, ubiquitination, offer, bonding, root, progression\n","  토픽 62: response, detected, central, separate, identified, used, confirmed, conducted, diverse, data\n","  토픽 63: separate, data, response, twice, significant, change, used, poor, confirmed, agricultural\n","  토픽 64: separate, activate, depend, oocysts, oropouche, twice, immunological, central, variation, component\n","  토픽 65: component, fold, stem, separate, reporting, detected, central, confirmed, fluc, switch\n","  토픽 66: detected, using, confirmed, article, separate, conducted, reduce, used, taken, economic\n","  토픽 67: separate, improving, elucidate, isolates, activate, used, putative, reduce, agricultural, tbd\n","  토픽 68: recycling, data, blocking, acidic, used, panel, affinity, lucifugus, adenocarcinoma, variation\n","  토픽 69: confirmed, identified, used, statistical, reading, range, elucidate, reduce, using, cancer\n","  토픽 70: data, detected, larger, active, confirmed, improving, isolates, response, globally, using\n","  토픽 71: interaction, structural, uniquely, confirmed, tracking, ubiquitination, twice, negativestrand, reporting, panel\n","  토픽 72: component, confirmed, poor, quantified, primary, data, elucidate, evidenced, detected, indicate\n","  토픽 73: separate, putative, elucidate, data, nucleic, detected, intense, loss, poor, sustained\n","  토픽 74: component, response, central, used, identified, diarrhea, confirmed, elucidate, conducted, rtpcr\n","  토픽 75: fold, hcove, detected, diarrhea, statement, agricultural, vaccinology, detection, mics, transcriptase\n","  토픽 76: reduce, blocking, detected, separate, isothermal, emergency, link, switch, morphologically, histidine\n","  토픽 77: data, separate, activate, elucidate, twice, switch, central, abnormality, economic, fitting\n","  토픽 78: central, confirmed, liver, fitting, intense, switch, conductance, limitation, scarce, wellcharacterized\n","  토픽 79: nucleic, detected, occurred, switch, time, poor, activate, pipeline, priori, blocking\n","  토픽 80: quantified, intense, separate, agricultural, reserved, fitting, detected, limitation, time, bonding\n","  토픽 81: data, separate, change, detected, identified, component, isothermal, reduce, nucleic, using\n","  토픽 82: discus, detected, bystander, separate, representing, ultimately, brown, villus, exist, inside\n","  토픽 83: purified, hsp, bonding, detected, polyproteins, separate, reduce, variability, quantified, exhibiting\n","  토픽 84: detected, data, taken, weak, inside, accumulation, confirms, rm, separate, coated\n","  토픽 85: needed, blocking, cameroonian, contributes, used, article, statistical, merscov, discus, panel\n","  토픽 86: separate, change, quantified, reduce, representing, agricultural, weak, cu, reflecting, skilled\n","  토픽 87: component, acidic, response, switch, data, twice, statistical, blocking, used, panel\n","  토픽 88: confirmed, deliberate, ventral, fitting, fold, separate, structural, rd, fluc, induced\n","  토픽 89: recently, data, confirmed, separate, conducted, nucleic, reduce, response, component, significant\n","  토픽 90: data, component, analyzed, change, central, conducted, identified, separate, used, reduce\n","  토픽 91: confirmed, response, interaction, structural, diarrhea, larger, potential, identified, taken, haemophilus\n","  토픽 92: epicenter, negativestrand, terminus, activate, purified, identifies, nucleic, poor, representing, trimer\n","  토픽 93: finding, confirmed, elucidate, identified, reduce, discus, combining, response, detected, using\n","  토픽 94: using, data, change, conducted, called, confirmed, component, detected, response, cancer\n","  토픽 95: detected, diarrhea, data, conducted, component, time, agricultural, common, reserved, structural\n","  토픽 96: detected, separate, response, data, component, taken, variation, structural, particularly, poor\n","  토픽 97: data, confirmed, screening, reduce, response, pharmacological, preparedness, identified, change, reporting\n","  토픽 98: pharmacological, reduce, tracking, haemophilus, negativestrand, representing, discus, blocking, priori, switch\n","  토픽 99: separate, central, conducted, elucidate, component, detected, confirmed, isolates, exist, poor\n","\n","도메인: media\n","VAE 토픽 수: 44\n","VAE 토픽:\n","  토픽 1: attractive, isnt, soft, come, im, legging, medium, classic, based, mentioned\n","  토픽 2: dress, spot, compliment, comfortable, dont, shoulder, im, large, love, quality\n","  토픽 3: shoulder, come, large, bit, comfortable, im, medium, shape, ultra, compliment\n","  토픽 4: im, comfortable, medium, quality, compliment, soft, shoulder, bit, look, love\n","  토픽 5: exactly, drape, large, compliment, mentioned, casual, woman, maybe, tied, polyester\n","  토픽 6: soft, flowy, compliment, large, lot, come, im, low, love, classic\n","  토픽 7: im, comfortable, soft, dont, dress, compliment, legging, flattering, bit, felt\n","  토픽 8: comfortable, large, arm, bit, im, drape, medium, love, really, dress\n","  토픽 9: legging, casual, flowy, compliment, sand, blocking, low, large, hei, mannequin\n","  토픽 10: classic, soft, im, cut, flowy, come, package, dressed, compliment, exactly\n","  토픽 11: im, dress, compliment, casual, isnt, spot, lined, lot, know, dont\n","  토픽 12: soft, large, typically, excited, comfortable, stock, bit, petite, compliment, thank\n","  토픽 13: dress, im, soft, large, comfortable, dont, lb, look, size, flattering\n","  토픽 14: large, legging, length, slipped, shoulder, mind, dress, lb, previously, spot\n","  토픽 15: im, quality, flattering, comfortable, dress, compliment, soft, legging, size, bit\n","  토픽 16: soft, sweater, lined, legging, shape, petite, maybe, silhouette, properly, pocket\n","  토픽 17: lb, love, size, bit, exactly, comfortable, open, im, soft, large\n","  토픽 18: isnt, casual, layering, im, flattering, lightweight, silhouette, drape, comfortable, legging\n","  토픽 19: large, silhouette, spot, roomy, stylish, petite, dont, exactly, denim, mind\n","  토픽 20: compliment, bit, smudge, busted, large, classic, low, stylish, dont, control\n","  토픽 21: legging, silhouette, sale, thigh, compliment, mentioned, maybe, petite, excited, perfect\n","  토픽 22: soft, legging, petite, lined, cut, wow, hold, roomy, im, looked\n","  토픽 23: soft, easy, legging, perfection, excited, thigh, accentuates, mentioned, im, classic\n","  토픽 24: soft, classic, shape, suggest, comfortable, lined, legging, compliment, control, large\n","  토픽 25: soft, medium, exactly, maybe, tailor, silhouette, drape, bit, comfortable, isnt\n","  토픽 26: classic, compliment, legging, im, bit, excited, dont, standard, xxsp, come\n","  토픽 27: compliment, im, look, comfortable, love, large, soft, size, shape, flowy\n","  토픽 28: soft, size, bit, comfortable, compliment, flowy, im, mentioned, legging, look\n","  토픽 29: casual, soft, medium, previously, isnt, classic, owned, look, booty, mannequin\n","  토픽 30: lb, im, loved, typically, ultra, thigh, spectrum, legging, drape, end\n","  토픽 31: compliment, exactly, based, polyester, lace, legging, thank, blocking, large, afternoon\n","  토픽 32: soft, im, comfortable, compliment, dress, length, size, dont, flattering, bit\n","  토픽 33: easy, classic, large, open, petite, felt, lot, low, legging, said\n","  토픽 34: dont, legging, exactly, typically, large, compliment, polyester, sweater, maybe, durable\n","  토픽 35: flowy, legging, sock, tights, strange, im, ultra, lined, compliment, perfection\n","  토픽 36: im, flattering, soft, sweater, comfortable, felt, size, lined, isnt, dont\n","  토픽 37: im, dress, love, quality, compliment, large, arm, size, usually, store\n","  토픽 38: compliment, legging, lined, bit, comfortable, tailor, quality, classic, roomy, dont\n","  토픽 39: quality, classic, dont, easy, flowy, typically, compliment, previously, lose, end\n","  토픽 40: soft, comfortable, shape, loved, im, quality, compliment, flattering, typically, legging\n","  토픽 41: bit, medium, quality, large, perfect, want, casual, excited, compliment, legging\n","  토픽 42: casual, shoulder, legging, picture, panel, stylish, booty, quality, im, dart\n","  토픽 43: im, comfortable, bit, quality, soft, legging, look, size, flattering, casual\n","  토픽 44: soft, classic, legging, chillier, chance, compliment, casual, cut, shame, wow\n","\n","도메인: news\n","VAE 토픽 수: 93\n","VAE 토픽:\n","  토픽 1: authority, ap, win, signed, sale, plan, cell, change, play, reuters\n","  토픽 2: financial, reuters, return, today, ap, authority, inning, play, president, yard\n","  토픽 3: president, afp, authority, sale, virus, play, fell, rising, ministry, pass\n","  토픽 4: domestic, authority, aimed, today, reuters, plan, ministry, russian, win, end\n","  토픽 5: authority, reuters, face, signed, end, gerry, rashad, trainee, play, australia\n","  토픽 6: today, president, authority, plan, reuters, medium, germany, popular, signed, issue\n","  토픽 7: ap, reuters, afp, said, sale, group, cup, market, win, federal\n","  토픽 8: reuters, meeting, plan, yard, group, employee, knocked, afp, civil, addressing\n","  토픽 9: plan, reuters, gorgeous, gb, peru, help, kim, forced, research, voted\n","  토픽 10: reuters, sale, financial, plan, ap, authority, return, aimed, play, federal\n","  토픽 11: today, named, reuters, squarely, corp, plan, explosion, email, night, face\n","  토픽 12: reuters, group, face, plan, authority, ministry, security, north, signed, ap\n","  토픽 13: reuters, plan, today, stock, end, talk, ap, record, meeting, authority\n","  토픽 14: charging, win, reuters, aimed, change, troubled, domestic, plan, squarely, looking\n","  토픽 15: reuters, cell, authority, play, signed, london, novak, linebacker, developer, lowcost\n","  토픽 16: reuters, plan, yard, return, powerful, end, gb, ap, financial, largest\n","  토픽 17: reuters, ap, financial, return, ministry, toutatis, server, yard, force, charging\n","  토픽 18: plan, win, domestic, herbert, germany, change, aimed, forced, olympic, squarely\n","  토픽 19: reuters, president, authority, nearly, amp, plan, ap, program, financial, championship\n","  토픽 20: sale, financial, president, quarterly, aimed, ministry, authority, wrapped, chavez, preliminary\n","  토픽 21: reuters, authority, plan, sale, president, help, drive, accident, half, play\n","  토픽 22: reuters, ap, plan, week, authority, announced, program, sale, russian, financial\n","  토픽 23: ministry, looking, authority, powerful, president, shape, financial, plan, named, reuters\n","  토픽 24: reuters, authority, plan, win, president, fell, signed, week, ap, sale\n","  토픽 25: reuters, force, afp, ap, voted, win, ministry, authority, sale, signed\n","  토픽 26: authority, reuters, named, today, powerful, squarely, medium, plan, announced, looking\n","  토픽 27: reuters, report, group, said, yard, ap, service, signed, plan, win\n","  토픽 28: reuters, ap, market, return, plan, cup, sale, rising, service, week\n","  토픽 29: reuters, today, plan, president, authority, sale, financial, ministry, week, ap\n","  토픽 30: plan, reuters, authority, financial, president, help, squarely, rival, knocked, signed\n","  토픽 31: signed, reuters, authority, olympic, win, plan, issue, fell, today, play\n","  토픽 32: minn, plan, issue, today, return, group, change, reuters, administration, soldier\n","  토픽 33: authority, signed, gorgeous, society, reuters, ind, plan, sale, cell, combined\n","  토픽 34: ap, plan, president, reuters, fell, sale, stock, rival, financial, research\n","  토픽 35: signed, authority, sale, plan, issue, dotcom, reuters, win, change, voted\n","  토픽 36: reuters, president, ap, win, end, authority, week, force, said, issue\n","  토픽 37: authority, reuters, sale, play, nearly, yard, western, london, disrupt, financial\n","  토픽 38: reuters, president, sale, authority, voted, plan, fell, aimed, stock, rival\n","  토픽 39: plan, authority, able, force, return, indonesian, urge, medium, kept, stock\n","  토픽 40: plan, germany, ap, today, used, financial, championship, afp, security, president\n","  토픽 41: reuters, market, cell, nearly, president, forced, hour, ap, federal, group\n","  토픽 42: reuters, report, plan, security, program, said, president, fell, signed, needed\n","  토픽 43: ministry, signed, face, plan, urge, fell, america, broke, financial, win\n","  토픽 44: reuters, plan, ap, stock, olympic, authority, report, play, end, security\n","  토픽 45: reuters, ap, force, largest, report, plan, executive, ministry, bid, medium\n","  토픽 46: reuters, win, authority, ap, end, signed, week, group, program, face\n","  토픽 47: reuters, sale, today, market, authority, president, return, announced, win, play\n","  토픽 48: reuters, signed, play, sale, magazine, president, authority, plan, federal, week\n","  토픽 49: plan, reuters, authority, return, ap, olympic, group, fell, signed, bid\n","  토픽 50: reuters, sale, ap, plan, authority, president, ministry, signed, win, today\n","  토픽 51: reuters, ap, nearly, london, seed, stock, ministry, president, plan, security\n","  토픽 52: piracy, authority, president, fell, plan, half, play, camp, london, medium\n","  토픽 53: reuters, ap, plan, service, said, set, president, financial, sale, week\n","  토픽 54: force, plan, change, issue, seed, death, explosion, reuters, signed, said\n","  토픽 55: reuters, authority, service, squarely, line, ap, comeback, broker, england, seed\n","  토픽 56: reuters, able, help, financial, genetically, london, linux, president, force, half\n","  토픽 57: reuters, ap, race, plan, financial, largest, aimed, force, end, week\n","  토픽 58: afp, ministry, authority, plan, sale, force, yard, amp, society, reuters\n","  토픽 59: today, market, reuters, ap, nearly, looking, win, sale, set, play\n","  토픽 60: afp, market, plan, president, fell, reuters, help, attack, group, authority\n","  토픽 61: plan, reuters, email, stock, friday, win, month, ap, needed, research\n","  토픽 62: signed, ap, win, authority, plan, force, stock, talk, reuters, voted\n","  토픽 63: ap, reuters, bid, stock, plan, said, sale, financial, senior, signed\n","  토픽 64: financial, reuters, authority, president, fell, magazine, win, signed, play, shelf\n","  토픽 65: today, reuters, issue, charging, aimed, stock, president, win, plan, said\n","  토픽 66: service, reuters, president, ap, plan, today, authority, championship, used, fell\n","  토픽 67: reuters, president, today, plan, olympic, fell, ministry, authority, country, sale\n","  토픽 68: reuters, stock, win, force, plan, australia, fell, tiger, financial, report\n","  토픽 69: reuters, security, help, issue, ministry, said, force, commission, plan, today\n","  토픽 70: reuters, month, today, season, ap, announced, win, rival, plan, meeting\n","  토픽 71: plan, reuters, sale, night, win, afp, british, today, squarely, authority\n","  토픽 72: authority, london, reuters, return, charging, play, weakness, president, colo, portion\n","  토픽 73: authority, outside, plan, signed, win, reuters, rashad, today, return, sale\n","  토픽 74: ap, reuters, signed, plan, today, face, olympic, week, ministry, month\n","  토픽 75: ap, today, employee, president, reuters, financial, cup, plan, federal, sale\n","  토픽 76: president, shape, issue, authority, today, looking, ministry, powerful, financial, america\n","  토픽 77: sale, charging, weakness, aimed, cup, toutatis, financial, ministry, apparent, reuters\n","  토픽 78: reuters, today, plan, financial, australia, ap, return, said, end, set\n","  토픽 79: authority, squarely, cell, looking, signed, ministry, picked, developer, dawn, championship\n","  토픽 80: financial, ap, win, plan, ministry, reuters, month, signed, stock, nearly\n","  토픽 81: reuters, ap, authority, win, sale, president, space, program, domestic, medium\n","  토픽 82: financial, ap, act, amp, ministry, nearly, sale, rising, authority, signed\n","  토픽 83: plan, authority, using, program, signed, win, reuters, developer, amp, service\n","  토픽 84: signed, service, forced, member, change, plan, group, fell, feather, authority\n","  토픽 85: return, reuters, week, today, signed, said, authority, bid, ap, win\n","  토픽 86: reuters, president, stock, authority, ninth, issue, fell, championship, plan, financial\n","  토픽 87: authority, reuters, plan, today, sale, financial, play, passage, end, leave\n","  토픽 88: signed, ap, plan, able, amp, reuters, today, security, recall, marcus\n","  토픽 89: reuters, ministry, authority, play, today, financial, president, signed, rising, yard\n","  토픽 90: plan, authority, reuters, steam, afp, signed, stock, today, vendorrelationship, england\n","  토픽 91: win, signed, authority, amid, declaring, reuters, ministry, fly, urge, looking\n","  토픽 92: reuters, return, plan, signed, president, stock, play, authority, win, end\n","  토픽 93: issue, authority, return, reuters, cell, president, powerful, turkey, amid, today\n","\n","도메인: academy\n","LowBERTopic 토픽 수: 5\n","LowBERTopic 토픽:\n","  토픽 1: virus, infection, cell, protein, viral, disease, study, result, respiratory, human\n","  토픽 2: compound, activity, extract, effect, sirna, cell, antiviral, acid, ic, delivery\n","  토픽 3: ace, angiotensin, heart, rat, mouse, ang, activity, expression, ii, effect\n","  토픽 4: calf, cryptosporidium, parvum, oocysts, cryptosporidiosis, bieneusi, diarrhea, sa, neonatal, spp\n","\n","도메인: media\n","LowBERTopic 토픽 수: 5\n","LowBERTopic 토픽:\n","  토픽 1: fit, size, love, like, look, great, dress, color, top, wear\n","  토픽 2: washed, shrunk, dry, shrink, wash, top, washing, cold, water, short\n","  토픽 3: button, dress, one, top, back, two, great, size, buttoning, half\n","  토픽 4: poncho, color, love, back, fall, wear, right, even, warm, petite\n","\n","도메인: news\n","LowBERTopic 토픽 수: 5\n","LowBERTopic 토픽:\n","  토픽 1: new, said, reuters, company, inc, price, oil, corp, million, year\n","  토픽 2: game, first, night, victory, league, season, win, ap, new, team\n","  토픽 3: said, minister, iraq, reuters, killed, president, prime, iraqi, people, palestinian\n","  토픽 4: scientist, hurricane, moon, nasa, saturn, planet, human, wind, specie, ap\n","Metrics DataFrame shape: (9, 5)\n","Metrics DataFrame columns: Index(['Domain', 'Model', 'Coherence', 'NPMI', 'U_Mass'], dtype='object')\n","Metrics DataFrame head:\n","    Domain        Model  Coherence      NPMI    U_Mass\n","0  academy     BERTopic   0.873761  0.346442 -2.046714\n","1  academy          VAE   0.898500  0.029391 -3.146339\n","2  academy  LowBERTopic   0.903045  0.219893 -1.703266\n","3    media     BERTopic   0.925173  0.091748 -2.368536\n","4    media          VAE   0.912745  0.015968 -2.981598\n","DataFrame columns: Index(['Domain', 'Model', 'Coherence', 'NPMI', 'U_Mass'], dtype='object')\n","DataFrame head:\n","    Domain        Model  Coherence      NPMI    U_Mass\n","0  academy     BERTopic   0.873761  0.346442 -2.046714\n","1  academy          VAE   0.898500  0.029391 -3.146339\n","2  academy  LowBERTopic   0.903045  0.219893 -1.703266\n","3    media     BERTopic   0.925173  0.091748 -2.368536\n","4    media          VAE   0.912745  0.015968 -2.981598\n"]},{"name":"stderr","output_type":"stream","text":["2024-10-11 16:45:19,656 - INFO - Sampled data type: <class 'list'>\n","2024-10-11 16:45:19,657 - INFO - Sample of sampled data: ['influenzaassociated bacterial viral infection responsible high level morbidity death pandemic seasonal influenza episode review undertaken assess evaluate incidence epidemiology aetiology clinical importance impact bacterial viral coinfection secondary infection associated influenza review carried published article covering bacterial viral infection associated pandemic seasonal influenza published december include pulmonary extrapulmonary infection pneumococcal infection remains predominant cause bacterial pneumonia review highlight importance co secondary bacterial viral infection associated influenza emergence newly identified dual infection associated hn pandemic strain severe influenzaassociated pneumonia often bacterial necessitate antibiotic treatment addition wellknown bacterial cause less common bacteria legionella pneumophila may also associated influenza new influenza strain emerge review provide clinician overview range bacterial viral co secondary infection could present influenza illness', 'background significance clinical utility multiple virus detection multiplex realtime polymerase chain reaction rtpcr respiratory tract infection remain unclear method retrospective cohort study analyzed virus detection affected clinical management month period clinical laboratory information collected child adult two swiss tertiary centre whose respiratory sample tested respiratory virus plex rtpcr test result pathogen identified patient patient virus bacterium virus bacterium patient viral infection received antibiotic virus detection associated discontinuation antibiotic adult child overall adult child managed correctly without antibiotic virus detection p taking biomarkers radiologic presentation antibiotic pretreatment account impact rtpcr appropriateness therapy clinically viral infection increased child adult conclusion substantial reduction unnecessary antibiotic prescription seems possible appropriate application rtpcr result respiratory tract infection encouraged', 'intranasal challenge influenza virus streptococcus pneumoniae promotes otitis medium pneumoniae chinchilla investigated whether influenza virus infection promotes oropharyngeal colonization pneumoniae middle ear pathogen selectively inhibiting commensal bacteria study day allergic nonallergic adult subject intranasally inoculated influenza akawasaki hn virus every subject infected virus demonstrated nasal shedding seroconversion average upper respiratory symptom score nasal secretion weight entire subject group elevated day acute phase significantly different allergic nonallergic subject pneumoniae isolated subject prior virus challenge isolated heavy density subject day p staphylococcus aureus isolated frequently nonallergic subject allergic subject day versus respectively versus respectively versus respectively p isolation rate middle ear pathogen significantly different virus challenge acute resolution phase day experimental infection entire subject group either allergic nonallergic subgroup density isolation rate commensal bacteria entire subject group similar throughout observational period result suggest virus infection promoted pneumoniae colonization oropharynx nonallergic person may vulnerable colonization aureus allergic person altered colonization rate attributed inhibition commensal bacteria', 'tuberculosis tb caused mycobacterium tuberculosis infection leading cause mortality morbidity causing million death annually cd cell several cytokine th cytokine ifn critical control infection conversely immunosuppressive cytokine il shown dampen th cell response tuberculosis infection impairing bacterial clearance however critical cellular source il tuberculosis infection still unknown using il reporter mouse show article first tuberculosis infection predominant cell expressing il lung lyc monocyte however day postinfection ilexpressing cell also highly represented notably mouse deficient cellderived il mouse deficient monocytederived il showed significant reduction lung bacterial load chronic tuberculosis infection compared fully ilcompetent mouse indicating major role cellderived il tb susceptibility ilexpressing cell detected among cd cd cell expressed high level cd tbet able coproduce ifn il upon ex vivo stimulation furthermore tuberculosis infection il expression cd cell partially regulated il type ifn signaling together data reveal despite multiple immune source il tuberculosis infection activated effector cell major source accounting ilinduced tb susceptibility', 'peptidase enzyme hydrolyse peptide bond protein peptide peptidase important pathological condition alzheimers disease tumour parasite invasion processing viral polyproteins merops database internet resource containing information peptidase substrate inhibitor database includes detail cleavage position substrate physiological nonphysiological natural synthetic cleavage collection including total different protein cleavage synthetic substrate number cleavage designated physiological data derived publication least one substrate cleavage known different peptidase recognized merops database website three new display two showing peptidase specificity logo frequency matrix third showing dynamically generated alignment protein substrate closely related homologues many protein described literature peptidase substrate studied vitro assumption physiologically relevant cleavage site would conserved specie conservation every site term peptidase preference examined number identified conserved number cogent reason site might conserved poorly conserved site examined reason postulated site identified poorly conserved cleavage likely fortuitous physiological relevance dataset freely available via internet useful training set algorithm predict substrate peptidase cleavage position within substrate data may also useful design inhibitor engineering novel specificity peptidase database url httpmeropssangeracuk']\n","2024-10-11 16:45:19,663 - INFO - Use pytorch device_name: cpu\n","2024-10-11 16:45:19,663 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n","2024-10-11 16:46:41,160 - INFO - adding document #0 to Dictionary<0 unique tokens: []>\n","2024-10-11 16:46:41,648 - INFO - built Dictionary<28149 unique tokens: ['addition', 'aetiology', 'also', 'antibiotic', 'article']...> from 4000 documents (total 510520 corpus positions)\n","2024-10-11 16:46:41,649 - INFO - Dictionary lifecycle event {'msg': \"built Dictionary<28149 unique tokens: ['addition', 'aetiology', 'also', 'antibiotic', 'article']...> from 4000 documents (total 510520 corpus positions)\", 'datetime': '2024-10-11T16:46:41.649015', 'gensim': '4.3.3', 'python': '3.10.14 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:44:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}\n","2024-10-11 16:46:47,932 - INFO - Sampled data type: <class 'list'>\n","2024-10-11 16:46:47,933 - INFO - Sample of sampled data: ['collaborative cross cc multiparent panel recombinant inbred ri mouse strain derived eight founder laboratory strain ri panel popular longterm genetic stability enhances reproducibility integration data collected across time condition characterization genome community effort reducing burden individual user present genome cc strain using two complementary approach resource improve power interpretation genetic experiment study also provides cautionary tale regarding limitation imposed basic biological process mutation selection distinct advantage inbred panel genotyping need performed panel individual mouse initial cc genome data haplotype reconstruction based dense genotyping recent common ancestor mrcas strain followed imputation genome sequence corresponding founder inbred strain mrca resource captured segregating region strain fully inbred limited resolution transition region founder haplotype uncertainty founder assignment region limited diversity report whole genome sequence cc strain generated pairedend short read coverage single male per strain sequencing lead substantial improvement fine structure completeness genome cc mrcas sequenced sample show significant reduction genomewide haplotype frequency two wildderived strain casteij pwkphj addition analysis evolution pattern heterozygosity indicates selection three wildderived founder strain played significant role shaping genome cc sequencing resource provides first description ten thousand new genetic variant introduced mutation drift cc genome estimate new snp mutation accumulating cc strain rate per gigabase per generation fixation new mutation genetic drift introduced thousand new variant cc strain majority mutation novel compared currently sequenced laboratory stock wild mouse predicted alter gene function approximately onethird cc inbred strain acquired large deletion kb many overlap known coding gene functional element sequence mouse critical resource cc user increase threefold number mouse inbred strain genome available publicly provides insight effect mutation drift common resource', 'proteolytic priming common method controlling activation membrane fusion mediated viral glycoprotein severe acute respiratory syndrome coronavirus spike protein sarscov primed variety host cell protease proteolytic cleavage occurring s boundary adjacent fusion peptide domain studied priming sarscov elastase show important role residue thr domain series alanine mutant generated vicinity cleavage site goal examining elastasemediated cleavage within proteolytic cleavage fusion activation modulated altering cleavage site position propose novel mechanism whereby sarscov fusion protein function controlled spatial regulation proteolytic priming site important implication viral pathogenesis', 'jatspobjectives amid continuing spread novel coronavirus covid incubation period covid regularly reassessed information available upon increase reported case present work estimated distribution incubation period patient infected outside hubei province china method clinical data collected individual case reported medium fully available official page chinese health authority mle used estimate distribution incubation period result found incubation period patient travel history hubei longer volatile conclusion recommended duration quarantine extended least weeksjatsp', 'asthma chronic respiratory disease whose prevalence increasing western world recently research begun focus role microbiome play asthma pathogenesis hope understanding respiratory disorder considered sterile recently lung revealed contain unique microbiota shift towards molecular method quantification sequencing microbial dna revealed airway harbour unique microbiota apparent reproducible difference present healthy diseased lung hope classifying microbial load asthmatic airway insight may afforded possible role pulmonary microbe may propagating asthmatic airway response could potentially pave way new therapeutic strategy treatment chronic lung condition asthma', 'numerous pathological state including cancer autoimmune disease viralbacterial infection often attributed uncontrollable dna replication inhibiting essential biological process provides obvious therapeutic target disease logical target dna polymerase enzyme responsible catalyzing addition mononucleotides growing polymer using dna rna template guide directing incorporation event review provides summary therapeutic agent target polymerase activity discussion biological function mechanism polymerase first provided illustrate strategy therapeutic intervention well rational design various nucleoside analog inhibit various polymerase associated viral infection cancer development nucleoside nonnucleoside inhibitor antiviral agent discussed particular emphasis mechanism action structureactivity relationship toxicity mechanism resistance addition commonly used anticancer agent described illustrate similarity difference associated various nucleoside analog therapeutic agent finally new therapeutic approach discussed include inhibition selective polymerase involved dna repair andor translesion dna synthesis anticancer agent']\n","2024-10-11 16:46:47,938 - INFO - Use pytorch device_name: cpu\n","2024-10-11 16:46:47,939 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n","2024-10-11 16:48:07,092 - INFO - adding document #0 to Dictionary<0 unique tokens: []>\n","2024-10-11 16:48:07,574 - INFO - built Dictionary<27965 unique tokens: ['accumulating', 'acquired', 'across', 'addition', 'advantage']...> from 4000 documents (total 513440 corpus positions)\n","2024-10-11 16:48:07,574 - INFO - Dictionary lifecycle event {'msg': \"built Dictionary<27965 unique tokens: ['accumulating', 'acquired', 'across', 'addition', 'advantage']...> from 4000 documents (total 513440 corpus positions)\", 'datetime': '2024-10-11T16:48:07.574710', 'gensim': '4.3.3', 'python': '3.10.14 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:44:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}\n","2024-10-11 16:48:14,407 - INFO - Sampled data type: <class 'list'>\n","2024-10-11 16:48:14,408 - INFO - Sample of sampled data: ['viral vaccine produced adherent suspension cell objective work screen human suspension cell line capacity support viral replication first step investigated whether poliovirus replicate cell line sabin poliovirus type serially passaged five human cell line hl k kg thp u sabin type capable efficiently replicating three cell line k kg u yielding high viral titer replication expression cd poliovirus receptor explain susceptibility replication since cell line expressed cd furthermore showed passaged virus replicated efficiently parental virus kg cell yielding higher virus titer supernatant early infection infection cell line moi resulted high viral titer supernatant day infection k passaged sabin type bioreactor system yielded high viral titer supernatant altogether data suggest k kg u cell line useful propagation poliovirus', 'discus model data crowd disaster crime terrorism war disease spreading show conventional recipe deterrence strategy often effective sufficient contain many common approach provide good picture actual system behavior neglect feedback loop instability cascade effect complex often counterintuitive behavior social system macrolevel collective dynamic better understood mean complexity science highlight suitable system design management help stop undesirable cascade effect enable favorable kind selforganization system way complexity science help save human life', 'objective describe aspect found hrct scan chest patient infected influenza hn virus method retrospectively analyzed hrct scan patient female male hn infection confirmed laboratory test july september hrct scan interpreted two thoracic radiologist independently case disagreement decision made consensus result common hrct finding groundglass opacity consolidation combination groundglass opacity consolidation finding airspace nodule bronchial wall thickening interlobular septal thickening crazypaving pattern perilobular pattern air trapping finding frequently bilateral random distribution pleural effusion observed typically minimal lymphadenopathy identified conclusion common finding groundglass opacity consolidation combination involvement commonly bilateral axial craniocaudal predominance distribution although major tomographic finding hn infection nonspecific important recognize finding order include infection hn virus differential diagnosis respiratory symptom', 'applied cartographic geostatistical method analyzing pattern disease spread severe acute respiratory syndrome sars outbreak hong kong using geographic information system gi technology analyzed integrated database contained clinical personal detail patient confirmed sars february june elementary mapping disease occurrence space time simultaneously revealed geographic extent spread throughout territory statistical surface created kernel method confirmed sars case highly clustered identified distinct disease hot spot contextual analysis mean standard deviation different density class indicated period day february day march prodrome epidemic whereas day may june marked declining phase outbreak originanddestination plot showed directional bias radius spread superspreading event integration gi technology routine field epidemiologic surveillance offer realtime quantitative method identifying tracking geospatial spread infectious disease experience sars demonstrated', 'coronavirus cov responsible severe acute respiratory syndrome sars sarscov encodes two large polyproteins ppa ppab processed two viral protease yield mature nonstructural protein nsps many nsps essential role viral replication several assigned function possess amino acid sequence unique cov family one protein sarscov nsp processed nterminus ppa ppab mature sarscov protein present cell several hour postinfection colocalizes viral replication complex function viral life cycle remains unknown furthermore nsp sequence highly divergent across cov family suggested due nsp possessing function specific viral interaction host cell acting host specific virulence factor order initiate structural biophysical study sarscov nsp recombinant expression system purification protocol developed yielding milligram quantity highly purified sarscov nsp purified protein characterized using circular dichroism size exclusion chromatography multiangle light scattering']\n","2024-10-11 16:48:14,414 - INFO - Use pytorch device_name: cpu\n","2024-10-11 16:48:14,415 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n","2024-10-11 16:49:31,466 - INFO - adding document #0 to Dictionary<0 unique tokens: []>\n","2024-10-11 16:49:31,944 - INFO - built Dictionary<28042 unique tokens: ['adherent', 'altogether', 'bioreactor', 'capable', 'capacity']...> from 4000 documents (total 514918 corpus positions)\n","2024-10-11 16:49:31,944 - INFO - Dictionary lifecycle event {'msg': \"built Dictionary<28042 unique tokens: ['adherent', 'altogether', 'bioreactor', 'capable', 'capacity']...> from 4000 documents (total 514918 corpus positions)\", 'datetime': '2024-10-11T16:49:31.944866', 'gensim': '4.3.3', 'python': '3.10.14 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:44:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}\n","2024-10-11 16:49:38,286 - INFO - Sampled data type: <class 'list'>\n","2024-10-11 16:49:38,287 - INFO - Sample of sampled data: ['synthesis characterization isotopomer tandem nucleic acid mass tagpeptide nucleic acid tntpna conjugate described along use electrospray ionisationcleavable esicleavable hybridization probe detection quantification target dna sequence electrospray ionisation tandem mass spectrometry esimsms esicleavable peptide tnt isotopomers introduced pna oligonucleotide sequence total synthesis approach conjugate evaluated hybridization probe detection quantification immobilized synthetic target dna using esimsms experiment pna portion conjugate act hybridization probe whereas peptide tnt released collisionbased process ionization probe conjugate electrospray ion source cleaved tnt act uniquely resolvable marker identify quantify unique target dna sequence method applicable wide variety assay requiring highly multiplexed quantitative dnarna analysis including gene expression monitoring genetic profiling detection pathogen', 'background molecular diagnostics enable sensitive detection respiratory virus clinical significance remains unclear pediatric lower respiratory tract infection lrti aimed determine whether viral coinfections increased lifethreatening disease large cohort method molecular testing performed respiratory virus nasopharyngeal aspirate collected child aged year within hour hospital admission sentinel surveillance severe acute respiratory illness sari hospitalization conducted south africa february december primary outcome lifethreatening disease defined mechanical ventilation intensive care unit admission death result hivuninfected child respiratory syncytial virus rsvassociated lrti rsv monoinfection lifethreatening disease rhinovirus adenovirus adv influenza virus rsv viral coinfection associated severe disease odds ratio confidence interval ci ci adv coinfection increased odds lifethreatening disease adjusted ci p influenza coinfection increased odds lifethreatening disease prolonged length stay adjusted ci p compared rsv monoinfection conclusion rsv coinfection respiratory virus associated severe disease compared rsv alone study however increased lifethreatening disease rsvadv rsvinfluenza coinfection warrant study', 'background cat susceptible feline panleukopenia virus fpv canine parvovirus cpv variant b c detection fpv cpv variant apparently healthy cat persistence white blood cell wbc tissue neutralising antibody simultaneously present suggest parvovirus may persist longterm tissue cat postinfection without causing clinical sign aim study screen population cat sardinia italy presence fpv cpv dna within buffy coat sample using polymerase chain reaction pcr dna viral load genetic diversity phylogeny antibody titre parvovirus investigated positive cat result carnivore protoparvovirus dna detected nine cat viral dna reassembled fpv four cat cpv cpvb c four cat one subject showed unusually high genetic complexity mixed infection involving fpv cpvc antibody parvovirus detected subject tested positive dna parvovirus conclusion identification fpv cpv dna wbc asymptomatic cat despite presence specific antibody parvovirus high genetic heterogeneity detected one sample confirmed relevant epidemiological role cat parvovirus infection', 'feline infectious peritonitis fip fatal inflammatory disease caused fip virus infection feline tumor necrosis factor ftnfalpha closely involved aggravation fip pathology previously described preparation neutralizing mouse antiftnfalpha monoclonal antibody mab clarified role clinical condition cat fip using vitro system however administration mouse mab cat may lead production feline antimouse antibody present study prepared mousefeline chimeric mab chimeric mab fusing variable region mouse mab constant region feline antibody chimeric mab confirmed ftnfalpha neutralization activity purified mouse mab chimeric mab repeatedly administered cat change ability induce feline antimouse antibody response investigated serum cat treated mouse mab feline antimouse antibody production induced ftnfalpha neutralization effect mouse mab reduced contrast cat treated chimeric mab feline antimouse antibody response decreased compared mouse mab treated cat', 'review intends present recapitulate current knowledge role importance regulatory rna micrornas small interfering rna rna binding protein enzyme processing rna activated rna cell infected rna virus review focus noncoding rna involved rna virus replication pathogenesis host response especially retrovirus hiv example mechanism action transcriptional regulation promotion increased stability target degradation']\n","2024-10-11 16:49:38,292 - INFO - Use pytorch device_name: cpu\n","2024-10-11 16:49:38,293 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n","2024-10-11 16:50:54,618 - INFO - adding document #0 to Dictionary<0 unique tokens: []>\n","2024-10-11 16:50:55,084 - INFO - built Dictionary<28043 unique tokens: ['acid', 'act', 'along', 'analysis', 'applicable']...> from 4000 documents (total 511381 corpus positions)\n","2024-10-11 16:50:55,085 - INFO - Dictionary lifecycle event {'msg': \"built Dictionary<28043 unique tokens: ['acid', 'act', 'along', 'analysis', 'applicable']...> from 4000 documents (total 511381 corpus positions)\", 'datetime': '2024-10-11T16:50:55.085707', 'gensim': '4.3.3', 'python': '3.10.14 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:44:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}\n","2024-10-11 16:51:01,731 - INFO - Sampled data type: <class 'list'>\n","2024-10-11 16:51:01,732 - INFO - Sample of sampled data: ['b protein middle east respiratory syndrome coronavirus merscov described antagonism host innate immunity however unlike clustering pattern complete gene sequence human camel merscovs b protein coding region constitute speciesspecific phylogenetic group moreover given estimated evolutionary rate complete b gene sequence b protein might less affected speciesspecific innate immune pressure result suggest b protein merscov may function host innate immunity manner independent host specie andor evolutionary clustering pattern', 'background proteolytic processing lassa virus envelope glycoprotein precursor gpc host proprotein convertase site protease sp prerequisite incorporation subunit gp gp viral particle hence essential infectivity virus spread therefore tested study concept using sp target block efficient virus replication methodologyprincipal finding demonstrate stable cell line inducibly expressing spadapted antitrypsin variant inhibit proteolytic maturation gpc introduction sp recognition motif rril rrll reactive center loop antitrypsin resulted abrogation gpc processing endogenous sp similar level observed spdeficient cell moreover spspecific antitrypsins significantly inhibited replication spread replicationcompetent recombinant vesicular stomatitis virus expressing lassa virus glycoprotein gp well authentic lassa virus inhibition viral replication correlated ability different antitrypsin variant inhibit processing lassa virus glycoprotein precursor conclusionssignificance data suggest glycoprotein cleavage sp promising target development novel antiarenaviral strategy', 'describe use xenopus laevis egg extract vitro translation post translational modification membrane secretory protein extract capable translation segregation membrane microgram per millilitre level protein added mrna signal sequence segregated protein efficiently cleaved appropriate nlinked glycosylation pattern produced extract also support quantitative assembly murine immunoglobulin heavy light chain tetramers two event take place beyond endoplasmic reticulum mannose phosphorylation murine cathepsin olinked glycosylation coronavirus e protein also occur reduced efficiency stability membrane allows protease protection study quantitative centrifugal fractionation segregated unsegregated protein performed condition use stored extract also determined', 'interferoninducible transmembrane ifitmfragilis gene encode homologous protein induced ifns show ifitm protein regulate murine cd th cell differentiation ifitm ifitm expressed wildtype wt cd cell activation ifitm downregulated ifitm upregulated resting ifitmfamilydeficient cd cell higher expression thassociated gene wt purified naive ifitmfamilydeficient cd cell differentiated efficiently th whereas th differentiation inhibited ifitmfamilydeficient mouse ifitmdeficient mouse less susceptible wt induction allergic airway disease weaker th response less severe disease lower il higher ifng expression il secretion thus ifitm family important adaptive immunity influencing thth polarization th immunopathology', 'empiric quantification human mobility pattern paramount better urban planning understanding social network structure responding infectious disease threat especially light rapid growth urbanization globalization need particular relevance developing country since host majority global urban population disproportionally affected burden disease used global positioning system gps dataloggers track finescale within city mobility pattern resident two neighborhood city iquitos peru used million gps datapoints quantify agespecific mobility parameter dynamic colocation network among tracked individual geographic space significantly affected human mobility giving rise highly local mobility kernel movement occurred within km individual home potential hourly contact among individual highly irregular temporally unstructured tracked participant showed regular predictable mobility routine sharp contrast situation developed world case study quantified impact spatially temporally unstructured routine dynamic transmission influenzalike pathogen within iquitos neighborhood temporally unstructured daily routine eg dominated single location workplace individual repeatedly spent significant amount time increased epidemic final size effective reproduction number comparison scenario modeling temporally structured contact finding provide mechanistic description basic rule shape human mobility within resourcepoor urban center contribute understanding role finescale pattern individual movement colocation infectious disease dynamic generally study emphasizes need careful consideration human social interaction designing infectious disease mitigation strategy particularly within resourcepoor urban environment']\n","2024-10-11 16:51:01,737 - INFO - Use pytorch device_name: cpu\n","2024-10-11 16:51:01,738 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n","2024-10-11 16:52:17,374 - INFO - adding document #0 to Dictionary<0 unique tokens: []>\n","2024-10-11 16:52:17,869 - INFO - built Dictionary<28087 unique tokens: ['affected', 'andor', 'antagonism', 'camel', 'clustering']...> from 4000 documents (total 513835 corpus positions)\n","2024-10-11 16:52:17,870 - INFO - Dictionary lifecycle event {'msg': \"built Dictionary<28087 unique tokens: ['affected', 'andor', 'antagonism', 'camel', 'clustering']...> from 4000 documents (total 513835 corpus positions)\", 'datetime': '2024-10-11T16:52:17.870156', 'gensim': '4.3.3', 'python': '3.10.14 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:44:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}\n","2024-10-11 16:52:22,613 - INFO - Sampled data type: <class 'list'>\n","2024-10-11 16:52:22,614 - INFO - Sample of sampled data: ['lassa mammarenavirus lasv enveloped rna virus cause lassa fever acute hemorrhagic fever syndrome associated significant morbidity high rate fatality endemic region western africa arenavirus matrix protein z several function virus life cycle including coordinating viral assembly driving release new virus particle regulating viral polymerase activity antagonizing host antiviral response limited knowledge regarding various function z regulated investigate possible mean regulation mass spectrometry used identify potential site phosphorylation lasv z protein analysis revealed two serine one tyrosine phosphorylated flexible n cterminal region protein notably two site located directly adjacent ppxy late domain important motif virus release study nonphosphorylatable phosphomimetic z protein revealed site important regulator release lasv particle hostdriven reversible phosphorylation may play important role regulation lasv z protein function', 'contact tracing followed treatment isolation key control measure battle infectious disease extreme form locally targeted control potential highly efficient dealing low number case reason frequently used combat sexually transmitted disease new invading pathogen accurate modelling contact tracing requires explicit information diseasetransmission pathway individual hence network contact pairwiseapproximation method full stochastic simulation used investigate utility contact tracing simple relationship found efficiency contact tracing necessary eradication basic reproductive ratio disease hold wide variety realistic situation including heterogeneous network containing coregroups superspreaders asymptomatic individual clustering transitivity within transmission network found destroy relationship requiring lower efficiency predicted', 'middle east respiratory syndrome coronavirus merscov belongs coronaviridae family spite several outbreak recent year vaccine deadly virus developed yet study receptor binding domain rbd spike glycoprotein merscov analyzed computational immunology approach identify antigenic determinant epitope order sequence glycoprotein belong different geographical region aligned observe conservancy merscov rbd immune parameter region determined using different silico tool immune epitope database iedb molecular docking study also employed check affinity potential epitope towards binding cleft specific hla allele nterminus rbd s glycoprotein found conserved among available strain merscov based lower ic value total eight potential tcell epitope major histocompatibility complex mhc classi allele identified conserved region mer epitope cysslildy displayed interaction maximum number mhc classi molecule projected highest peak bcell antigenicity plot concludes could better choice designing epitope based peptide vaccine merscov considering must undergo vitro vivo experiment moreover molecular docking study epitope found significant binding affinity kcalmol towards binding cleft hlac molecule', 'transmissible gastroenteritis tge caused devastating economic loss swine industry worldwide despite extensive research focusing pathogenesis virus infection molecular pathogenic mechanism tgevinduced diarrhea piglet unknown intestinal diarrhea closely related function nah exchanger protein nhe brush border membrane small intestine epithelial cell epidermal growth factor receptor egfr may act regulate nhe expression addition egfr may promote viral invasion host cell present study aimed determine whether nhe activity regulated altering egfr expression affect na absorption tgevinfected intestinal epithelial cell porcine intestinal epithelial cell used model tgev infection result showed na absorption nhe expression level decreased tgevinfected cell proliferation tgev within ipecj cell could inhibited treatment egfr inhibitor ag knockdown resulting recovery na absorption tgev infected cell increasing activity expression nhe moreover demonstrated nhe activity regulated egfrerk pathway importantly nhe mobility plasma membrane tgev infected cell significantly weaker normal cell egfr inhibition knockdown recovered mobility research indicated nhe activity negatively regulated egfr tgevinfected intestinal epithelial cell', 'outbreak covid developed aboard princess cruise ship januaryfebruary using mathematical modeling timeseries incidence data describing trajectory outbreak among passenger crew member characterize transmission potential varied course outbreak estimate mean reproduction number confined setting reached value high higher mean estimate reported communitylevel transmission dynamic china singapore approximate range finding suggest rt decreased substantially compared value early phase japanese government implemented enhanced quarantine control recent estimate rt reached value largely epidemic threshold indicating secondary outbreak novel coronavirus unlikely occur aboard diamond princess ship']\n","2024-10-11 16:52:22,619 - INFO - Use pytorch device_name: cpu\n","2024-10-11 16:52:22,619 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"]}],"source":["# Cell 1: 모델 실행, 평가 지표 실행, 기타 결과 분석\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from gensim import models, corpora\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.preprocessing import MinMaxScaler\n","from gensim.models.coherencemodel import CoherenceModel\n","import time\n","import json\n","from nltk.corpus import stopwords\n","from math import log\n","from itertools import combinations\n","from tqdm import tqdm\n","import logging\n","from collections import Counter, defaultdict\n","import gensim\n","from gensim import corpora\n","from scipy.sparse import csr_matrix\n","from gensim.utils import simple_preprocess\n","from gensim.corpora import Dictionary\n","from transformers import BertTokenizer, BertModel\n","from bertopic import BERTopic\n","import seaborn as sns\n","from scipy import stats\n","import os\n","import re\n","import matplotlib\n","from tabulate import tabulate\n","matplotlib.use('Agg')\n","import matplotlib.pyplot as plt\n","from sklearn.manifold import MDS\n","from sklearn.preprocessing import MinMaxScaler\n","from torch.utils.data import DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.cluster import KMeans\n","\n","# NLTK 데이터 다운로드\n","import nltk\n","nltk.download('punkt', quiet=True)\n","nltk.download('stopwords', quiet=True)\n","\n","# 로깅 설정\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","\n","# stop_words 정의\n","stop_words = set(stopwords.words('english'))\n","\n","# 전역 변수로 BERT 모델과 토크나이저 선언\n","global tokenizer, bert_model\n","\n","# BERT 모델 로딩 함수\n","def load_bert_model():\n","    global tokenizer, bert_model\n","    if 'tokenizer' not in globals() or 'bert_model' not in globals():\n","        logging.info(\"BERT 모델 및 토크나이저 로딩 중...\")\n","        try:\n","            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","            bert_model = BertModel.from_pretrained('bert-base-uncased')\n","            \n","            # GPU 사용 가능 여부 확인 및 설정\n","            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","            bert_model = bert_model.to(device)\n","            \n","            logging.info(f\"BERT 모델 및 토크나이저 로딩 완료. 사용 중인 디바이스: {device}\")\n","        except Exception as e:\n","            logging.error(f\"BERT 모델 로딩 중 오류 발생: {e}\")\n","            raise\n","\n","def load_data(file_path, sample_size=5000):\n","    try:\n","        df = pd.read_csv(file_path, header=None, names=['text'])\n","    except FileNotFoundError:\n","        logging.error(f\"File not found: {file_path}\")\n","        return []\n","    except Exception as e:\n","        logging.error(f\"Error loading file {file_path}: {e}\")\n","        return []\n","    texts = df['text'].astype(str)\n","    if len(texts) > sample_size:\n","        texts = texts.sample(n=sample_size, random_state=42)\n","    print(f\"Loaded {len(texts)} texts from {file_path}\")\n","    return texts.tolist()\n","\n","def load_all_datasets():\n","    datasets = {\n","        'academy': {\n","            'covid': load_data('data/academy/covid.csv')\n","        },\n","        'media': {\n","            'clothing_review': load_data('data/media/clothing_review.csv')\n","        },\n","        'news': {\n","            'agnews': load_data('data/news/agnews.csv')\n","        }\n","    }\n","    return datasets\n","\n","class VAE(nn.Module):\n","    def __init__(self, input_dim, hidden_dim=50, latent_dim=None):\n","        if latent_dim is None:\n","            raise ValueError(\"latent_dim must be specified\")\n","        super(VAE, self).__init__()\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        self.fc21 = nn.Linear(hidden_dim, latent_dim)  \n","        self.fc22 = nn.Linear(hidden_dim, latent_dim)  \n","        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n","        self.fc4 = nn.Linear(hidden_dim, input_dim)\n","\n","    def encode(self, x):\n","        h1 = F.relu(self.fc1(x))\n","        return self.fc21(h1), self.fc22(h1)\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5 * logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps * std\n","\n","    def decode(self, z):\n","        h3 = F.relu(self.fc3(z))\n","        return torch.sigmoid(self.fc4(h3))\n","\n","    def forward(self, x):\n","        mu, logvar = self.encode(x)\n","        z = self.reparameterize(mu, logvar)\n","        return self.decode(z), mu, logvar\n","\n","def vae_loss(recon_x, x, mu, logvar):\n","    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n","    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    return BCE + KLD\n","\n","def extract_vae_topics(vae_model, vectorizer, num_topics, top_n=10):\n","    with torch.no_grad():\n","        latent_vectors = torch.eye(num_topics).to(vae_model.fc3.weight.device)\n","        decoder_output = vae_model.decode(latent_vectors)\n","        decoder_output = decoder_output.cpu().numpy()\n","\n","    feature_names = vectorizer.get_feature_names_out()\n","    topics = []\n","    for topic_distribution in decoder_output:\n","        top_indices = topic_distribution.argsort()[-top_n:][::-1]\n","        topic_words = [feature_names[i] for i in top_indices]\n","        topics.append(topic_words)\n","    return topics\n","\n","# def perform_vae_topic_modeling(data, num_topics, num_epochs=5, hidden_dim=50):\n","#     data = [str(doc) for doc in data if isinstance(doc, str) and len(doc) > 0]\n","\n","#     # TfidfVectorizer 사용\n","#     vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n","#     doc_term_matrix = vectorizer.fit_transform(data)\n","\n","#     # MinMaxScaler를 사용하여 0-1 사이로 정규화\n","#     scaler = MinMaxScaler()\n","#     normalized_matrix = scaler.fit_transform(doc_term_matrix.toarray())\n","\n","#     vocab_size = len(vectorizer.get_feature_names_out())\n","#     input_dim = vocab_size\n","\n","#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#     vae_model = VAE(input_dim=input_dim, hidden_dim=hidden_dim, latent_dim=num_topics).to(device)\n","#     optimizer = torch.optim.Adam(vae_model.parameters(), lr=1e-3)\n","\n","#     batch_size = 64\n","#     data_loader = DataLoader(normalized_matrix.astype(np.float32), batch_size=batch_size, shuffle=True)\n","\n","#     vae_model.train()\n","#     for epoch in range(num_epochs):\n","#         train_loss = 0\n","#         for batch in data_loader:\n","#             batch = batch.to(device)\n","#             optimizer.zero_grad()\n","#             recon_batch, mu, logvar = vae_model(batch)\n","#             loss = vae_loss(recon_batch, batch, mu, logvar)\n","#             loss.backward()\n","#             optimizer.step()\n","#             train_loss += loss.item()\n","#         logging.info(f\"에폭 {epoch+1}/{num_epochs}, 손실: {train_loss / len(data_loader.dataset):.4f}\")\n","\n","#     topics = extract_vae_topics(vae_model, vectorizer, num_topics)\n","#     return vae_model, topics\n","\n","def perform_bertopic_modeling(data):\n","    if isinstance(data, dict):\n","        data = list(data.values())[0]\n","    elif isinstance(data, pd.DataFrame):\n","        data = data['text'].tolist() if 'text' in data.columns else data.values.flatten().tolist()\n","    elif isinstance(data, pd.Series):\n","        data = data.tolist()\n","    elif isinstance(data, np.ndarray):\n","        data = data.flatten().tolist()\n","    elif isinstance(data, list):\n","        pass\n","    else:\n","        raise ValueError(f\"Unsupported data format for BERTopic modeling: {type(data)}\")\n","\n","    # 데이터가 문자열 리스트인지 확인\n","    if not all(isinstance(item, str) for item in data):\n","        raise ValueError(\"All items in the data must be strings\")\n","\n","    try:\n","        bertopic_model = BERTopic(language=\"english\", calculate_probabilities=True)\n","        topics, _ = bertopic_model.fit_transform(data)\n","        \n","        num_topics = len(bertopic_model.get_topics())\n","        topic_words = []\n","        for i in range(num_topics):\n","            topic = bertopic_model.get_topic(i)\n","            if topic:\n","                words = [word for word, _ in topic[:10]]  # 상위 10개 단어만 추출\n","                topic_words.append(words)\n","        \n","        return bertopic_model, topic_words, num_topics\n","    except AttributeError as e:\n","        logging.error(f\"BERTopic 모델링 중 오류 발생: {e}\")\n","        return None, None, None\n","    \n","\n","def perform_lowbertopic_modeling(data, low_num_topics):\n","    # 토픽 수를 num_topics로 설정 (기본값 5)\n","    low_num_topics = 5\n","    \n","    # 데이터 전처리\n","    if isinstance(data, dict):\n","        data = list(data.values())[0]\n","    elif isinstance(data, pd.DataFrame):\n","        data = data['text'].tolist() if 'text' in data.columns else data.values.flatten().tolist()\n","    elif isinstance(data, pd.Series):\n","        data = data.tolist()\n","    elif isinstance(data, np.ndarray):\n","        data = data.flatten().tolist()\n","    elif isinstance(data, list):\n","        pass\n","    else:\n","        raise ValueError(f\"Unsupported data format for BERTopic modeling: {type(data)}\")\n","\n","    # 데이터가 문자열 리스트인지 확인\n","    if not all(isinstance(item, str) for item in data):\n","        raise ValueError(\"All items in the data must be strings\")\n","\n","    try:\n","        \n","        # BERTopic 모델 생성\n","        lowbertopic_model = BERTopic(language=\"english\", calculate_probabilities=True, nr_topics=low_num_topics)\n","        \n","        # 모델 학습\n","        topics, _ = lowbertopic_model.fit_transform(data)\n","        \n","        # 토픽 단어 추출\n","        topic_words = []\n","        for i in range(low_num_topics):\n","            topic = lowbertopic_model.get_topic(i)\n","            if topic:\n","                words = [word for word, _ in topic[:10]]  # 상위 10개 단어 추출\n","                topic_words.append(words)\n","        \n","        return lowbertopic_model, topic_words, low_num_topics\n","    except Exception as e:\n","        logging.error(f\"LowBERTopic 모델링 중 오류 발생: {e}\")\n","        return None, None, None\n","\n","\n","def perform_vae_topic_modeling(data, num_topics, num_epochs=5, hidden_dim=50):\n","    try:\n","        # 데이터 전처리\n","        data = [str(doc) for doc in data if isinstance(doc, str) and len(doc) > 0]\n","        vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n","        doc_term_matrix = vectorizer.fit_transform(data)\n","\n","        # MinMaxScaler를 사용하여 0-1 사이로 정규화\n","        scaler = MinMaxScaler()\n","        normalized_matrix = scaler.fit_transform(doc_term_matrix.toarray())\n","\n","        # VAE 모델 초기화 및 학습\n","        input_dim = doc_term_matrix.shape[1]\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        vae_model = VAE(input_dim=input_dim, hidden_dim=hidden_dim, latent_dim=num_topics).to(device)\n","        optimizer = torch.optim.Adam(vae_model.parameters(), lr=1e-3)\n","\n","        batch_size = 64\n","        data_loader = DataLoader(normalized_matrix.astype(np.float32), batch_size=batch_size, shuffle=True)\n","\n","        vae_model.train()\n","        for epoch in range(num_epochs):\n","            train_loss = 0\n","            for batch in data_loader:\n","                batch = batch.to(device)\n","                optimizer.zero_grad()\n","                recon_batch, mu, logvar = vae_model(batch)\n","                loss = vae_loss(recon_batch, batch, mu, logvar)\n","                loss.backward()\n","                optimizer.step()\n","                train_loss += loss.item()\n","            logging.info(f\"에폭 {epoch+1}/{num_epochs}, 손실: {train_loss / len(data_loader.dataset):.4f}\")\n","\n","        topics = extract_vae_topics(vae_model, vectorizer, num_topics)\n","        return vae_model, topics\n","    except Exception as e:\n","        logging.error(f\"VAE 모델링 중 오류 발생: {e}\")\n","        return None, None\n","\n","# def calculate_coherence(topics, tokenizer, bert_model):\n","#     \"\"\"\n","#     Calculate the coherence score for given topics using BERT embeddings.\n","\n","#     Args:\n","#     topics (list): List of topic word lists.\n","#     tokenizer (BertTokenizer): BERT tokenizer.\n","#     bert_model (BertModel): Pre-trained BERT model.\n","\n","#     Returns:\n","#     float: Average coherence score across all topics.\n","#     \"\"\"\n","#     coherence_scores = []\n","#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","#     bert_model.to(device)\n","#     bert_model.eval()\n","\n","#     for topic_words in topics:\n","#         inputs = tokenizer(topic_words, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","#         with torch.no_grad():\n","#             outputs = bert_model(**inputs)\n","#         embeddings = outputs.last_hidden_state[:, 0, :]  # [CLS] 토큰의 임베딩 사용\n","\n","#         num_words = len(topic_words)\n","#         if num_words < 2:\n","#             coherence_scores.append(0)\n","#             continue\n","\n","#         pairwise_similarities = []\n","#         for i in range(num_words):\n","#             for j in range(i + 1, num_words):\n","#                 cosine_sim = torch.nn.functional.cosine_similarity(embeddings[i], embeddings[j], dim=0)\n","#                 pairwise_similarities.append(cosine_sim.item())\n","\n","#         coherence = np.mean(pairwise_similarities)\n","#         coherence_scores.append(coherence)\n","\n","#     final_coherence = np.mean(coherence_scores) if coherence_scores else 0\n","#     return final_coherence\n","\n","def calculate_coherence(topics, tokenizer, bert_model, top_n=10, batch_size=16):\n","    coherence_scores = []\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    bert_model.to(device)\n","    bert_model.eval()\n","\n","    for topic_words in topics:\n","        # 토픽 단어 수 제한\n","        topic_words = topic_words[:top_n]\n","        # 유효한 단어만 선택\n","        topic_words = [word for word in topic_words if word and isinstance(word, str)]\n","        num_words = len(topic_words)\n","\n","        if num_words < 2:\n","            coherence_scores.append(0)\n","            continue\n","\n","        # 단어 임베딩을 배치로 계산\n","        embeddings = []\n","        for i in range(0, num_words, batch_size):\n","            batch_words = topic_words[i:i + batch_size]\n","            inputs = tokenizer(batch_words, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","            with torch.no_grad():\n","                outputs = bert_model(**inputs)\n","            batch_embeddings = outputs.last_hidden_state[:, 0, :]  # [CLS] 토큰의 임베딩 사용\n","            embeddings.append(batch_embeddings)\n","            # 메모리 관리\n","            del outputs\n","            torch.cuda.empty_cache()\n","\n","        embeddings = torch.cat(embeddings, dim=0)\n","\n","        # 코사인 유사도 계산\n","        pairwise_similarities = []\n","        for i in range(num_words):\n","            for j in range(i + 1, num_words):\n","                cosine_sim = torch.nn.functional.cosine_similarity(embeddings[i], embeddings[j], dim=0)\n","                pairwise_similarities.append(cosine_sim.item())\n","\n","        coherence = np.mean(pairwise_similarities)\n","        coherence_scores.append(coherence)\n","\n","    final_coherence = np.mean(coherence_scores) if coherence_scores else 0\n","    return final_coherence\n","\n","\n","def process_metrics(domain, model_type, topics, data, metrics_list, tokenizer, bert_model):\n","    tokenized_data = [simple_preprocess(doc) for doc in data]\n","    dictionary = Dictionary(tokenized_data)\n","    corpus = [dictionary.doc2bow(text) for text in tokenized_data]\n","\n","    coherence = calculate_coherence(topics, tokenizer, bert_model)\n","    npmi = calculate_npmi(topics, corpus, dictionary)\n","    umass = calculate_umass(topics, corpus, dictionary)\n","\n","    metrics_list.append({\n","        'Domain': domain,\n","        'Model': model_type,\n","        'Coherence': coherence,\n","        'NPMI': npmi,\n","        'U_Mass': umass\n","    })\n","\n","    logging.info(f\"Coherence: {coherence:.4f}, NPMI: {npmi:.4f}, U_Mass: {umass:.4f}\")\n","    \n","    return [metrics_list[-1]]  # 마지막에 추가된 메트릭을 리스트로 반환\n","\n","def calculate_npmi(topics, corpus, dictionary, top_n=10):\n","    # 토픽에서 사용된 모든 단어의 집합 생성\n","    topic_words_set = set()\n","    for topic in topics:\n","        topic_words_set.update(topic[:top_n])\n","\n","    # 단어를 ID로 매핑\n","    word2id = {word: dictionary.token2id[word] for word in topic_words_set if word in dictionary.token2id}\n","    id2word = {id: word for word, id in word2id.items()}\n","\n","    # 단어와 단어 쌍의 문서 빈도 계산\n","    total_docs = len(corpus)\n","    word_doc_freq = defaultdict(int)\n","    pair_doc_freq = defaultdict(int)\n","\n","    for doc in corpus:\n","        doc_word_ids = set([id for id, _ in doc])\n","        topic_word_ids_in_doc = doc_word_ids.intersection(set(word2id.values()))\n","\n","        for word_id in topic_word_ids_in_doc:\n","            word_doc_freq[word_id] += 1\n","\n","        for word_id1, word_id2 in combinations(topic_word_ids_in_doc, 2):\n","            pair = tuple(sorted((word_id1, word_id2)))\n","            pair_doc_freq[pair] += 1\n","\n","    # NPMI 계산\n","    npmi_scores = []\n","    for topic in topics:\n","        topic_word_ids = [word2id[word] for word in topic[:top_n] if word in word2id]\n","        if len(topic_word_ids) < 2:\n","            continue\n","        pair_npmi_scores = []\n","        for word_id1, word_id2 in combinations(topic_word_ids, 2):\n","            pair = tuple(sorted((word_id1, word_id2)))\n","            co_doc_count = pair_doc_freq.get(pair, 0)\n","            if co_doc_count == 0:\n","                continue\n","            p_w1_w2 = co_doc_count / total_docs\n","            p_w1 = word_doc_freq[word_id1] / total_docs\n","            p_w2 = word_doc_freq[word_id2] / total_docs\n","\n","            pmi = np.log(p_w1_w2 / (p_w1 * p_w2) + 1e-12)\n","            npmi = pmi / (-np.log(p_w1_w2 + 1e-12))\n","            pair_npmi_scores.append(npmi)\n","        if pair_npmi_scores:\n","            npmi_scores.append(np.mean(pair_npmi_scores))\n","\n","    return np.mean(npmi_scores) if npmi_scores else float('nan')\n","\n","def calculate_umass(topics, corpus, dictionary, top_n=10):\n","    # 토픽에서 사용된 모든 단어의 집합 생성\n","    topic_words_set = set()\n","    for topic in topics:\n","        topic_words_set.update(topic[:top_n])\n","\n","    # 단어를 ID로 매핑\n","    word2id = {word: dictionary.token2id[word] for word in topic_words_set if word in dictionary.token2id}\n","\n","    # 단어와 단어 쌍의 빈도 계산\n","    word_counts = defaultdict(int)\n","    pair_counts = defaultdict(int)\n","\n","    for doc in corpus:\n","        doc_word_ids = set([id for id, _ in doc])\n","        topic_word_ids_in_doc = doc_word_ids.intersection(set(word2id.values()))\n","\n","        for word_id in topic_word_ids_in_doc:\n","            word_counts[word_id] += 1\n","\n","        for word_id1, word_id2 in combinations(topic_word_ids_in_doc, 2):\n","            pair = tuple(sorted((word_id1, word_id2)))\n","            pair_counts[pair] += 1\n","\n","    # U_Mass 계산\n","    umass_scores = []\n","    for topic in topics:\n","        topic_word_ids = [word2id[word] for word in topic[:top_n] if word in word2id]\n","        if len(topic_word_ids) < 2:\n","            continue\n","        pair_umass_scores = []\n","        for i, word_id1 in enumerate(topic_word_ids):\n","            for word_id2 in topic_word_ids[:i]:\n","                pair = tuple(sorted((word_id1, word_id2)))\n","                co_occurrence = pair_counts.get(pair, 0) + 1  # 스무딩을 위해 +1\n","                word2_count = word_counts[word_id2] + 1  # 스무딩을 위해 +1\n","                umass = np.log(co_occurrence / word2_count)\n","                pair_umass_scores.append(umass)\n","        if pair_umass_scores:\n","            umass_scores.append(np.mean(pair_umass_scores))\n","\n","    return np.mean(umass_scores) if umass_scores else float('nan')\n","\n","# 일치도 분석 함수\n","def analyze_agreement(metrics_df):\n","    agreement_results = {}\n","    \n","    # Add debugging information\n","    print(\"DataFrame columns:\", metrics_df.columns)\n","    print(\"DataFrame head:\")\n","    print(metrics_df.head())\n","    \n","    for domain in metrics_df['Domain'].unique():\n","        domain_df = metrics_df[metrics_df['Domain'] == domain]\n","        for model in ['BERTopic', 'VAE', 'LowBERTopic']:\n","            model_df = domain_df[domain_df['Model'] == model]\n","            if len(model_df) == 0:\n","                continue\n","            \n","            # Check if the required columns exist\n","            required_columns = ['Coherence', 'NPMI', 'U_Mass']\n","            if not all(col in model_df.columns for col in required_columns):\n","                print(f\"Warning: Missing required columns for {domain} - {model}\")\n","                continue\n","            \n","            coherence = model_df['Coherence'].values[0]\n","            npmi = model_df['NPMI'].values[0]\n","            umass = model_df['U_Mass'].values[0]\n","            \n","            # 메트릭 간 일치도 계산\n","            metrics = [coherence, npmi, -umass]  # U_Mass는 낮을수록 좋으므로 부호를 바꿈\n","            agreement = np.std(metrics) / np.mean(metrics)  # 변동계수 (Coefficient of Variation)\n","            \n","            if domain not in agreement_results:\n","                agreement_results[domain] = {}\n","            agreement_results[domain][model] = {\n","                'Coherence': coherence,\n","                'NPMI': npmi,\n","                'U_Mass': umass,\n","                'Agreement': agreement\n","            }\n","    \n","    return agreement_results\n","\n","# 안정성 분석 함수\n","def analyze_stability(datasets, model_types, n_runs=10, sample_ratio=0.8):\n","    stability_results = []\n","    \n","    for domain, domain_datasets in datasets.items():\n","        # 각 도메인에서 첫 번째 데이터셋만 사용\n","        data = next(iter(domain_datasets.values()))\n","        \n","        logging.info(f\"Analyzing stability for domain: {domain}\")\n","        logging.info(f\"Original data type: {type(data)}\")\n","        \n","        # 데이터 형식 확인 및 변환\n","        if isinstance(data, pd.DataFrame):\n","            data = data['text'].tolist() if 'text' in data.columns else data.values.flatten().tolist()\n","        elif isinstance(data, pd.Series):\n","            data = data.tolist()\n","        elif isinstance(data, np.ndarray):\n","            data = data.flatten().tolist()\n","        elif isinstance(data, list):\n","            pass\n","        else:\n","            raise ValueError(f\"Unsupported data format for domain {domain}: {type(data)}\")\n","        \n","        logging.info(f\"Processed data type: {type(data)}\")\n","        logging.info(f\"Sample of processed data: {data[:5]}\")  # 처음 5개 항목 출력\n","        \n","        # BERTopic으로 초기 토픽 수 결정\n","        _, _, num_topics = perform_bertopic_modeling(data)\n","        \n","        for model_type in model_types:\n","            metric_values = {\n","                'Coherence': [],\n","                'NPMI': [],\n","                'U_Mass': []\n","            }\n","            \n","            for _ in range(n_runs):\n","                sampled_data = np.random.choice(data, size=int(len(data) * sample_ratio), replace=False)\n","                sampled_data = sampled_data.tolist()  # numpy array를 리스트로 변환\n","                \n","                logging.info(f\"Sampled data type: {type(sampled_data)}\")\n","                logging.info(f\"Sample of sampled data: {sampled_data[:5]}\")  # 처음 5개 항목 출력\n","                \n","                if model_type == 'BERTopic':\n","                    model, topics, _ = perform_bertopic_modeling(sampled_data)\n","                elif model_type == 'VAE':\n","                    model, topics = perform_vae_topic_modeling(sampled_data, num_topics)\n","                elif model_type == 'LowBERTopic':\n","                    model, topics, _ = perform_lowbertopic_modeling(sampled_data, num_topics)  # num_topics 인자 제거\n","                else:\n","                    raise ValueError(f\"Unsupported model type: {model_type}\")\n","                \n","                if model is None or topics is None:\n","                    logging.warning(f\"Model or topics is None for {model_type} in domain {domain}\")\n","                    continue\n","                \n","                tokenized_data = [simple_preprocess(doc) for doc in sampled_data]\n","                dictionary = Dictionary(tokenized_data)\n","                corpus = [dictionary.doc2bow(text) for text in tokenized_data]\n","                \n","                coherence = calculate_coherence(topics, tokenizer, bert_model)\n","                npmi = calculate_npmi(topics, corpus, dictionary)\n","                umass = calculate_umass(topics, corpus, dictionary)\n","                \n","                metric_values['Coherence'].append(coherence)\n","                metric_values['NPMI'].append(npmi)\n","                metric_values['U_Mass'].append(umass)\n","            \n","            for metric, values in metric_values.items():\n","                cv = np.std(values) / np.mean(values) if np.mean(values) != 0 else float('nan')\n","                stability_results.append({\n","                    'Domain': domain,\n","                    'Model': model_type,\n","                    'Metric': metric,\n","                    'CV': cv\n","                })\n","    \n","    return pd.DataFrame(stability_results)\n","\n","# 개선된 토픽 품질 시각화 함수\n","\n","def visualize_topic_quality(metrics_df):\n","    plt.figure(figsize=(12, 8))\n","    \n","    # Add debugging information\n","    print(\"DataFrame columns:\", metrics_df.columns)\n","    print(\"DataFrame head:\")\n","    print(metrics_df.head())\n","    \n","    # Check if required columns exist\n","    required_columns = ['Model', 'Coherence', 'NPMI']\n","    if not all(col in metrics_df.columns for col in required_columns):\n","        print(f\"Error: Missing required columns. Available columns: {metrics_df.columns}\")\n","        return\n","    \n","    # 모델별로 다른 마커 사용\n","    markers = {'BERTopic': 'o', 'VAE': 's', 'LowBERTopic': '^'}\n","    \n","    for model in ['BERTopic', 'VAE', 'LowBERTopic']:\n","        model_data = metrics_df[metrics_df['Model'] == model]\n","        coherence = model_data['Coherence']\n","        npmi = model_data['NPMI']\n","        plt.scatter(coherence, npmi, label=model, marker=markers[model])\n","    \n","    plt.xlabel('Coherence')\n","    plt.ylabel('NPMI')\n","    plt.title('Topic Quality: Coherence vs NPMI')\n","    plt.legend()\n","    \n","    # 도메인 레이블 추가\n","    for _, row in metrics_df.iterrows():\n","        plt.annotate(row['Domain'], (row['Coherence'], row['NPMI']), xytext=(5, 5), \n","                     textcoords='offset points', fontsize=8)\n","    \n","    plt.tight_layout()\n","    plt.savefig('topic_quality.png')\n","    plt.close()\n","    \n","    logging.info(\"토픽 품질 시각화 완료: topic_quality.png\")\n","\n","\n","def analyze_llm_results(llm_df):\n","    llm_df['LLM_Avg_Score'] = llm_df['LLM_Scores'].apply(lambda scores: np.mean([s for s in scores if s is not None]))\n","    llm_df['LLM_Std_Score'] = llm_df['LLM_Scores'].apply(lambda scores: np.std([s for s in scores if s is not None]))\n","    llm_df['LLM_Median_Score'] = llm_df['LLM_Scores'].apply(lambda scores: np.median([s for s in scores if s is not None]))\n","\n","    print(\"\\nLLM 평가 결과:\")\n","    print(llm_df[['Domain', 'Model', 'LLM_Avg_Score', 'LLM_Std_Score', 'LLM_Median_Score']])\n","\n","def llm_auto_metric_correlation(metrics_df, llm_df):\n","    merged_df = pd.merge(metrics_df, llm_df, on=['Domain', 'Model'])\n","\n","    metric_names = ['Coherence', 'NPMI', 'U_Mass']\n","    for metric in metric_names:\n","        valid_idx = merged_df['LLM_Avg_Score'].notnull()\n","        pearson_corr, p_value_pearson = stats.pearsonr(merged_df.loc[valid_idx, metric], merged_df.loc[valid_idx, 'LLM_Avg_Score'])\n","        spearman_corr, p_value_spearman = stats.spearmanr(merged_df.loc[valid_idx, metric], merged_df.loc[valid_idx, 'LLM_Avg_Score'])\n","        print(f\"\\nLLM 평가 점수와 {metric}의 상관관계:\")\n","        print(f\"Pearson: 상관계수 = {pearson_corr:.4f}, p-value = {p_value_pearson:.4f}\")\n","        print(f\"Spearman: 상관계수 = {spearman_corr:.4f}, p-value = {p_value_spearman:.4f}\")\n","\n","def verify_llm_consistency(topics, documents, n_repeats=5):\n","    all_scores = []\n","    for _ in range(n_repeats):\n","        scores, _ = llm_evaluation(topics, documents)\n","        all_scores.append(scores)\n","    all_scores = np.array(all_scores)\n","    std_scores = np.std(all_scores, axis=0)\n","    avg_std = np.mean(std_scores)\n","    cv_scores = std_scores / np.mean(all_scores, axis=0)\n","    avg_cv = np.mean(cv_scores)\n","    print(f\"\\nLLM 평가의 평균 표준편차: {avg_std:.4f}\")\n","    print(f\"LLM 평가의 평균 변동계수(CV): {avg_cv:.4f}\")\n","\n","def analyze_llm_feedback(llm_df):\n","    all_words = []\n","    for feedbacks in llm_df['LLM_Feedbacks']:\n","        for feedback in feedbacks:\n","            words = feedback.lower().split()\n","            all_words.extend([word for word in words if word not in stop_words])\n","\n","    word_freq = Counter(all_words)\n","    print(\"\\n피드백에서 가장 자주 등장하는 키워드:\")\n","    for word, count in word_freq.most_common(10):\n","        print(f\"{word}: {count}\")\n","\n","    coherence_keywords = ['coherent', 'consistent', 'related', 'connected', 'meaningful']\n","    print(\"\\n일관성 관련 키워드 빈도:\")\n","    for keyword in coherence_keywords:\n","        print(f\"{keyword}: {word_freq[keyword]}\")\n","\n","    positive_keywords = ['good', 'great', 'excellent', 'well', 'clear']\n","    negative_keywords = ['poor', 'bad', 'unclear', 'confusing', 'unrelated']\n","    \n","    positive_count = sum(word_freq[word] for word in positive_keywords)\n","    negative_count = sum(word_freq[word] for word in negative_keywords)\n","    \n","    print(f\"\\n긍정적 피드백 키워드 수: {positive_count}\")\n","    print(f\"부정적 피드백 키워드 수: {negative_count}\")\n","\n","    relationship_keywords = ['related', 'similar', 'overlapping', 'connected', 'distinct']\n","    print(\"\\n토픽 간 관계 관련 키워드 빈도:\")\n","    for keyword in relationship_keywords:\n","        print(f\"{keyword}: {word_freq[keyword]}\")\n","\n","    quality_keywords = ['coherent', 'meaningful', 'interpretable', 'clear', 'specific']\n","    print(\"\\n토픽 품질 관련 키워드 빈도:\")\n","    for keyword in quality_keywords:\n","        print(f\"{keyword}: {word_freq[keyword]}\")\n","\n","    scores = [score for scores in llm_df['LLM_Scores'] for score in scores if score is not None]\n","    print(\"\\n일관성 점수 분포:\")\n","    print(f\"평균: {np.mean(scores):.2f}\")\n","    print(f\"중앙값: {np.median(scores):.2f}\")\n","    print(f\"표준편차: {np.std(scores):.2f}\")\n","    print(f\"최소값: {np.min(scores):.2f}\")\n","    print(f\"최대값: {np.max(scores):.2f}\")\n","\n","    print(\"\\n모델별 평균 일관성 점수:\")\n","    for model in llm_df['Model'].unique():\n","        model_scores = [score for scores, m in zip(llm_df['LLM_Scores'], llm_df['Model']) \n","                        for score in scores if score is not None and m == model]\n","        print(f\"{model}: {np.mean(model_scores):.2f}\")\n","\n","def visualize_llm_results(llm_df):\n","    plt.figure(figsize=(12, 6))\n","    sns.boxplot(x='Model', y='LLM_Avg_Score', data=llm_df)\n","    plt.title('모델별 LLM 평가 점수 분포')\n","    plt.savefig('llm_model_score_distribution.png')\n","    plt.close()\n","\n","    plt.figure(figsize=(12, 6))\n","    sns.scatterplot(x='Model', y='LLM_Avg_Score', data=llm_df)\n","    plt.title('모델별 LLM 평가 점수')\n","    plt.legend()\n","    plt.savefig('llm_model_score.png')\n","    plt.close()\n","\n","\n","def evaluate_coherence_stability(models, domains, datasets, n_runs=5):\n","    stability_results = []\n","    \n","    for model in models:\n","        for domain, data in zip(domains, datasets):\n","            # data가 딕셔너리인 경우 적절히 처리\n","            if isinstance(data, dict):\n","                data = list(data.values())[0]\n","            elif isinstance(data, pd.DataFrame):\n","                data = data['text'].tolist()\n","            elif not isinstance(data, list):\n","                raise ValueError(f\"Unsupported data format for domain {domain}\")\n","\n","            coherence_scores = []\n","            npmi_scores = []\n","            umass_scores = []\n","\n","            for _ in range(n_runs):\n","                # 데이터 샘플링 (예: 80%의 데이터 사용)\n","                sampled_data = np.random.choice(data, size=int(len(data) * 0.8), replace=False)\n","\n","                if model == 'BERTopic':\n","                    _, topics, _ = perform_bertopic_modeling(sampled_data)\n","                elif model == 'VAE':\n","                    _, topics = perform_vae_topic_modeling(sampled_data, num_topics=10)  # num_topics는 적절히 조정\n","                elif model == 'LowBERTopic':\n","                    _, topics, _ = perform_lowbertopic_modeling(sampled_data, num_topics=10)  # num_topics 인자 제거\n","                else:                    \n","                    raise ValueError(f\"Unsupported model type: {model}\")\n","\n","                # 토큰화된 데이터 준비\n","                tokenized_data = [simple_preprocess(doc) for doc in sampled_data]\n","                dictionary = Dictionary(tokenized_data)\n","                corpus = [dictionary.doc2bow(text) for text in tokenized_data]\n","\n","                # 일관성 메트릭 계산\n","                coherence = calculate_coherence(topics, tokenizer, bert_model)\n","                npmi = calculate_npmi(topics, corpus, dictionary)\n","                umass = calculate_umass(topics, corpus, dictionary)\n","\n","                coherence_scores.append(coherence)\n","                npmi_scores.append(npmi)\n","                umass_scores.append(umass)\n","\n","            # 안정성 계산 (변동 계수 사용)\n","            coherence_stability = np.std(coherence_scores) / np.mean(coherence_scores)\n","            npmi_stability = np.std(npmi_scores) / np.mean(npmi_scores)\n","            umass_stability = np.std(umass_scores) / np.mean(umass_scores)\n","\n","            stability_results.append({\n","                'Model': model,\n","                'Domain': domain,\n","                'Coherence_Stability': coherence_stability,\n","                'NPMI_Stability': npmi_stability,\n","                'UMass_Stability': umass_stability,\n","                'Mean_Coherence': np.mean(coherence_scores),\n","                'Mean_NPMI': np.mean(npmi_scores),\n","                'Mean_UMass': np.mean(umass_scores)\n","            })\n","\n","    return pd.DataFrame(stability_results)\n","\n","def print_results(metrics_df, agreement_results, stability_df, stability_results):\n","    logging.info(\"\\n=== 결과 분석 ===\")\n","    \n","    # 모델별 지표 평균 성능 출력\n","    logging.info(\"\\n모델별 지표 평균 성능:\")\n","    models = metrics_df['Model'].unique()\n","    for model in models:\n","        model_metrics = metrics_df[metrics_df['Model'] == model]\n","        mean_metrics = model_metrics[['Coherence', 'NPMI', 'U_Mass']].mean()\n","        logging.info(f\"\\n모델: {model}\")\n","        logging.info(f\"  - Coherence 평균: {mean_metrics['Coherence']:.4f}\")\n","        logging.info(f\"  - NPMI 평균: {mean_metrics['NPMI']:.4f}\")\n","        logging.info(f\"  - U_Mass 평균: {mean_metrics['U_Mass']:.4f}\")\n","    \n","    # 도메인별 지표 평균 성능 출력\n","    logging.info(\"\\n도메인별 지표 평균 성능:\")\n","    domains = metrics_df['Domain'].unique()\n","    for domain in domains:\n","        domain_metrics = metrics_df[metrics_df['Domain'] == domain]\n","        mean_metrics = domain_metrics[['Coherence', 'NPMI', 'U_Mass']].mean()\n","        logging.info(f\"\\n도메인: {domain}\")\n","        logging.info(f\"  - Coherence 평균: {mean_metrics['Coherence']:.4f}\")\n","        logging.info(f\"  - NPMI 평균: {mean_metrics['NPMI']:.4f}\")\n","        logging.info(f\"  - U_Mass 평균: {mean_metrics['U_Mass']:.4f}\")\n","    \n","    # 지표 간 상관관계 분석 결과 출력\n","    logging.info(\"\\n지표 간 일치도 분석 결과:\")\n","    for domain, model_results in agreement_results.items():\n","        logging.info(f\"\\n도메인: {domain}\")\n","        for model, metrics in model_results.items():\n","            logging.info(f\"  모델: {model}\")\n","            for metric, value in metrics.items():\n","                logging.info(f\"    {metric}: {value:.4f}\")\n","    \n","    # 일관성 지표의 안정성 결과 출력\n","    logging.info(\"\\n일관성지표 안정성 개별 결과:\")\n","    logging.info(stability_results)\n","    logging.info(\"\\n일관성지표 안정성 전체 결과:\")\n","    stability_summary = stability_df.groupby(['Model', 'Metric'])['CV'].mean().reset_index()\n","    for _, row in stability_summary.iterrows():\n","        logging.info(f\"모델: {row['Model']}, 지표: {row['Metric']}, CV 평균: {row['CV']:.4f}\")\n","    \n","    logging.info(\"\\n분석 완료. 결과를 확인하고 해석하세요.\")\n","\n","def process_datasets(datasets):\n","    all_metrics = []\n","    bertopic_results = {}\n","    vae_results = {}\n","    lowbertopic_results = {}  # LowBERTopic 결과 저장을 위한 딕셔너리 추가\n","    \n","    for domain, domain_datasets in datasets.items():\n","        for dataset_name, data in domain_datasets.items():\n","            # BERTopic 모델링\n","            bertopic_model, bertopic_topics, num_topics = perform_bertopic_modeling(data)\n","            bertopic_results[domain] = {\n","                'num_topics': num_topics,\n","                'topics': bertopic_topics\n","            }\n","            \n","            # BERTopic 메트릭 계산\n","            bertopic_metrics = process_metrics(domain, 'BERTopic', bertopic_topics, data, [], tokenizer, bert_model)\n","            all_metrics.extend(bertopic_metrics)\n","            \n","            # VAE 모델링 (BERTopic의 토픽 수 사용)\n","            vae_model, vae_topics = perform_vae_topic_modeling(data, num_topics)\n","            vae_results[domain] = {\n","                'num_topics': num_topics,\n","                'topics': vae_topics\n","            }\n","            \n","            # VAE 메트릭 계산\n","            vae_metrics = process_metrics(domain, 'VAE', vae_topics, data, [], tokenizer, bert_model)\n","            all_metrics.extend(vae_metrics)\n","            \n","            # LowBERTopic 모델링 추가\n","            lowbertopic_model, lowbertopic_topics, low_num_topics = perform_lowbertopic_modeling(data, low_num_topics=num_topics)\n","            lowbertopic_results[domain] = {\n","                'num_topics': low_num_topics,\n","                'topics': lowbertopic_topics\n","            }\n","            \n","            # LowBERTopic 메트릭 계산\n","            lowbertopic_metrics = process_metrics(domain, 'LowBERTopic', lowbertopic_topics, data, [], tokenizer, bert_model)\n","            all_metrics.extend(lowbertopic_metrics)\n","    \n","    return all_metrics, bertopic_results, vae_results, lowbertopic_results\n","\n","\n","def main():\n","    load_bert_model()\n","    try:\n","        logging.info(\"데이터셋 로딩 시작\")\n","        datasets = load_all_datasets()\n","        \n","        logging.info(\"토픽 모델링 및 메트릭 계산 시작\")\n","        all_metrics, bertopic_results, vae_results, lowbertopic_results = process_datasets(datasets)\n","        \n","        # 토픽 정보를 저장할 DataFrame 생성\n","        topics_df = pd.DataFrame(columns=['Domain', 'Model', 'Topics'])\n","        \n","        # BERTopic 결과 처리 및 저장\n","        logging.info(\"BERTopic 결과 출력 및 저장\")\n","        for domain, result in bertopic_results.items():\n","            print(f\"\\n도메인: {domain}\")\n","            print(f\"BERTopic 토픽 수: {result['num_topics']}\")\n","            print(\"BERTopic 토픽:\")\n","            for i, topic in enumerate(result['topics']):\n","                print(f\"  토픽 {i+1}: {', '.join(topic[:10])}\")\n","            \n","            # topics_df에 추가\n","            topics_df = pd.concat([topics_df, pd.DataFrame({\n","                'Domain': [domain],\n","                'Model': ['BERTopic'],\n","                'Topics': [result['topics']]\n","            })], ignore_index=True)\n","        \n","        # VAE 결과 처리 및 저장\n","        logging.info(\"\\nVAE 결과 출력 및 저장\")\n","        for domain, result in vae_results.items():\n","            print(f\"\\n도메인: {domain}\")\n","            print(f\"VAE 토픽 수: {result['num_topics']}\")\n","            print(\"VAE 토픽:\")\n","            for i, topic in enumerate(result['topics']):\n","                print(f\"  토픽 {i+1}: {', '.join(topic[:10])}\")\n","            \n","            # topics_df에 추가\n","            topics_df = pd.concat([topics_df, pd.DataFrame({\n","                'Domain': [domain],\n","                'Model': ['VAE'],\n","                'Topics': [result['topics']]\n","            })], ignore_index=True)\n","        \n","        # LowBERTopic 결과 처리 및 저장\n","        logging.info(\"\\nLowBERTopic 결과 출력 및 저장\")\n","        for domain, result in lowbertopic_results.items():\n","            print(f\"\\n도메인: {domain}\")\n","            print(f\"LowBERTopic 토픽 수: {result['num_topics']}\")\n","            print(\"LowBERTopic 토픽:\")\n","            for i, topic in enumerate(result['topics']):\n","                print(f\"  토픽 {i+1}: {', '.join(topic[:10])}\")\n","            \n","            # topics_df에 추가\n","            topics_df = pd.concat([topics_df, pd.DataFrame({\n","                'Domain': [domain],\n","                'Model': ['LowBERTopic'],\n","                'Topics': [result['topics']]\n","            })], ignore_index=True)\n","        \n","        # topics_df를 CSV 파일로 저장\n","        topics_df.to_csv('topics_df.csv', index=False)\n","        logging.info(\"토픽 정보가 topics_df.csv 파일로 저장되었습니다.\")\n","        \n","        logging.info(\"메트릭 분석 시작\")\n","        metrics_df = pd.DataFrame(all_metrics)\n","        \n","        # Add debugging information\n","        print(\"Metrics DataFrame shape:\", metrics_df.shape)\n","        print(\"Metrics DataFrame columns:\", metrics_df.columns)\n","        print(\"Metrics DataFrame head:\")\n","        print(metrics_df.head())\n","        \n","        metrics_df.to_csv('topic_modeling_metrics.csv', index=False)\n","        \n","        logging.info(\"일치도 분석 시작\")\n","        agreement_results = analyze_agreement(metrics_df)\n","        \n","        logging.info(\"안정성 분석 시작\")\n","        stability_df = analyze_stability(datasets, ['BERTopic', 'VAE', 'LowBERTopic'])\n","        \n","        logging.info(\"토픽 품질 시각화 시작\")\n","        visualize_topic_quality(metrics_df)\n","        \n","        logging.info(\"일관성 안정성 평가 시작\")\n","        stability_results = evaluate_coherence_stability(['BERTopic', 'VAE', 'LowBERTopic'], list(datasets.keys()), list(datasets.values()))\n","        \n","        logging.info(\"결과 출력 시작\")\n","        print_results(metrics_df, agreement_results, stability_df, stability_results)\n","        \n","        logging.info(\"모든 분석 완료\")\n","    except Exception as e:\n","        logging.error(f\"메인 함수 실행 중 예상치 못한 오류 발생: {e}\")\n","        raise\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Cell 2: LLM evaluation related functions and execution code\n","import os\n","import anthropic\n","from tenacity import retry, stop_after_attempt, wait_random_exponential\n","\n","@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(3))\n","def call_anthropic_api(prompt: str, max_tokens_to_sample: int = 3000) -> str:\n","    anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n","    if not anthropic_api_key:\n","        raise ValueError(\"Anthropic API key not found in environment variable ANTHROPIC_API_KEY\")\n","    \n","    client = anthropic.Anthropic(api_key=anthropic_api_key)\n","    \n","    try:\n","        logging.info(\"Calling Anthropic API...\")\n","        response = client.completions.create(\n","            model=\"claude-2\",\n","            prompt=f\"{anthropic.HUMAN_PROMPT} {prompt} {anthropic.AI_PROMPT}\",\n","            max_tokens_to_sample=max_tokens_to_sample,\n","            temperature=0,\n","        )\n","        logging.info(\"Anthropic API call successful\")\n","        return response.completion\n","    except Exception as e:\n","        logging.error(f\"Error in Anthropic API call: {e}\")\n","        logging.error(f\"Client object: {client}\")\n","        logging.error(f\"Prompt: {prompt[:100]}...\")  # Log only the first 100 characters\n","        raise\n","\n","\n","def llm_evaluation(topics):\n","    scores = []\n","    feedbacks = []\n","\n","    prompt = f\"\"\"\n","    Evaluate the following topics based on their coherence. Coherence is an important metric for assessing the quality of topic modeling:\n","\n","    1. Coherence measures how semantically related the words within each topic are.\n","    2. It is typically calculated by considering the co-occurrence probabilities of word pairs within the topic.\n","    3. Higher coherence scores indicate that the words in a topic are closely related and form a meaningful theme.\n","    4. Lower coherence scores suggest that the topic may be less meaningful or coherent.\n","\n","    Please evaluate the following topics. For each topic, provide a coherence score on a scale of 1-10 and explain your reasoning:\n","\n","    {topics}\n","\n","    When evaluating, consider:\n","    1. How semantically related are the words within each topic?\n","    2. How clear and interpretable is the topic?\n","    3. Do the words in the topic represent a consistent theme or concept?\n","\n","    Please respond for each topic in the following format:\n","    Topic X: [score]\n","    Reason: [explanation]\n","    \"\"\"\n","\n","    try:\n","        evaluation = call_anthropic_api(prompt)\n","\n","        # Updated parsing logic to extract structured responses\n","        topic_evaluations = re.findall(r\"Topic \\d+:.*?(?=Topic \\d+:|$)\", evaluation, re.DOTALL)\n","        for eval in topic_evaluations:\n","            score_match = re.search(r'Topic (\\d+):\\s*(\\d+)', eval)\n","            reason_match = re.search(r'Reason:\\s*(.*)', eval, re.DOTALL)\n","            if score_match and reason_match:\n","                topic_score = int(score_match.group(2))\n","                if 1 <= topic_score <= 10:\n","                    scores.append(topic_score)\n","                    feedbacks.append(reason_match.group(1).strip())\n","                else:\n","                    print(f\"Invalid score (not between 1 and 10): {eval}\")\n","            else:\n","                print(f\"Could not extract score or reason: {eval}\")\n","\n","    except Exception as e:\n","        print(f\"Unexpected error: {e}\")\n","        raise\n","\n","    return scores, feedbacks\n","\n","def run_llm_evaluation(sample_size=100, chunk_size=10):\n","    topics_df = pd.read_csv('topics_df.csv')\n","    llm_results = []\n","    actual_sample_size = min(sample_size, len(topics_df))\n","    \n","    for index, row in tqdm(topics_df.sample(n=actual_sample_size, random_state=42).iterrows(), total=actual_sample_size):\n","        domain = row['Domain']\n","        model_type = row['Model']\n","        topics = eval(row['Topics'])  # Convert string to list\n","        \n","        logging.info(f\"LLM evaluation in progress - Domain: {domain}, Model: {model_type}\")\n","\n","        try:\n","            scores, feedbacks = llm_evaluation(topics)  # Removed documents argument\n","\n","            result = {\n","                'Domain': domain,\n","                'Model': model_type,\n","                'LLM_Scores': scores,\n","                'LLM_Feedbacks': feedbacks\n","            }\n","            llm_results.append(result)\n","\n","            if len(llm_results) % chunk_size == 0:\n","                save_results_chunk(llm_results[-chunk_size:])\n","                \n","        except Exception as e:\n","            logging.error(f\"Error processing {domain} - {model_type}: {str(e)}\")\n","            continue\n","\n","    if len(llm_results) % chunk_size != 0:\n","        save_results_chunk(llm_results[-(len(llm_results) % chunk_size):])\n","\n","    llm_df = pd.DataFrame(llm_results)\n","    return llm_df\n","\n","def save_results_chunk(results_chunk):\n","    with open('llm_evaluation_results.json', 'a') as f:\n","        for result in results_chunk:\n","            json.dump(result, f)\n","            f.write('\\n')\n","\n","# Execute LLM evaluation\n","logging.info(\"Starting LLM evaluation\")\n","metrics_df = pd.read_csv('topic_modeling_metrics.csv')\n","\n","# Removed topics_df argument\n","llm_df = run_llm_evaluation()\n","analyze_llm_results(llm_df)\n","visualize_llm_results(llm_df)\n","\n","logging.info(\"Starting correlation analysis between LLM evaluation and automatic metrics\")\n","llm_auto_metric_correlation(metrics_df, llm_df)\n","\n","logging.info(\"Starting LLM feedback analysis\")\n","analyze_llm_feedback(llm_df)\n","\n","logging.info(\"LLM evaluation completed\")"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyN1HMc8DJjEA0c4VoVpICAu","gpuType":"T4","mount_file_id":"1A2KBLTvWLDpZRfvqTW6X-K99HG5QCvQ-","provenance":[]},"kernelspec":{"display_name":"topic","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}
