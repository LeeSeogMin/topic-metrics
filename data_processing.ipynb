{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 18846 documents to 20newsgroups_data.json\n"
     ]
    }
   ],
   "source": [
    "# 20news\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import json\n",
    "\n",
    "# Fetch the 20 newsgroups dataset\n",
    "docs = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))['data']\n",
    "\n",
    "# Save the text data to a JSON file\n",
    "with open('20newsgroups_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(docs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved {len(docs)} documents to 20newsgroups_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 18846 preprocessed documents to 20newsgroups_preprocessed.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the JSON file\n",
    "with open('20newsgroups_data.json', 'r', encoding='utf-8') as f:\n",
    "    docs = json.load(f)\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Preprocess all documents\n",
    "preprocessed_docs = [preprocess_text(doc) for doc in docs]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'text': preprocessed_docs})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('20newsgroups_preprocessed.csv', index=False)\n",
    "\n",
    "print(f\"Saved {len(preprocessed_docs)} preprocessed documents to 20newsgroups_preprocessed.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed 50000 texts and saved to preprocessed_tweets.csv\n",
      "\n",
      "First 5 preprocessed samples:\n",
      "541200                           chrishasboobs ahhh hope ok\n",
      "750                      misstoriblack cool tweet apps razr\n",
      "766711    tiannachaos know family drama lamehey next tim...\n",
      "285055    school email wont open geography stuff revise ...\n",
      "705995                                 upper airway problem\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv('training.noemoticon.csv', encoding='latin-1', names=['target', 'id', 'date', 'flag', 'user', 'text'])\n",
    "\n",
    "# 50000개 샘플 무작위 추출\n",
    "sampled_df = df.sample(n=50000, random_state=42)\n",
    "\n",
    "# 'text' 열만 선택\n",
    "text_samples = sampled_df['text']\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Preprocess the sampled texts\n",
    "preprocessed_texts = text_samples.apply(preprocess_text)\n",
    "\n",
    "# Save preprocessed texts to a new CSV file\n",
    "preprocessed_texts.to_csv('preprocessed_tweets.csv', index=False, header=True)\n",
    "\n",
    "print(f\"Preprocessed {len(preprocessed_texts)} texts and saved to preprocessed_tweets.csv\")\n",
    "print(\"\\nFirst 5 preprocessed samples:\")\n",
    "print(preprocessed_texts.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After whitespace processing:\n",
      "541200                           chrishasboobs ahhh hope ok\n",
      "750                      misstoriblack cool tweet apps razr\n",
      "766711    tiannachaos know family drama lamehey next tim...\n",
      "285055    school email wont open geography stuff revise ...\n",
      "705995                                 upper airway problem\n",
      "Name: text, dtype: object\n",
      "\n",
      "Saved 50000 whitespace-processed texts to preprocessed_tweets_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# Remove leading and trailing whitespace from preprocessed texts\n",
    "preprocessed_texts = preprocessed_texts.str.strip()\n",
    "\n",
    "# Remove extra spaces between words\n",
    "preprocessed_texts = preprocessed_texts.str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "# Remove any remaining leading spaces\n",
    "preprocessed_texts = preprocessed_texts.str.lstrip()\n",
    "\n",
    "print(\"\\nAfter whitespace processing:\")\n",
    "print(preprocessed_texts.head())\n",
    "\n",
    "# Save the whitespace-processed texts to a new CSV file\n",
    "preprocessed_texts.to_csv('preprocessed_tweets_clean.csv', index=False, header=True)\n",
    "\n",
    "print(f\"\\nSaved {len(preprocessed_texts)} whitespace-processed texts to preprocessed_tweets_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리된 샘플 수: 23000\n",
      "\n",
      "처음 5개 전처리된 샘플:\n",
      "1297     agree reviewer color isnt pink person subtle t...\n",
      "7228             many sweater bought sale soft comfortable\n",
      "21410    beautiful top run whole size small gave daught...\n",
      "8358     loved pantsthey comfortable however wearing tw...\n",
      "20840    ordered dress petite arrived regular rest fit ...\n",
      "Name: Review.Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv('Women_s_E-Commerce_Clothing_Reviews_1594_1.csv', delimiter=';')\n",
    "\n",
    "# 'Review.Text' 열만 추출\n",
    "reviews = df['Review.Text']\n",
    "\n",
    "# 5000개 샘플 무작위 추출\n",
    "sampled_reviews = reviews.sample(n=23000, random_state=42)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 입력값이 문자열이 아닐 경우 빈 문자열로 처리\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    # 소문자 변환\n",
    "    text = text.lower()\n",
    "    # 숫자 및 특수 문자 제거\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # 숫자 제거\n",
    "    # 토큰화\n",
    "    tokens = word_tokenize(text)\n",
    "    # 불용어 제거 및 lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# 전처리 적용\n",
    "preprocessed_reviews = sampled_reviews.apply(preprocess_text)\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"전처리된 샘플 수: {len(preprocessed_reviews)}\")\n",
    "print(\"\\n처음 5개 전처리된 샘플:\")\n",
    "print(preprocessed_reviews.head())\n",
    "\n",
    "# 전처리된 결과를 새 CSV 파일로 저장\n",
    "preprocessed_reviews.to_csv('preprocessed_clothing_reviews.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 및 필터링된 샘플 수: 20574\n",
      "\n",
      "처음 5개 전처리된 샘플:\n",
      "1297     agree reviewer color isnt pink person subtle t...\n",
      "8358     loved pantsthey comfortable however wearing tw...\n",
      "20840    ordered dress petite arrived regular rest fit ...\n",
      "8193     cute shirt picked whitewithflowers shown onlin...\n",
      "17261    send back exchange way big im excited get smal...\n",
      "Name: Review.Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# CSV 파일 읽기\n",
    "df = pd.read_csv('Women_s_E-Commerce_Clothing_Reviews_1594_1.csv', delimiter=';')\n",
    "\n",
    "# 'Review.Text' 열만 추출\n",
    "reviews = df['Review.Text']\n",
    "\n",
    "# 23,000개 샘플 무작위 추출\n",
    "sampled_reviews = reviews.sample(n=23000, random_state=42)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 입력값이 문자열이 아닐 경우 빈 문자열로 처리\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    # 소문자 변환\n",
    "    text = text.lower()\n",
    "    # 숫자 및 특수 문자 제거\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # 숫자 및 특수 문자 제거\n",
    "    # 토큰화\n",
    "    tokens = word_tokenize(text)\n",
    "    # 불용어 제거 및 표제어 추출\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def filter_short_texts(texts, min_length=10):\n",
    "    \"\"\"\n",
    "    최소 단어 수(min_length) 이상의 텍스트만 보존합니다.\n",
    "\n",
    "    :param texts: 텍스트 시리즈 또는 리스트\n",
    "    :param min_length: 최소 단어 수\n",
    "    :return: 필터링된 텍스트 시리즈 또는 리스트\n",
    "    \"\"\"\n",
    "    filtered_texts = texts[texts.apply(lambda x: len(x.split()) >= min_length)]\n",
    "    return filtered_texts\n",
    "\n",
    "# 전처리 적용\n",
    "preprocessed_reviews = sampled_reviews.apply(preprocess_text)\n",
    "\n",
    "# 빈 문자열 제거\n",
    "preprocessed_reviews = preprocessed_reviews[preprocessed_reviews != '']\n",
    "\n",
    "# 짧은 텍스트 필터링 (예: 단어 수 10개 이상인 텍스트만 보존)\n",
    "filtered_reviews = filter_short_texts(preprocessed_reviews, min_length=10)\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"전처리 및 필터링된 샘플 수: {len(filtered_reviews)}\")\n",
    "print(\"\\n처음 5개 전처리된 샘플:\")\n",
    "print(filtered_reviews.head())\n",
    "\n",
    "# 전처리된 결과를 새 CSV 파일로 저장 (빈 행 제거)\n",
    "filtered_reviews.to_csv('preprocessed_clothing_reviews.csv', index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리된 샘플 수: 45200\n",
      "\n",
      "처음 5개 전처리된 샘플:\n",
      "0    wont create new opportunity well also uncover ...\n",
      "1    glorious week aol chief executive tim armstron...\n",
      "2       protester refused leave remained site thursday\n",
      "3    scientist didnt know human played game well la...\n",
      "4    five way get beauty sleep hosting sleepover child\n",
      "Name: short_description, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# JSON 파일 읽기\n",
    "with open('News_Dataset_v3.json', 'r', encoding='utf-8') as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "\n",
    "# DataFrame으로 변환\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 'short_description' 열만 추출\n",
    "descriptions = df['short_description']\n",
    "\n",
    "# 50000개 샘플 무작위 추출\n",
    "sampled_descriptions = descriptions.sample(n=50000, random_state=42)\n",
    "\n",
    "# 전처리 적용\n",
    "preprocessed_descriptions = sampled_descriptions.apply(preprocess_text)\n",
    "\n",
    "# 빈 샘플 제거\n",
    "preprocessed_descriptions = preprocessed_descriptions[preprocessed_descriptions != '']\n",
    "\n",
    "# 인덱스 재설정\n",
    "preprocessed_descriptions = preprocessed_descriptions.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"전처리된 샘플 수: {len(preprocessed_descriptions)}\")\n",
    "print(\"\\n처음 5개 전처리된 샘플:\")\n",
    "print(preprocessed_descriptions.head())\n",
    "\n",
    "# 전처리된 결과를 새 CSV 파일로 저장\n",
    "preprocessed_descriptions.to_csv('preprocessed_news_descriptions.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리된 샘플 수: 26908\n",
      "\n",
      "처음 5개 전처리된 샘플:\n",
      "0    geographic spread novel coronavirus covid infe...\n",
      "1    december case unidentified pneumonia history e...\n",
      "2    basic reproduction number infectious agent ave...\n",
      "3    initial cluster severe pneumonia case triggere...\n",
      "4    cruise ship carry large number people confined...\n",
      "Name: abstract, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv('covid_abstract.csv')\n",
    "\n",
    "# 'abstract' 열만 선택\n",
    "abstracts = df['abstract']\n",
    "\n",
    "# 전처리 적용\n",
    "preprocessed_abstracts = abstracts.apply(preprocess_text)\n",
    "\n",
    "# 빈 샘플 제거\n",
    "preprocessed_abstracts = preprocessed_abstracts[preprocessed_abstracts != '']\n",
    "\n",
    "# 인덱스 재설정\n",
    "preprocessed_abstracts = preprocessed_abstracts.reset_index(drop=True)\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"전처리된 샘플 수: {len(preprocessed_abstracts)}\")\n",
    "print(\"\\n처음 5개 전처리된 샘플:\")\n",
    "print(preprocessed_abstracts.head())\n",
    "\n",
    "# 전처리된 결과를 새 CSV 파일로 저장\n",
    "preprocessed_abstracts.to_csv('preprocessed_covid_abstracts.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리된 샘플 수: 36833\n",
      "\n",
      "처음 5개 전처리된 샘플:\n",
      "0    family mormon never tried explain still stare ...\n",
      "1    buddhism much lot compatible christianity espe...\n",
      "2    seriously say thing first get complex explain ...\n",
      "3    learned want teach different focus goal wrappi...\n",
      "4    benefit may want read living buddha living chr...\n",
      "Name: clean_comment, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv('Reddit_Data.csv')\n",
    "\n",
    "# 'clean_comment' 열만 선택\n",
    "clean_comments = df['clean_comment']\n",
    "\n",
    "# 전처리 적용\n",
    "preprocessed_comments = clean_comments.apply(preprocess_text)\n",
    "\n",
    "# 빈 샘플 제거\n",
    "preprocessed_comments = preprocessed_comments[preprocessed_comments != '']\n",
    "\n",
    "# 인덱스 재설정\n",
    "preprocessed_comments = preprocessed_comments.reset_index(drop=True)\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"전처리된 샘플 수: {len(preprocessed_comments)}\")\n",
    "print(\"\\n처음 5개 전처리된 샘플:\")\n",
    "print(preprocessed_comments.head())\n",
    "\n",
    "# 전처리된 결과를 새 CSV 파일로 저장\n",
    "preprocessed_comments.to_csv('preprocessed_reddit_comments.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리된 샘플 수: 120000\n",
      "\n",
      "처음 5개 전처리된 샘플:\n",
      "0    reuters shortsellers wall street dwindlingband...\n",
      "1    reuters private investment firm carlyle groupw...\n",
      "2    reuters soaring crude price plus worriesabout ...\n",
      "3    reuters authority halted oil exportflows main ...\n",
      "4    afp tearaway world oil price toppling record s...\n",
      "Name: article, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv('agnews.csv', encoding='utf-8')\n",
    "\n",
    "# 'Description' 열만 선택\n",
    "descriptions = df['article']\n",
    "\n",
    "# 전처리 적용\n",
    "preprocessed_descriptions = descriptions.apply(preprocess_text)\n",
    "\n",
    "# 빈 샘플 제거\n",
    "preprocessed_descriptions = preprocessed_descriptions[preprocessed_descriptions != '']\n",
    "\n",
    "# 인덱스 재설정\n",
    "preprocessed_descriptions = preprocessed_descriptions.reset_index(drop=True)\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"전처리된 샘플 수: {len(preprocessed_descriptions)}\")\n",
    "print(\"\\n처음 5개 전처리된 샘플:\")\n",
    "print(preprocessed_descriptions.head())\n",
    "\n",
    "# 전처리된 결과를 새 CSV 파일로 저장\n",
    "preprocessed_descriptions.to_csv('preprocessed_agnews_descriptions.csv', index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 샘플링된 데이터 수: 50000\n",
      "\n",
      "처음 5개 샘플:\n",
      "0    london british broadcasting corporation world ...\n",
      "1    embattled insurance broker bank agree waive cl...\n",
      "2    ap derek jeter turned season started terrible ...\n",
      "3    genesis capsule come back earth sample sun hel...\n",
      "4    new york reuters u stock set open near unchang...\n",
      "Name: article, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 50000개 랜덤 샘플링\n",
    "sampled_descriptions = preprocessed_descriptions.sample(n=50000, random_state=42)\n",
    "\n",
    "# 인덱스 재설정\n",
    "sampled_descriptions = sampled_descriptions.reset_index(drop=True)\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"랜덤 샘플링된 데이터 수: {len(sampled_descriptions)}\")\n",
    "print(\"\\n처음 5개 샘플:\")\n",
    "print(sampled_descriptions.head())\n",
    "\n",
    "# 샘플링된 결과를 새 CSV 파일로 저장\n",
    "sampled_descriptions.to_csv('sampled_agnews_descriptions.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리된 Abstract 수: 384\n",
      "\n",
      "처음 5개 전처리된 Abstract:\n",
      "0    last year use social networking site increased...\n",
      "1    paper proposes aspect based approach sentiment...\n",
      "2    twitter identified one largest social networki...\n",
      "3    although term big data often used refer large ...\n",
      "4    analysis content stream gathered social networ...\n",
      "Name: Abstract, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv('academy/article_dataframe_business_intelligence.csv', encoding='utf-8')\n",
    "\n",
    "# 'Abstract' 열만 선택\n",
    "abstracts = df['Abstract']\n",
    "\n",
    "# 전처리 적용\n",
    "preprocessed_abstracts = abstracts.apply(preprocess_text)\n",
    "\n",
    "# 빈 샘플 제거\n",
    "preprocessed_abstracts = preprocessed_abstracts[preprocessed_abstracts != '']\n",
    "\n",
    "# 인덱스 재설정\n",
    "preprocessed_abstracts = preprocessed_abstracts.reset_index(drop=True)\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"전처리된 Abstract 수: {len(preprocessed_abstracts)}\")\n",
    "print(\"\\n처음 5개 전처리된 Abstract:\")\n",
    "print(preprocessed_abstracts.head())\n",
    "\n",
    "# 전처리된 결과를 새 CSV 파일로 저장\n",
    "preprocessed_abstracts.to_csv('preprocessed_business_intelligence_abstracts.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 및 필터링된 샘플 수: 4948\n",
      "\n",
      "처음 5개 전처리된 샘플:\n",
      "0    online petition costeffective way citizen coll...\n",
      "1    language model trained largescale corpus gener...\n",
      "2    paper propose srlnlp new approach data augment...\n",
      "3    generative model increasingly used various app...\n",
      "4    embedding layer transforming input word real v...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# CSV 파일 읽기\n",
    "df = pd.read_csv('data/academy/ACL.csv', encoding='utf-8', header=0, names=['text'])\n",
    "# Extract the 'text' column\n",
    "text_column = df['text']\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 입력값이 문자열이 아닐 경우 빈 문자열로 처리\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    # 소문자 변환\n",
    "    text = text.lower()\n",
    "    # 숫자 및 특수 문자 제거\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # 숫자 및 특수 문자 제거\n",
    "    # 토큰화\n",
    "    tokens = word_tokenize(text)\n",
    "    # 불용어 제거 및 표제어 추출\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def filter_short_texts(texts, min_length=10):\n",
    "    \"\"\"\n",
    "    최소 단어 수(min_length) 이상의 텍스트만 보존합니다.\n",
    "\n",
    "    :param texts: 텍스트 시리즈 또는 리스트\n",
    "    :param min_length: 최소 단어 수\n",
    "    :return: 필터링된 텍스트 시리즈 또는 리스트\n",
    "    \"\"\"\n",
    "    filtered_texts = texts[texts.apply(lambda x: len(x.split()) >= min_length)]\n",
    "    return filtered_texts\n",
    "\n",
    "# 전처리 적용\n",
    "texts = text_column.apply(preprocess_text)\n",
    "\n",
    "# # 빈 문자열 제거\n",
    "# preprocessed_texts = preprocessed_texts[preprocessed_texts != '']\n",
    "\n",
    "# # 짧은 텍스트 필터링 (예: 단어 수 10개 이상인 텍스트만 보존)\n",
    "# filtered_reviews = filter_short_texts(preprocessed_texts, min_length=10)\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"전처리 및 필터링된 샘플 수: {len(texts)}\")\n",
    "print(\"\\n처음 5개 전처리된 샘플:\")\n",
    "print(texts.head())\n",
    "\n",
    "# 전처리된 결과를 새 CSV 파일로 저장 (빈 행 제거)\n",
    "texts.to_csv('ACL_preprocessed.csv', index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of preprocessed tweets: 228207\n",
      "\n",
      "First 5 preprocessed tweets:\n",
      "0    folk said daikon paste could treat cytokine st...\n",
      "1    world wrong side history year hopefully bigges...\n",
      "2    coronavirus sputnikv astrazeneca pfizerbiontec...\n",
      "3    fact immutable senator even youre ethically st...\n",
      "4    explain need vaccine borisjohnson matthancock ...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('media/tweets.csv', encoding='utf-8')\n",
    "\n",
    "# Extract the 'text' column\n",
    "text_column = df['text']\n",
    "\n",
    "# Apply preprocessing\n",
    "preprocessed_texts = text_column.apply(preprocess_text)\n",
    "\n",
    "# Remove empty samples\n",
    "preprocessed_texts = preprocessed_texts[preprocessed_texts != '']\n",
    "\n",
    "# Reset index\n",
    "preprocessed_texts = preprocessed_texts.reset_index(drop=True)\n",
    "\n",
    "# Save the preprocessed texts to a new CSV file\n",
    "preprocessed_texts.to_csv('preprocessed_tweets_text.csv', index=False, header=True)\n",
    "\n",
    "print(f\"Number of preprocessed tweets: {len(preprocessed_texts)}\")\n",
    "print(\"\\nFirst 5 preprocessed tweets:\")\n",
    "print(preprocessed_texts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플링된 트윗 수: 110000\n",
      "\n",
      "샘플링된 처음 5개 트윗:\n",
      "0    didnt expect moderna vaccine knock as im fucki...\n",
      "1    honble pm narendramodi watching making make in...\n",
      "2    yearold man died day receiving sinopharm vacci...\n",
      "3                             sputnikv dr reddys stock\n",
      "4      age slot covaxin agadi hospital researchrs bbmp\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 11만개 샘플링\n",
    "sampled_texts = preprocessed_texts.sample(n=110000, random_state=42)\n",
    "\n",
    "# 인덱스 재설정\n",
    "sampled_texts = sampled_texts.reset_index(drop=True)\n",
    "\n",
    "# 샘플링된 텍스트를 새 CSV 파일로 저장\n",
    "sampled_texts.to_csv('sampled_preprocessed_tweets.csv', index=False, header=True)\n",
    "\n",
    "print(f\"샘플링된 트윗 수: {len(sampled_texts)}\")\n",
    "print(\"\\n샘플링된 처음 5개 트윗:\")\n",
    "print(sampled_texts.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 데이터 수: 36832\n",
      "필터링 후 데이터 수: 36075\n",
      "제거된 데이터 수: 757\n",
      "\n",
      "필터링된 처음 5개 트윗:\n",
      "                                                text\n",
      "0  buddhism much lot compatible christianity espe...\n",
      "1  seriously say thing first get complex explain ...\n",
      "2  learned want teach different focus goal wrappi...\n",
      "3  benefit may want read living buddha living chr...\n",
      "4  sit together watch simpson episode lisa become...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 읽기\n",
    "sampled_texts = pd.read_csv('media/reddit_comments.csv', header=0, names=['text'])\n",
    "\n",
    "# 글자 수가 5개 초과인 행만 유지\n",
    "\n",
    "filtered_texts = sampled_texts[sampled_texts['text'].str.len() > 5]\n",
    "\n",
    "# 인덱스 재설정\n",
    "filtered_texts = filtered_texts.reset_index(drop=True)\n",
    "\n",
    "# 필터링된 텍스트를 새 CSV 파일로 저장\n",
    "filtered_texts.to_csv('filtered_preprocessed.csv', index=False, header=True)\n",
    "\n",
    "print(f\"원본 데이터 수: {len(sampled_texts)}\")\n",
    "print(f\"필터링 후 데이터 수: {len(filtered_texts)}\")\n",
    "print(f\"제거된 데이터 수: {len(sampled_texts) - len(filtered_texts)}\")\n",
    "print(\"\\n필터링된 처음 5개 트윗:\")\n",
    "print(filtered_texts.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
