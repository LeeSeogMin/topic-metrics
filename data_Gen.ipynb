{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "# 위키피디아 API를 통해 문서 내용 수집 함수\n",
    "def get_wikipedia_content(topic):\n",
    "    url = f\"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": topic,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "        \"redirects\": 1\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    pages = data['query']['pages']\n",
    "    for page_id, page_data in pages.items():\n",
    "        if 'extract' in page_data:\n",
    "            return page_data['extract']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "# 각 토픽별로 800개의 데이터 생성 함수 (위키피디아 API 활용)\n",
    "def generate_topic_data_wikipedia(topics, n_per_topic=800):\n",
    "    all_data = []\n",
    "    failed_topics = []  # 실패한 토픽을 저장할 리스트\n",
    "    topic_data_count = {}  # 각 토픽별 데이터 수를 저장할 딕셔너리\n",
    "    for topic in topics:\n",
    "        content = get_wikipedia_content(topic)\n",
    "        if content:\n",
    "            # 텍스트를 n개의 샘플로 나누어 저장\n",
    "            sentences = content.split(\". \")\n",
    "            samples = [sentence for sentence in sentences if len(sentence.split()) >= 10]\n",
    "            samples = samples[:n_per_topic] if len(samples) >= n_per_topic else samples\n",
    "            topic_data_count[topic] = len(samples)  # 각 토픽별 데이터 수 저장\n",
    "            for sentence in samples:\n",
    "                all_data.append((sentence, topic))  # 튜플로 저장 (문장, 토픽)\n",
    "        else:\n",
    "            print(f\"Failed to retrieve content for {topic}\")\n",
    "            failed_topics.append(topic)  # 실패한 토픽 추가\n",
    "    return all_data, failed_topics, topic_data_count\n",
    "\n",
    "# 노이즈 데이터 생성\n",
    "def generate_noise_data(n):\n",
    "    noise_words = [\"random\", \"unrelated\", \"irrelevant\", \"topic\", \"data\", \"sentence\", \"generated\"]\n",
    "    data = []\n",
    "    for i in range(n):\n",
    "        sentence = \" \".join(random.choices(noise_words, k=8))\n",
    "        data.append((sentence, \"Noise\"))  # 노이즈 데이터의 레이블은 \"Noise\"\n",
    "    return data\n",
    "\n",
    "# 토픽 목록\n",
    "distinct_topics = [\n",
    "    \"Quantum Mechanics\", \"CRISPR\", \"General Relativity\", \"Robotics\",\n",
    "    \"Quantum Computing\", \"Climate Change\", \"Renewable Energy\", \"Artificial Intelligence\"\n",
    "]\n",
    "\n",
    "similar_topics = [\n",
    "    \"Machine Learning\", \"Deep Learning\", \"Electric Vehicles\",\n",
    "    \"Natural Language Processing\", \"Computer Vision\", \"Bioinformatics\", \"Nanotechnology\"\n",
    "]\n",
    "\n",
    "# 데이터 생성\n",
    "topic_data, failed_topics, topic_data_count = generate_topic_data_wikipedia(distinct_topics + similar_topics, 800)\n",
    "noise_data = generate_noise_data(500)\n",
    "\n",
    "# 모든 토픽에 대해 데이터 수집이 성공한 경우에만 저장\n",
    "if not failed_topics:\n",
    "    all_data = topic_data + noise_data\n",
    "    with open('/data/data.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['text', 'label'])\n",
    "        for text, label in all_data:\n",
    "            writer.writerow([text, label])\n",
    "    print(f\"Total generated data samples: {len(all_data)}\")\n",
    "    print(\"Sample data:\")\n",
    "    for i in range(5):\n",
    "        print(all_data[i])\n",
    "    \n",
    "    # 각 토픽별 데이터 수 출력\n",
    "    print(\"\\nData count per topic:\")\n",
    "    for topic, count in topic_data_count.items():\n",
    "        print(f\"{topic}: {count} samples\")\n",
    "    \n",
    "    # 노이즈 데이터 수 출력\n",
    "    print(f\"\\nNoise data: {len(noise_data)} samples\")\n",
    "else:\n",
    "    print(\"Data retrieval failed for some topics. Data not saved.\")\n",
    "    print(\"Failed topics:\")\n",
    "    for topic in failed_topics:\n",
    "        print(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리된 샘플 수: 3750\n",
      "\n",
      "처음 5개 전처리된 샘플:\n",
      "                                                text              label\n",
      "0  quantum mechanic fundamental theory describes ...  Quantum Mechanics\n",
      "1  classical physic describe many aspect nature o...  Quantum Mechanics\n",
      "2  theory classical physic derived quantum mechan...  Quantum Mechanics\n",
      "3  measurement quantum system show characteristic...  Quantum Mechanics\n",
      "4  early attempt understand microscopic phenomeno...  Quantum Mechanics\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# CSV 파일 읽기\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv('data/data.csv', delimiter=',')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 입력값이 문자열이 아닐 경우 빈 문자열로 처리\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    # 소문자 변환\n",
    "    text = text.lower()\n",
    "    # 숫자 및 특수 문자 제거\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # 숫자 제거\n",
    "    # 토큰화\n",
    "    tokens = word_tokenize(text)\n",
    "    # 불용어 제거 및 lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# 전처리 적용 (text 칼럼에만)\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"전처리된 샘플 수: {len(df)}\")\n",
    "print(\"\\n처음 5개 전처리된 샘플:\")\n",
    "print(df.head())\n",
    "\n",
    "# 전처리된 결과를 새 CSV 파일로 저장\n",
    "df.to_csv('data/pre_data.csv', index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts:\n",
      "Noise: 500 samples\n",
      "Artificial Intelligence: 364 samples\n",
      "Climate Change: 350 samples\n",
      "Robotics: 311 samples\n",
      "General Relativity: 283 samples\n",
      "Deep Learning: 257 samples\n",
      "Machine Learning: 235 samples\n",
      "Renewable Energy: 227 samples\n",
      "CRISPR: 181 samples\n",
      "Quantum Mechanics: 178 samples\n",
      "Quantum Computing: 174 samples\n",
      "Electric Vehicles: 166 samples\n",
      "Computer Vision: 150 samples\n",
      "Bioinformatics: 148 samples\n",
      "Nanotechnology: 114 samples\n",
      "Natural Language Processing: 112 samples\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the preprocessed data\n",
    "df = pd.read_csv('data/pre_data.csv')\n",
    "\n",
    "# Get the unique labels and their counts\n",
    "label_counts = df['label'].value_counts()\n",
    "\n",
    "# Print the label names and their counts\n",
    "print(\"Label counts:\")\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"{label}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
