{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gensim import models, corpora\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import time\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from math import log\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from collections import Counter, defaultdict\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from scipy.sparse import csr_matrix\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from bertopic import BERTopic\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import os\n",
    "import re\n",
    "import matplotlib\n",
    "from tabulate import tabulate\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# NLTK 데이터 다운로드\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# 로깅 설정\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# stop_words 정의\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 전역 변수로 BERT 모델과 토크나이저 선언\n",
    "global tokenizer, bert_model\n",
    "\n",
    "# BERT 모델 로딩 함수\n",
    "def load_bert_model():\n",
    "    global tokenizer, bert_model\n",
    "    if 'tokenizer' not in globals() or 'bert_model' not in globals():\n",
    "        logging.info(\"Loading BERT model and tokenizer...\")\n",
    "        try:\n",
    "            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "            bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "            \n",
    "            # Check GPU availability and set device\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            bert_model = bert_model.to(device)\n",
    "            \n",
    "            logging.info(f\"BERT model and tokenizer loaded. Using device: {device}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading BERT model: {e}\")\n",
    "            raise\n",
    "\n",
    "def load_data(file_path, sample_size=20000, min_words=10):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, header=None, names=['text'])\n",
    "        texts = df['text'].astype(str)\n",
    "        \n",
    "        # 최소 단어 수 조건을 만족하는 텍스트만 필터링\n",
    "        valid_texts = texts[texts.apply(lambda x: len(x.split()) >= min_words)]\n",
    "        total_samples = len(valid_texts)\n",
    "        \n",
    "        if total_samples > sample_size:\n",
    "            sampled_texts = valid_texts.sample(n=sample_size, random_state=42)\n",
    "        else:\n",
    "            sampled_texts = valid_texts\n",
    "            \n",
    "        print(f\"Loaded {len(sampled_texts)} texts from {file_path} (Total valid samples: {total_samples})\")\n",
    "        return sampled_texts.tolist()\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"File not found: {file_path}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading file {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def load_all_datasets():\n",
    "    datasets = {\n",
    "        'academy': {\n",
    "            'covid': load_data('data/academy/covid.csv')\n",
    "        },\n",
    "        'media': {\n",
    "            'clothing_review': load_data('data/media/clothing_review.csv')\n",
    "        },\n",
    "        'news': {\n",
    "            'agnews': load_data('data/news/agnews.csv')\n",
    "        }\n",
    "    }\n",
    "    return datasets\n",
    "\n",
    "def print_dataset_statistics(datasets):\n",
    "    print(\"\\nSampled Dataset Statistics:\")\n",
    "    print(\"===========================\")\n",
    "    for domain, domain_data in datasets.items():\n",
    "        print(f\"\\nDomain: {domain}\")\n",
    "        for dataset_name, data in domain_data.items():\n",
    "            print(f\"  Dataset: {dataset_name}\")\n",
    "            \n",
    "            # 데이터가 이미 문자열의 리스트라고 가정\n",
    "            print(f\"    Sampled texts: {len(data)}\")\n",
    "            \n",
    "            # 텍스트 길이 통계 (샘플링된 데이터)\n",
    "            text_lengths = [len(text.split()) for text in data]\n",
    "            print(f\"    Average text length (words): {np.mean(text_lengths):.2f}\")\n",
    "            print(f\"    Median text length (words): {np.median(text_lengths):.2f}\")\n",
    "            print(f\"    Min text length (words): {np.min(text_lengths)}\")\n",
    "            print(f\"    Max text length (words): {np.max(text_lengths)}\")\n",
    "            \n",
    "            # 고유 단어 수 (샘플링된 데이터)\n",
    "            unique_words = set(word for text in data for word in text.split())\n",
    "            print(f\"    Unique words: {len(unique_words)}\")\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=50, latent_dim=None):\n",
    "        if latent_dim is None:\n",
    "            raise ValueError(\"latent_dim must be specified\")\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, latent_dim)  \n",
    "        self.fc22 = nn.Linear(hidden_dim, latent_dim)  \n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "def extract_vae_topics(vae_model, vectorizer, num_topics, top_n=10):\n",
    "    with torch.no_grad():\n",
    "        latent_vectors = torch.eye(num_topics).to(vae_model.fc3.weight.device)\n",
    "        decoder_output = vae_model.decode(latent_vectors)\n",
    "        decoder_output = decoder_output.cpu().numpy()\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topics = []\n",
    "    for topic_distribution in decoder_output:\n",
    "        top_indices = topic_distribution.argsort()[-top_n:][::-1]\n",
    "        topic_words = [feature_names[i] for i in top_indices]\n",
    "        topics.append(topic_words)\n",
    "    return topics\n",
    "\n",
    "def perform_bertopic_modeling(data, max_topics=20):\n",
    "    try:\n",
    "        # Data preprocessing: Remove empty or non-string documents\n",
    "        data = [doc for doc in data if isinstance(doc, str) and len(doc.strip()) > 0]\n",
    "        if len(data) == 0:\n",
    "            logging.error(\"No valid documents to process.\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # Adjust min_df to 1\n",
    "        vectorizer_model = CountVectorizer(stop_words=\"english\", min_df=1, max_df=0.95)\n",
    "        \n",
    "        # Initialize BERTopic\n",
    "        bertopic_model = BERTopic(\n",
    "            language=\"english\",\n",
    "            calculate_probabilities=True,\n",
    "            nr_topics=\"auto\",\n",
    "            vectorizer_model=vectorizer_model,\n",
    "            top_n_words=10,\n",
    "            min_topic_size=30,\n",
    "            n_gram_range=(1, 2),\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Fit the model\n",
    "        topics, _ = bertopic_model.fit_transform(data)\n",
    "        \n",
    "        # Reduce topics if necessary\n",
    "        num_topics = len(set(topics)) - (1 if -1 in topics else 0)\n",
    "        if num_topics > max_topics:\n",
    "            logging.info(f\"Reducing number of topics from {num_topics} to {max_topics}\")\n",
    "            bertopic_model = bertopic_model.reduce_topics(data, nr_topics=max_topics)\n",
    "            topics = bertopic_model.transform(data)\n",
    "            num_topics = len(set(topics)) - (1 if -1 in topics else 0)\n",
    "        \n",
    "        # Extract topic words\n",
    "        topic_words = []\n",
    "        for i in range(num_topics):\n",
    "            topic = bertopic_model.get_topic(i)\n",
    "            if topic:\n",
    "                words = [word for word, _ in topic[:10]]\n",
    "                topic_words.append(words)\n",
    "        \n",
    "        logging.info(f\"BERTopic modeling completed. Number of topics: {num_topics}\")\n",
    "        return bertopic_model, topic_words, num_topics\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in BERTopic modeling: {e}\")\n",
    "        return None, [], 0  # 빈 리스트와 0을 반환\n",
    "\n",
    "\n",
    "\n",
    "def perform_lowbertopic_modeling(data, low_num_topics):\n",
    "    # 토픽 수를 num_topics로 설정 (기본값 5)\n",
    "    low_num_topics = 5\n",
    "    \n",
    "    # 데이터 전처리\n",
    "    if isinstance(data, dict):\n",
    "        data = list(data.values())[0]\n",
    "    elif isinstance(data, pd.DataFrame):\n",
    "        data = data['text'].tolist() if 'text' in data.columns else data.values.flatten().tolist()\n",
    "    elif isinstance(data, pd.Series):\n",
    "        data = data.tolist()\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        data = data.flatten().tolist()\n",
    "    elif isinstance(data, list):\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported data format for BERTopic modeling: {type(data)}\")\n",
    "\n",
    "    # 데이터가 문자열 리스트인지 확인\n",
    "    if not all(isinstance(item, str) for item in data):\n",
    "        raise ValueError(\"All items in the data must be strings\")\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # BERTopic 모델 생성\n",
    "        lowbertopic_model = BERTopic(language=\"english\", calculate_probabilities=True, nr_topics=low_num_topics)\n",
    "        \n",
    "        # 모델 학습\n",
    "        topics, _ = lowbertopic_model.fit_transform(data)\n",
    "        \n",
    "        # 토픽 단어 추출\n",
    "        topic_words = []\n",
    "        for i in range(low_num_topics):\n",
    "            topic = lowbertopic_model.get_topic(i)\n",
    "            if topic:\n",
    "                words = [word for word, _ in topic[:10]]  # 상위 10개 단어 추출\n",
    "                topic_words.append(words)\n",
    "        \n",
    "        return lowbertopic_model, topic_words, low_num_topics\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in LowBERTopic modeling: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def perform_vae_topic_modeling(data, num_topics, num_epochs=5, hidden_dim=50):\n",
    "    try:\n",
    "        # 데이터 전처리\n",
    "        data = [str(doc) for doc in data if isinstance(doc, str) and len(doc) > 0]\n",
    "        vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "        doc_term_matrix = vectorizer.fit_transform(data)\n",
    "\n",
    "        # MinMaxScaler를 사용하여 0-1 사이로 정규화\n",
    "        scaler = MinMaxScaler()\n",
    "        normalized_matrix = scaler.fit_transform(doc_term_matrix.toarray())\n",
    "\n",
    "        # VAE 모델 초기화 및 학습\n",
    "        input_dim = doc_term_matrix.shape[1]\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        vae_model = VAE(input_dim=input_dim, hidden_dim=hidden_dim, latent_dim=num_topics).to(device)\n",
    "        optimizer = torch.optim.Adam(vae_model.parameters(), lr=1e-3)\n",
    "\n",
    "        batch_size = 64\n",
    "        data_loader = DataLoader(normalized_matrix.astype(np.float32), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        vae_model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = 0\n",
    "            for batch in data_loader:\n",
    "                batch = batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                recon_batch, mu, logvar = vae_model(batch)\n",
    "                loss = vae_loss(recon_batch, batch, mu, logvar)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            logging.info(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss / len(data_loader.dataset):.4f}\")\n",
    "\n",
    "        topics = extract_vae_topics(vae_model, vectorizer, num_topics)\n",
    "        return vae_model, topics\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in VAE modeling: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def calculate_coherence(topics, tokenizer, bert_model, top_n=10, batch_size=16):\n",
    "    coherence_scores = []\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    bert_model.to(device)\n",
    "    bert_model.eval()\n",
    "\n",
    "    for topic_words in topics:\n",
    "        # 토픽 단어 수 제한\n",
    "        topic_words = topic_words[:top_n]\n",
    "        # 유효한 단어만 선택\n",
    "        topic_words = [word for word in topic_words if word and isinstance(word, str)]\n",
    "        num_words = len(topic_words)\n",
    "\n",
    "        if num_words < 2:\n",
    "            coherence_scores.append(0)\n",
    "            continue\n",
    "\n",
    "        # 단어 임베딩을 배치로 계산     \n",
    "        embeddings = []\n",
    "        for i in range(0, num_words, batch_size):\n",
    "            batch_words = topic_words[i:i + batch_size]\n",
    "            inputs = tokenizer(batch_words, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = bert_model(**inputs)\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :]  # [CLS] 토큰의 임베딩 사용\n",
    "            embeddings.append(batch_embeddings)\n",
    "            # 메모리 관리\n",
    "            del outputs\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        embeddings = torch.cat(embeddings, dim=0)\n",
    "\n",
    "        # 코사인 유사도 계산\n",
    "        pairwise_similarities = []\n",
    "        for i in range(num_words):\n",
    "            for j in range(i + 1, num_words):\n",
    "                cosine_sim = torch.nn.functional.cosine_similarity(embeddings[i], embeddings[j], dim=0)\n",
    "                pairwise_similarities.append(cosine_sim.item())\n",
    "\n",
    "        coherence = np.mean(pairwise_similarities)\n",
    "        coherence_scores.append(coherence)\n",
    "\n",
    "    final_coherence = np.mean(coherence_scores) if coherence_scores else 0\n",
    "    return final_coherence\n",
    "\n",
    "\n",
    "def process_metrics(domain, model_type, topics, data, metrics_list, tokenizer, bert_model):\n",
    "    tokenized_data = [simple_preprocess(doc) for doc in data]\n",
    "    dictionary = Dictionary(tokenized_data)\n",
    "    corpus = [dictionary.doc2bow(text) for text in tokenized_data]\n",
    "\n",
    "    coherence = calculate_coherence(topics, tokenizer, bert_model)\n",
    "    npmi = calculate_npmi(topics, corpus, dictionary)\n",
    "    umass = calculate_umass(topics, corpus, dictionary)\n",
    "\n",
    "    metrics_list.append({\n",
    "        'Domain': domain,\n",
    "        'Model': model_type,\n",
    "        'Coherence': coherence,\n",
    "        'NPMI': npmi,\n",
    "        'U_Mass': umass\n",
    "    })\n",
    "\n",
    "    logging.info(f\"Coherence: {coherence:.4f}, NPMI: {npmi:.4f}, U_Mass: {umass:.4f}\")\n",
    "    \n",
    "    return [metrics_list[-1]]  # Return the last added metric as a list\n",
    "\n",
    "def calculate_npmi(topics, corpus, dictionary, top_n=10):\n",
    "    # Create a set of all words used in topics\n",
    "    topic_words_set = set()\n",
    "    for topic in topics:\n",
    "        topic_words_set.update(topic[:top_n])\n",
    "\n",
    "    # Map words to IDs\n",
    "    word2id = {word: dictionary.token2id[word] for word in topic_words_set if word in dictionary.token2id}\n",
    "    id2word = {id: word for word, id in word2id.items()}\n",
    "\n",
    "    # Calculate word and word pair document frequencies\n",
    "    total_docs = len(corpus)\n",
    "    word_doc_freq = defaultdict(int)\n",
    "    pair_doc_freq = defaultdict(int)\n",
    "\n",
    "    for doc in corpus:\n",
    "        doc_word_ids = set([id for id, _ in doc])     \n",
    "        topic_word_ids_in_doc = doc_word_ids.intersection(set(word2id.values()))\n",
    "\n",
    "        for word_id in topic_word_ids_in_doc:\n",
    "            word_doc_freq[word_id] += 1\n",
    "\n",
    "        for word_id1, word_id2 in combinations(topic_word_ids_in_doc, 2):\n",
    "            pair = tuple(sorted((word_id1, word_id2)))\n",
    "            pair_doc_freq[pair] += 1\n",
    "\n",
    "    # Calculate NPMI\n",
    "    npmi_scores = []\n",
    "    for topic in topics:\n",
    "        topic_word_ids = [word2id[word] for word in topic[:top_n] if word in word2id]\n",
    "        if len(topic_word_ids) < 2:\n",
    "            continue\n",
    "        pair_npmi_scores = []\n",
    "        for word_id1, word_id2 in combinations(topic_word_ids, 2):\n",
    "            pair = tuple(sorted((word_id1, word_id2)))\n",
    "            co_doc_count = pair_doc_freq.get(pair, 0)\n",
    "            if co_doc_count == 0:\n",
    "                continue\n",
    "            p_w1_w2 = co_doc_count / total_docs\n",
    "            p_w1 = word_doc_freq[word_id1] / total_docs\n",
    "            p_w2 = word_doc_freq[word_id2] / total_docs\n",
    "\n",
    "            pmi = np.log(p_w1_w2 / (p_w1 * p_w2) + 1e-12)\n",
    "            npmi = pmi / (-np.log(p_w1_w2 + 1e-12))\n",
    "            pair_npmi_scores.append(npmi)\n",
    "        if pair_npmi_scores:\n",
    "            npmi_scores.append(np.mean(pair_npmi_scores))\n",
    "\n",
    "    return np.mean(npmi_scores) if npmi_scores else float('nan')\n",
    "\n",
    "def calculate_umass(topics, corpus, dictionary, top_n=10):\n",
    "    # Create a set of all words used in topics\n",
    "    topic_words_set = set()\n",
    "    for topic in topics:\n",
    "        topic_words_set.update(topic[:top_n])\n",
    "\n",
    "    # Map words to IDs\n",
    "    word2id = {word: dictionary.token2id[word] for word in topic_words_set if word in dictionary.token2id}\n",
    "\n",
    "    # Calculate word and word pair frequencies\n",
    "    word_counts = defaultdict(int)\n",
    "    pair_counts = defaultdict(int)\n",
    "\n",
    "    for doc in corpus:\n",
    "        doc_word_ids = set([id for id, _ in doc])\n",
    "        topic_word_ids_in_doc = doc_word_ids.intersection(set(word2id.values()))\n",
    "\n",
    "        for word_id in topic_word_ids_in_doc:\n",
    "            word_counts[word_id] += 1\n",
    "\n",
    "        for word_id1, word_id2 in combinations(topic_word_ids_in_doc, 2):\n",
    "            pair = tuple(sorted((word_id1, word_id2)))\n",
    "            pair_counts[pair] += 1\n",
    "\n",
    "    # Calculate U_Mass\n",
    "    umass_scores = []\n",
    "    for topic in topics:\n",
    "        topic_word_ids = [word2id[word] for word in topic[:top_n] if word in word2id]\n",
    "        if len(topic_word_ids) < 2:\n",
    "            continue\n",
    "        pair_umass_scores = []\n",
    "        for i, word_id1 in enumerate(topic_word_ids):\n",
    "            for word_id2 in topic_word_ids[:i]:\n",
    "                pair = tuple(sorted((word_id1, word_id2)))\n",
    "                co_occurrence = pair_counts.get(pair, 0) + 1  # Add 1 for smoothing\n",
    "                word2_count = word_counts[word_id2] + 1  # Add 1 for smoothing\n",
    "                umass = np.log(co_occurrence / word2_count)\n",
    "                pair_umass_scores.append(umass)\n",
    "        if pair_umass_scores:\n",
    "            umass_scores.append(-np.mean(pair_umass_scores))\n",
    "\n",
    "    return -np.mean(umass_scores) if umass_scores else float('nan')\n",
    "\n",
    "# 일치도 분석 함수\n",
    "def analyze_agreement(metrics_df):\n",
    "    agreement_results = {}\n",
    "    \n",
    "    # Add debugging information\n",
    "    print(\"DataFrame columns:\", metrics_df.columns)\n",
    "    print(\"DataFrame head:\")\n",
    "    print(metrics_df.head())\n",
    "    \n",
    "    for domain in metrics_df['Domain'].unique():\n",
    "        domain_df = metrics_df[metrics_df['Domain'] == domain]\n",
    "        for model in ['BERTopic', 'VAE', 'LowBERTopic']:\n",
    "            model_df = domain_df[domain_df['Model'] == model]\n",
    "            if len(model_df) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Check if the required columns exist\n",
    "            required_columns = ['Coherence', 'NPMI', 'U_Mass']\n",
    "            if not all(col in model_df.columns for col in required_columns):\n",
    "                print(f\"Warning: Missing required columns for {domain} - {model}\")\n",
    "                continue\n",
    "            \n",
    "            coherence = model_df['Coherence'].values[0]\n",
    "            npmi = model_df['NPMI'].values[0]\n",
    "            umass = model_df['U_Mass'].values[0]\n",
    "            \n",
    "            # Calculate agreement between metrics\n",
    "            metrics = [coherence, npmi, -umass]  # Invert U_Mass as lower is better\n",
    "            agreement = np.std(metrics) / np.mean(metrics)  # Coefficient of Variation\n",
    "            \n",
    "            if domain not in agreement_results:\n",
    "                agreement_results[domain] = {}\n",
    "            agreement_results[domain][model] = {\n",
    "                'Coherence': coherence,\n",
    "                'NPMI': npmi,\n",
    "                'U_Mass': umass,\n",
    "                'Agreement': agreement\n",
    "            }\n",
    "    \n",
    "    return agreement_results\n",
    "\n",
    "# 안정성 분석 함수\n",
    "def analyze_stability(datasets, model_types, n_runs=10, sample_ratio=0.8):\n",
    "    stability_results = []\n",
    "    \n",
    "    for domain, domain_datasets in datasets.items():\n",
    "        # 각 도메인에서 첫 번째 데이터셋만 사용\n",
    "        data = next(iter(domain_datasets.values()))\n",
    "        \n",
    "        logging.info(f\"Analyzing stability for domain: {domain}\")\n",
    "        logging.info(f\"Original data type: {type(data)}\")\n",
    "        \n",
    "        # 데이터 형식 확인 및 변환\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data = data['text'].tolist() if 'text' in data.columns else data.values.flatten().tolist()\n",
    "        elif isinstance(data, pd.Series):\n",
    "            data = data.tolist()\n",
    "        elif isinstance(data, np.ndarray):\n",
    "            data = data.flatten().tolist()\n",
    "        elif isinstance(data, list):\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported data format for domain {domain}: {type(data)}\")\n",
    "        \n",
    "        logging.info(f\"Processed data type: {type(data)}\")\n",
    "        logging.info(f\"Sample of processed data: {data[:5]}\")  # 처음 5개 항목 출력\n",
    "        \n",
    "        # BERTopic으로 초기 토픽 수 결정\n",
    "        _, _, num_topics = perform_bertopic_modeling(data)\n",
    "        \n",
    "        for model_type in model_types:\n",
    "            metric_values = {\n",
    "                'Coherence': [],\n",
    "                'NPMI': [],\n",
    "                'U_Mass': []\n",
    "            }\n",
    "            \n",
    "            for _ in range(n_runs):\n",
    "                sampled_data = np.random.choice(data, size=int(len(data) * sample_ratio), replace=False)\n",
    "                sampled_data = sampled_data.tolist()  # numpy array를 리스트로 변환\n",
    "                \n",
    "                logging.info(f\"Sampled data type: {type(sampled_data)}\")\n",
    "                logging.info(f\"Sample of sampled data: {sampled_data[:5]}\")  # 처음 5개 항목 출력\n",
    "                \n",
    "                if model_type == 'BERTopic':\n",
    "                    model, topics, _ = perform_bertopic_modeling(sampled_data)\n",
    "                elif model_type == 'VAE':\n",
    "                    model, topics = perform_vae_topic_modeling(sampled_data, num_topics)\n",
    "                elif model_type == 'LowBERTopic':\n",
    "                    model, topics, _ = perform_lowbertopic_modeling(sampled_data, num_topics)  # num_topics 인자 제거\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "                \n",
    "                if model is None or topics is None:\n",
    "                    logging.warning(f\"Model or topics is None for {model_type} in domain {domain}\")\n",
    "                    continue\n",
    "                \n",
    "                tokenized_data = [simple_preprocess(doc) for doc in sampled_data]\n",
    "                dictionary = Dictionary(tokenized_data)\n",
    "                corpus = [dictionary.doc2bow(text) for text in tokenized_data]\n",
    "                \n",
    "                coherence = calculate_coherence(topics, tokenizer, bert_model)\n",
    "                npmi = calculate_npmi(topics, corpus, dictionary)\n",
    "                umass = calculate_umass(topics, corpus, dictionary)\n",
    "                \n",
    "                metric_values['Coherence'].append(coherence)\n",
    "                metric_values['NPMI'].append(npmi)\n",
    "                metric_values['U_Mass'].append(umass)\n",
    "            \n",
    "            for metric, values in metric_values.items():\n",
    "                cv = np.std(values) / np.mean(values) if np.mean(values) != 0 else float('nan')\n",
    "                stability_results.append({\n",
    "                    'Domain': domain,\n",
    "                    'Model': model_type,\n",
    "                    'Metric': metric,\n",
    "                    'CV': cv\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(stability_results)\n",
    "\n",
    "# 개선된 토픽 품질 시각화 함수\n",
    "\n",
    "def visualize_topic_quality(metrics_df):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Add debugging information\n",
    "    print(\"DataFrame columns:\", metrics_df.columns)\n",
    "    print(\"DataFrame head:\")\n",
    "    print(metrics_df.head())\n",
    "    \n",
    "    # Check if required columns exist\n",
    "    required_columns = ['Model', 'Coherence', 'NPMI']\n",
    "    if not all(col in metrics_df.columns for col in required_columns):\n",
    "        print(f\"Error: Missing required columns. Available columns: {metrics_df.columns}\")\n",
    "        return\n",
    "    \n",
    "    # Use different markers for each model\n",
    "    markers = {'BERTopic': 'o', 'VAE': 's', 'LowBERTopic': '^'}\n",
    "    \n",
    "    for model in ['BERTopic', 'VAE', 'LowBERTopic']:\n",
    "        model_data = metrics_df[metrics_df['Model'] == model]\n",
    "        coherence = model_data['Coherence']\n",
    "        npmi = model_data['NPMI']\n",
    "        plt.scatter(coherence, npmi, label=model, marker=markers[model])\n",
    "    \n",
    "    plt.xlabel('Coherence')\n",
    "    plt.ylabel('NPMI')\n",
    "    plt.title('Topic Quality: Coherence vs NPMI')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Add domain labels\n",
    "    for _, row in metrics_df.iterrows():\n",
    "        plt.annotate(row['Domain'], (row['Coherence'], row['NPMI']), xytext=(5, 5), \n",
    "                     textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('topic_quality.png')\n",
    "    plt.close()\n",
    "    \n",
    "    logging.info(\"Topic quality visualization completed: topic_quality.png\")\n",
    "\n",
    "\n",
    "def evaluate_coherence_stability(models, domains, datasets, n_runs=5):\n",
    "    stability_results = []\n",
    "    \n",
    "    for model in models:\n",
    "        for domain, data in zip(domains, datasets):\n",
    "            # Handle dictionary data appropriately\n",
    "            if isinstance(data, dict):\n",
    "                data = list(data.values())[0]\n",
    "            elif isinstance(data, pd.DataFrame):\n",
    "                data = data['text'].tolist()\n",
    "            elif not isinstance(data, list):\n",
    "                raise ValueError(f\"Unsupported data format for domain {domain}\")\n",
    "\n",
    "            coherence_scores = []\n",
    "            npmi_scores = []\n",
    "            umass_scores = []\n",
    "\n",
    "            for _ in range(n_runs):\n",
    "                # Sample 80% of the data\n",
    "                sampled_data = np.random.choice(data, size=int(len(data) * 0.8), replace=False)\n",
    "\n",
    "                if model == 'BERTopic':\n",
    "                    _, topics, _ = perform_bertopic_modeling(sampled_data)\n",
    "                elif model == 'VAE':\n",
    "                    _, topics = perform_vae_topic_modeling(sampled_data, num_topics=10)  # Adjust num_topics as needed\n",
    "                elif model == 'LowBERTopic':\n",
    "                    _, topics, _ = perform_lowbertopic_modeling(sampled_data, low_num_topics=5)  \n",
    "                else:                    \n",
    "                    raise ValueError(f\"Unsupported model type: {model}\")\n",
    "\n",
    "                # Prepare tokenized data\n",
    "                tokenized_data = [simple_preprocess(doc) for doc in sampled_data]\n",
    "                dictionary = Dictionary(tokenized_data)\n",
    "                corpus = [dictionary.doc2bow(text) for text in tokenized_data]\n",
    "\n",
    "                # Calculate coherence metrics\n",
    "                coherence = calculate_coherence(topics, tokenizer, bert_model)\n",
    "                npmi = calculate_npmi(topics, corpus, dictionary)\n",
    "                umass = calculate_umass(topics, corpus, dictionary)\n",
    "\n",
    "                coherence_scores.append(coherence)\n",
    "                npmi_scores.append(npmi)\n",
    "                umass_scores.append(umass)\n",
    "\n",
    "            # Calculate stability (coefficient of variation)\n",
    "            coherence_stability = np.std(coherence_scores) / np.mean(coherence_scores)\n",
    "            npmi_stability = np.std(npmi_scores) / np.mean(npmi_scores)\n",
    "            umass_stability = np.std(umass_scores) / np.mean(umass_scores)\n",
    "\n",
    "            stability_results.append({\n",
    "                'Model': model,\n",
    "                'Domain': domain,\n",
    "                'Coherence_Stability': coherence_stability,\n",
    "                'NPMI_Stability': npmi_stability,\n",
    "                'UMass_Stability': umass_stability,\n",
    "                'Mean_Coherence': np.mean(coherence_scores),\n",
    "                'Mean_NPMI': np.mean(npmi_scores),\n",
    "                'Mean_UMass': np.mean(umass_scores)\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(stability_results)\n",
    "\n",
    "def print_results(metrics_df, agreement_results, stability_df, stability_results):\n",
    "    logging.info(\"\\n=== Results Analysis ===\")\n",
    "    \n",
    "    # Print average performance of metrics by model\n",
    "    logging.info(\"\\nAverage performance of metrics by model:\")\n",
    "    models = metrics_df['Model'].unique()\n",
    "    for model in models:\n",
    "        model_metrics = metrics_df[metrics_df['Model'] == model]\n",
    "        mean_metrics = model_metrics[['Coherence', 'NPMI', 'U_Mass']].mean()\n",
    "        logging.info(f\"\\nModel: {model}\")\n",
    "        logging.info(f\"  - Average Coherence: {mean_metrics['Coherence']:.4f}\")\n",
    "        logging.info(f\"  - Average NPMI: {mean_metrics['NPMI']:.4f}\")\n",
    "        logging.info(f\"  - Average U_Mass: {mean_metrics['U_Mass']:.4f}\")\n",
    "    \n",
    "    # Print average performance of metrics by domain\n",
    "    logging.info(\"\\nAverage performance of metrics by domain:\")\n",
    "    domains = metrics_df['Domain'].unique()\n",
    "    for domain in domains:\n",
    "        domain_metrics = metrics_df[metrics_df['Domain'] == domain]\n",
    "        mean_metrics = domain_metrics[['Coherence', 'NPMI', 'U_Mass']].mean()\n",
    "        logging.info(f\"\\nDomain: {domain}\")\n",
    "        logging.info(f\"  - Average Coherence: {mean_metrics['Coherence']:.4f}\")\n",
    "        logging.info(f\"  - Average NPMI: {mean_metrics['NPMI']:.4f}\")\n",
    "        logging.info(f\"  - Average U_Mass: {mean_metrics['U_Mass']:.4f}\")\n",
    "    \n",
    "    # Print agreement analysis results\n",
    "    logging.info(\"\\nAgreement analysis results between metrics:\")\n",
    "    for domain, model_results in agreement_results.items():\n",
    "        logging.info(f\"\\nDomain: {domain}\")\n",
    "        for model, metrics in model_results.items():\n",
    "            logging.info(f\"  Model: {model}\")\n",
    "            for metric, value in metrics.items():\n",
    "                logging.info(f\"    {metric}: {value:.4f}\")\n",
    "    \n",
    "    # Print coherence metric stability results\n",
    "    logging.info(\"\\nIndividual results of coherence metric stability:\")\n",
    "    logging.info(stability_results)\n",
    "    logging.info(\"\\nOverall results of coherence metric stability:\")\n",
    "    stability_summary = stability_df.groupby(['Model', 'Metric'])['CV'].mean().reset_index()\n",
    "    for _, row in stability_summary.iterrows():\n",
    "        logging.info(f\"Model: {row['Model']}, Metric: {row['Metric']}, Average CV: {row['CV']:.4f}\")\n",
    "    \n",
    "    logging.info(\"\\nAnalysis complete. Please review and interpret the results.\")\n",
    "\n",
    "def process_datasets(datasets):\n",
    "    all_metrics = []\n",
    "    bertopic_results = {}\n",
    "    vae_results = {}\n",
    "    lowbertopic_results = {}\n",
    "    \n",
    "    for domain, domain_datasets in datasets.items():\n",
    "        for dataset_name, data in domain_datasets.items():\n",
    "            \n",
    "            # 데이터가 리스트가 아닌 경우 리스트로 변환\n",
    "            if not isinstance(data, list):\n",
    "                data = [data]\n",
    "            \n",
    "            # BERTopic modeling\n",
    "            bertopic_model, bertopic_topics, num_topics = perform_bertopic_modeling(data, max_topics=20)\n",
    "            if bertopic_topics is None:\n",
    "                bertopic_topics = []\n",
    "                num_topics = 0\n",
    "            \n",
    "            bertopic_results[domain] = {\n",
    "                'num_topics': num_topics,\n",
    "                'topics': bertopic_topics\n",
    "            }\n",
    "            \n",
    "            # Calculate BERTopic metrics\n",
    "            if bertopic_topics:\n",
    "                bertopic_metrics = process_metrics(domain, 'BERTopic', bertopic_topics, data, [], tokenizer, bert_model)\n",
    "                all_metrics.extend(bertopic_metrics)\n",
    "            \n",
    "            # VAE modeling (using the number of topics from BERTopic)\n",
    "            vae_model, vae_topics = perform_vae_topic_modeling(data, num_topics)\n",
    "            if vae_topics is None:\n",
    "                vae_topics = []\n",
    "            \n",
    "            vae_results[domain] = {\n",
    "                'num_topics': num_topics,\n",
    "                'topics': vae_topics\n",
    "            }\n",
    "            \n",
    "            # Calculate VAE metrics\n",
    "            if vae_topics:\n",
    "                vae_metrics = process_metrics(domain, 'VAE', vae_topics, data, [], tokenizer, bert_model)\n",
    "                all_metrics.extend(vae_metrics)\n",
    "            \n",
    "            # LowBERTopic modeling\n",
    "            lowbertopic_model, lowbertopic_topics, low_num_topics = perform_lowbertopic_modeling(data, low_num_topics=num_topics)\n",
    "            if lowbertopic_topics is None:\n",
    "                lowbertopic_topics = []\n",
    "                low_num_topics = 0\n",
    "            \n",
    "            lowbertopic_results[domain] = {\n",
    "                'num_topics': low_num_topics,\n",
    "                'topics': lowbertopic_topics\n",
    "            }\n",
    "            \n",
    "            # Calculate LowBERTopic metrics\n",
    "            if lowbertopic_topics:\n",
    "                lowbertopic_metrics = process_metrics(domain, 'LowBERTopic', lowbertopic_topics, data, [], tokenizer, bert_model)\n",
    "                all_metrics.extend(lowbertopic_metrics)\n",
    "    \n",
    "    return all_metrics, bertopic_results, vae_results, lowbertopic_results\n",
    "        \n",
    "        \n",
    "def main():\n",
    "    try:\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        load_bert_model()\n",
    "        logging.info(\"Starting dataset loading\")\n",
    "        datasets = load_all_datasets()\n",
    "        print_dataset_statistics(datasets)\n",
    "        \n",
    "        logging.info(\"Starting topic modeling and metric calculation\")\n",
    "        all_metrics, bertopic_results, vae_results, lowbertopic_results = process_datasets(datasets)\n",
    "        \n",
    "        # Create DataFrame to store topic information\n",
    "        topics_df = pd.DataFrame(columns=['Domain', 'Model', 'Topics'])\n",
    "        \n",
    "        # Process and save BERTopic results\n",
    "        logging.info(\"Outputting and saving BERTopic results\")\n",
    "        for domain, result in bertopic_results.items():\n",
    "            print(f\"\\nDomain: {domain}\")\n",
    "            print(f\"Number of BERTopic topics: {result['num_topics']}\")\n",
    "            print(\"BERTopic topics:\")\n",
    "            for i, topic in enumerate(result['topics']):\n",
    "                print(f\"  Topic {i+1}: {', '.join(topic[:10])}\")\n",
    "            \n",
    "            # Add to topics_df\n",
    "            topics_df = pd.concat([topics_df, pd.DataFrame({\n",
    "                'Domain': [domain],\n",
    "                'Model': ['BERTopic'],\n",
    "                'Topics': [result['topics']]\n",
    "            })], ignore_index=True)\n",
    "        \n",
    "        # Process and save VAE results\n",
    "        logging.info(\"\\nOutputting and saving VAE results\")\n",
    "        for domain, result in vae_results.items():\n",
    "            print(f\"\\nDomain: {domain}\")\n",
    "            print(f\"Number of VAE topics: {result['num_topics']}\")\n",
    "            print(\"VAE topics:\")\n",
    "            for i, topic in enumerate(result['topics']):\n",
    "                print(f\"  Topic {i+1}: {', '.join(topic[:10])}\")\n",
    "            \n",
    "            # Add to topics_df\n",
    "            topics_df = pd.concat([topics_df, pd.DataFrame({\n",
    "                'Domain': [domain],\n",
    "                'Model': ['VAE'],\n",
    "                'Topics': [result['topics']]\n",
    "            })], ignore_index=True)\n",
    "        \n",
    "        # Process and save LowBERTopic results\n",
    "        logging.info(\"\\nOutputting and saving LowBERTopic results\")\n",
    "        for domain, result in lowbertopic_results.items():\n",
    "            print(f\"\\nDomain: {domain}\")\n",
    "            print(f\"Number of LowBERTopic topics: {result['num_topics']}\")\n",
    "            print(\"LowBERTopic topics:\")\n",
    "            for i, topic in enumerate(result['topics']):\n",
    "                print(f\"  Topic {i+1}: {', '.join(topic[:10])}\")\n",
    "            \n",
    "            # Add to topics_df\n",
    "            topics_df = pd.concat([topics_df, pd.DataFrame({\n",
    "                'Domain': [domain],\n",
    "                'Model': ['LowBERTopic'],\n",
    "                'Topics': [result['topics']]\n",
    "            })], ignore_index=True)\n",
    "        \n",
    "        # Save topics_df to CSV file\n",
    "        topics_df.to_csv('topics_df.csv', index=False)\n",
    "        logging.info(\"Topic information has been saved to topics_df.csv file.\")\n",
    "        \n",
    "        logging.info(\"Starting metric analysis\")\n",
    "        metrics_df = pd.DataFrame(all_metrics)\n",
    "        \n",
    "        # U_Mass 값의 부호를 바꿉니다\n",
    "        metrics_df['U_Mass'] = -metrics_df['U_Mass']\n",
    "        logging.info(\"U_Mass values have been inverted for consistent interpretation with other metrics\")\n",
    "        \n",
    "        metrics_df.to_csv('topic_modeling_metrics.csv', index=False)\n",
    "        \n",
    "        logging.info(\"Starting agreement analysis\")\n",
    "        agreement_results = analyze_agreement(metrics_df)\n",
    "        \n",
    "        logging.info(\"Starting stability analysis\")\n",
    "        stability_df = analyze_stability(datasets, ['BERTopic', 'VAE', 'LowBERTopic'])\n",
    "        \n",
    "        logging.info(\"Starting topic quality visualization\")\n",
    "        visualize_topic_quality(metrics_df)\n",
    "        \n",
    "        logging.info(\"Starting coherence stability evaluation\")\n",
    "        stability_results = evaluate_coherence_stability(['BERTopic', 'VAE', 'LowBERTopic'], list(datasets.keys()), list(datasets.values()))\n",
    "        \n",
    "        logging.info(\"Starting result output\")\n",
    "        print_results(metrics_df, agreement_results, stability_df, stability_results)\n",
    "        \n",
    "        logging.info(\"All analyses completed\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error occurred in main function execution: {e}\")\n",
    "        raise\n",
    "\n",
    "# 메인 실행 부분\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " import os\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import anthropic\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# BERT 모델과 토크나이저를 위한 import\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# 필요한 경우 NLTK 데이터 다운로드\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# 로깅 설정 (첫 번째 셀에서 이미 설정되었을 수 있지만, 안전을 위해 다시 설정)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# stop_words 정의 (첫 번째 셀에서 이미 정의되었을 수 있지만, 안전을 위해 다시 정의)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 전역 변수로 BERT 모델과 토크나이저 선언 (첫 번째 셀에서 이미 선언되었을 수 있음)\n",
    "global tokenizer, bert_model\n",
    "\n",
    "# matplotlib 백엔드 설정 (이미 설정되어 있을 수 있지만, 안전을 위해 다시 설정)\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(3))\n",
    "def call_anthropic_api(prompt: str, max_tokens_to_sample: int = 3000) -> str:\n",
    "    anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "    if not anthropic_api_key:\n",
    "        raise ValueError(\"Anthropic API key not found in environment variable ANTHROPIC_API_KEY\")\n",
    "    \n",
    "    client = anthropic.Anthropic(api_key=anthropic_api_key)\n",
    "    \n",
    "    try:\n",
    "        logging.info(\"Calling Anthropic API...\")\n",
    "        response = client.completions.create(\n",
    "            model=\"claude-2\",\n",
    "            prompt=f\"{anthropic.HUMAN_PROMPT} {prompt} {anthropic.AI_PROMPT}\",\n",
    "            max_tokens_to_sample=max_tokens_to_sample,\n",
    "            temperature=0,\n",
    "        )\n",
    "        logging.info(\"Anthropic API call successful\")\n",
    "        return response.completion\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in Anthropic API call: {e}\")\n",
    "        logging.error(f\"Client object: {client}\")\n",
    "        logging.error(f\"Prompt: {prompt[:100]}...\")  # Log only the first 100 characters\n",
    "        raise\n",
    "\n",
    "def llm_evaluation(topics):\n",
    "    scores = []\n",
    "    feedbacks = []\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Evaluate the following topics based on their coherence. Coherence is an important metric for assessing the quality of topic modeling:\n",
    "\n",
    "    1. Coherence measures how semantically related the words within each topic are.\n",
    "    2. High coherence scores indicate that the words in a topic are closely related and form a meaningful theme.\n",
    "    3. High coherence scores suggest that the words within a topic have strong semantic connections, making it easy for humans to understand and interpret the topic.\n",
    "    4. Low coherence scores indicate that the words within a topic have little semantic relevance, potentially making the topic ambiguous and difficult to interpret.\n",
    "\n",
    "    Please evaluate the following topics. For each topic, provide a coherence score on a scale of 1-10 and explain your reasoning:\n",
    "\n",
    "    {topics}\n",
    "\n",
    "    When evaluating, consider:\n",
    "    1. How semantically related are the words within each topic?\n",
    "    2. How clear and interpretable is the topic?\n",
    "    3. Do the words in the topic represent a consistent theme or concept?\n",
    "\n",
    "    Please respond for each topic in the following format exactly:\n",
    "    Topic X: [score]\n",
    "    Reason: [explanation]\n",
    "\n",
    "    Note: Ensure the score is enclosed within square brackets, e.g., [7].\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        evaluation = call_anthropic_api(prompt)\n",
    "\n",
    "        # Updated parsing logic to extract structured responses\n",
    "        topic_evaluations = re.findall(r\"Topic \\d+:.*?(?=Topic \\d+:|$)\", evaluation, re.DOTALL)\n",
    "        for eval in topic_evaluations:\n",
    "            score_match = re.search(r'Topic (\\d+):\\s*\\[?(\\d+)\\]?', eval)\n",
    "            reason_match = re.search(r'Reason:\\s*(.*)', eval, re.DOTALL)\n",
    "            if score_match and reason_match:\n",
    "                topic_score = int(score_match.group(2))\n",
    "                if 1 <= topic_score <= 10:\n",
    "                    scores.append(topic_score)\n",
    "                    feedbacks.append(reason_match.group(1).strip())\n",
    "                else:\n",
    "                    print(f\"Invalid score (not between 1 and 10): {eval}\")\n",
    "            else:\n",
    "                print(f\"Could not extract score or reason: {eval}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        raise\n",
    "\n",
    "    return scores, feedbacks\n",
    "\n",
    "def run_llm_evaluation(sample_size=100, chunk_size=10):\n",
    "    topics_df = pd.read_csv('topics_df.csv')\n",
    "    llm_results = []\n",
    "    actual_sample_size = min(sample_size, len(topics_df))\n",
    "    \n",
    "    for index, row in tqdm(topics_df.sample(n=actual_sample_size, random_state=42).iterrows(), total=actual_sample_size):\n",
    "        domain = row['Domain']\n",
    "        model_type = row['Model']\n",
    "        topics = eval(row['Topics'])  # Convert string to list\n",
    "        \n",
    "        logging.info(f\"LLM evaluation in progress - Domain: {domain}, Model: {model_type}\")\n",
    "\n",
    "        try:\n",
    "            scores, feedbacks = llm_evaluation(topics)\n",
    "\n",
    "            result = {\n",
    "                'Domain': domain,\n",
    "                'Model': model_type,\n",
    "                'LLM_Scores': scores,\n",
    "                'LLM_Feedbacks': feedbacks\n",
    "            }\n",
    "            llm_results.append(result)\n",
    "\n",
    "            if len(llm_results) % chunk_size == 0:\n",
    "                save_results_chunk(llm_results[-chunk_size:])\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {domain} - {model_type}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    if len(llm_results) % chunk_size != 0:\n",
    "        save_results_chunk(llm_results[-(len(llm_results) % chunk_size):])\n",
    "\n",
    "    llm_df = pd.DataFrame(llm_results)\n",
    "    return llm_df\n",
    "         \n",
    "          \n",
    "def analyze_llm_results(llm_df):\n",
    "    llm_df['LLM_Avg_Score'] = llm_df['LLM_Scores'].apply(lambda scores: np.mean([s for s in scores if s is not None]))\n",
    "    llm_df['LLM_Std_Score'] = llm_df['LLM_Scores'].apply(lambda scores: np.std([s for s in scores if s is not None]))\n",
    "    llm_df['LLM_Median_Score'] = llm_df['LLM_Scores'].apply(lambda scores: np.median([s for s in scores if s is not None]))\n",
    "\n",
    "    print(\"\\nLLM Evaluation Results:\")\n",
    "    print(llm_df[['Domain', 'Model', 'LLM_Avg_Score', 'LLM_Std_Score', 'LLM_Median_Score']])\n",
    "\n",
    "def llm_auto_metric_correlation(metrics_df, llm_df):\n",
    "    merged_df = pd.merge(metrics_df, llm_df, on=['Domain', 'Model'])\n",
    "\n",
    "    metric_names = ['Coherence', 'NPMI', 'U_Mass']\n",
    "    for metric in metric_names:\n",
    "        valid_idx = merged_df['LLM_Avg_Score'].notnull()\n",
    "        \n",
    "        # Pearson 및 Spearman 상관계수 계산\n",
    "        pearson_corr, p_value_pearson = stats.pearsonr(merged_df.loc[valid_idx, metric], merged_df.loc[valid_idx, 'LLM_Avg_Score'])\n",
    "        spearman_corr, p_value_spearman = stats.spearmanr(merged_df.loc[valid_idx, metric], merged_df.loc[valid_idx, 'LLM_Avg_Score'])\n",
    "        \n",
    "        # ARI 계산을 위한 데이터 준비\n",
    "        auto_metric = merged_df.loc[valid_idx, metric].values\n",
    "        llm_scores = merged_df.loc[valid_idx, 'LLM_Avg_Score'].values\n",
    "        \n",
    "        # 점수를 범주화 (예: 3개의 범주로 나누기)\n",
    "        auto_categories = np.digitize(auto_metric, bins=np.linspace(min(auto_metric), max(auto_metric), 4))\n",
    "        llm_categories = np.digitize(llm_scores, bins=np.linspace(min(llm_scores), max(llm_scores), 4))\n",
    "        \n",
    "        # ARI 계산\n",
    "        ari = adjusted_rand_score(auto_categories, llm_categories)\n",
    "        \n",
    "        print(f\"\\nComparison between LLM evaluation scores and {metric}:\")\n",
    "        print(f\"Pearson: correlation coefficient = {pearson_corr:.4f}, p-value = {p_value_pearson:.4f}\")\n",
    "        print(f\"Spearman: correlation coefficient = {spearman_corr:.4f}, p-value = {p_value_spearman:.4f}\")\n",
    "        print(f\"Adjusted Rand Index (ARI) = {ari:.4f}\")\n",
    "\n",
    "def verify_llm_consistency(topics, n_repeats=5):\n",
    "    all_scores = []\n",
    "    for _ in range(n_repeats):\n",
    "        scores, _ = llm_evaluation(topics)\n",
    "        all_scores.append(scores)\n",
    "    all_scores = np.array(all_scores)\n",
    "    std_scores = np.std(all_scores, axis=0)\n",
    "    avg_std = np.mean(std_scores)\n",
    "    cv_scores = std_scores / np.mean(all_scores, axis=0)\n",
    "    avg_cv = np.mean(cv_scores)\n",
    "    print(f\"\\nAverage standard deviation of LLM evaluation: {avg_std:.4f}\")\n",
    "    print(f\"Average coefficient of variation (CV) of LLM evaluation: {avg_cv:.4f}\")\n",
    "\n",
    "def analyze_llm_feedback(llm_df):\n",
    "    all_words = []\n",
    "    for feedbacks in llm_df['LLM_Feedbacks']:\n",
    "        for feedback in feedbacks:\n",
    "            words = feedback.lower().split()\n",
    "            all_words.extend([word for word in words if word not in stop_words])\n",
    "\n",
    "    word_freq = Counter(all_words)\n",
    "    print(\"\\nMost frequent keywords in feedback:\")\n",
    "    for word, count in word_freq.most_common(10):\n",
    "        print(f\"{word}: {count}\")\n",
    "\n",
    "    coherence_keywords = ['coherent', 'consistent', 'related', 'connected', 'meaningful']\n",
    "    print(\"\\nFrequency of coherence-related keywords:\")\n",
    "    for keyword in coherence_keywords:\n",
    "        print(f\"{keyword}: {word_freq[keyword]}\")\n",
    "\n",
    "    positive_keywords = ['good', 'great', 'excellent', 'well', 'clear']\n",
    "    negative_keywords = ['poor', 'bad', 'unclear', 'confusing', 'unrelated']\n",
    "    \n",
    "    positive_count = sum(word_freq[word] for word in positive_keywords)\n",
    "    negative_count = sum(word_freq[word] for word in negative_keywords)\n",
    "    \n",
    "    print(f\"\\nNumber of positive feedback keywords: {positive_count}\")\n",
    "    print(f\"Number of negative feedback keywords: {negative_count}\")\n",
    "\n",
    "    relationship_keywords = ['related', 'similar', 'overlapping', 'connected', 'distinct']          \n",
    "              \n",
    "    print(\"\\nFrequency of topic relationship keywords:\")\n",
    "    for keyword in relationship_keywords:\n",
    "        print(f\"{keyword}: {word_freq[keyword]}\")\n",
    "\n",
    "    quality_keywords = ['coherent', 'meaningful', 'interpretable', 'clear', 'specific']\n",
    "    print(\"\\nFrequency of topic quality keywords:\")\n",
    "    for keyword in quality_keywords:\n",
    "        print(f\"{keyword}: {word_freq[keyword]}\")\n",
    "\n",
    "    scores = [score for scores in llm_df['LLM_Scores'] for score in scores if score is not None]\n",
    "    print(\"\\nDistribution of coherence scores:\")\n",
    "    print(f\"Mean: {np.mean(scores):.2f}\")\n",
    "    print(f\"Median: {np.median(scores):.2f}\")\n",
    "    print(f\"Standard deviation: {np.std(scores):.2f}\")\n",
    "    print(f\"Minimum: {np.min(scores):.2f}\")\n",
    "    print(f\"Maximum: {np.max(scores):.2f}\")\n",
    "\n",
    "    print(\"\\nAverage coherence score by model:\")\n",
    "    for model in llm_df['Model'].unique():\n",
    "        model_scores = [score for scores, m in zip(llm_df['LLM_Scores'], llm_df['Model']) \n",
    "                        for score in scores if score is not None and m == model]\n",
    "        print(f\"{model}: {np.mean(model_scores):.2f}\")\n",
    "\n",
    "def visualize_llm_results(llm_df):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x='Model', y='LLM_Avg_Score', data=llm_df)\n",
    "    plt.title('Distribution of LLM Evaluation Scores by Model')\n",
    "    plt.savefig('llm_model_score_distribution.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.scatterplot(x='Model', y='LLM_Avg_Score', data=llm_df)\n",
    "    plt.title('LLM Evaluation Scores by Model')\n",
    "    plt.legend()\n",
    "    plt.savefig('llm_model_score.png')\n",
    "    plt.close()\n",
    "\n",
    "def save_results_chunk(results_chunk):\n",
    "    with open('llm_evaluation_results.json', 'a') as f:\n",
    "        for result in results_chunk:\n",
    "            json.dump(result, f)\n",
    "            f.write('\\n')\n",
    "\n",
    "# Execute LLM evaluation\n",
    "logging.info(\"Starting LLM evaluation\")\n",
    "metrics_df = pd.read_csv('topic_modeling_metrics.csv')\n",
    "\n",
    "llm_df = run_llm_evaluation()\n",
    "analyze_llm_results(llm_df)\n",
    "visualize_llm_results(llm_df)\n",
    "\n",
    "logging.info(\"Starting correlation analysis between LLM evaluation and automatic metrics\")\n",
    "llm_auto_metric_correlation(metrics_df, llm_df)\n",
    "\n",
    "logging.info(\"Starting LLM feedback analysis\")\n",
    "analyze_llm_feedback(llm_df)\n",
    "\n",
    "logging.info(\"LLM evaluation completed\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
